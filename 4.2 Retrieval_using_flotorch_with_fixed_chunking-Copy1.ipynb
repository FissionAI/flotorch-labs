{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2adb3d98-bb17-4dff-b150-40d9ab043465",
   "metadata": {},
   "source": [
    "# Retrieval and Generation with Bedrock Foundational Models\n",
    "\n",
    "### Overview  \n",
    "This notebook demonstrates how to perform retrieval-augmented generation (RAG) using Amazon Bedrock's foundational models. It covers retrieving relevant documents from a knowledge base and generating responses based on the retrieved context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427d8982-69f2-4705-b402-590a0252e5b2",
   "metadata": {},
   "source": [
    "# üîç Retrieval in Flotorch\n",
    "\n",
    "[Flotorch](https://www.flotorch.ai/) is a real-time Retrieval-Augmented Generation (RAG) orchestration engine designed to streamline operational complexity and enhance observability in deploying AI workflows.\n",
    "\n",
    "In Flotorch, **retrieval** refers to the process of fetching relevant information from external knowledge bases to augment the responses generated by language models. This ensures that the AI system provides accurate, timely, and context-aware answers by combining its pre-trained knowledge with up-to-date external data.\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Key Components of Retrieval in Flotorch\n",
    "\n",
    "1. **Retriever**  \n",
    "   Searches external databases or knowledge sources to find relevant information based on the user's query.\n",
    "\n",
    "2. **Augmentation**  \n",
    "   Incorporates the retrieved data into the model's input to enhance the quality and relevance of the generated response.\n",
    "\n",
    "3. **Generator**  \n",
    "   Synthesizes a response by integrating the retrieved information with the model's existing knowledge.\n",
    "\n",
    "---\n",
    "\n",
    "This retrieval mechanism is integral to Flotorch's ability to deliver precise and context-aware AI solutions across various industries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afd887e-afe6-4dc8-8f45-328c1c0a86ef",
   "metadata": {},
   "source": [
    "### Build your own Retrieval Augmented Generation (RAG) system\n",
    "When constructing your own retrieval augmented generation (RAG) system, you can leverage a retriever system and a generator system. The retriever can be an embedding model that identifies the relevant chunks from the vector database based on similarity scores. The generator can be a Large Language Model (LLM) that utilizes the model's capability to answer questions based on the retrieved results (also known as chunks). In the following sections, we will provide additional tips on how to optimize the prompts for your RAG system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c114e690-2fef-4dc8-8a37-f909e512dc56",
   "metadata": {},
   "source": [
    "## üîß Step 1: load aws variables created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5381fe1f-8477-421a-83ea-a498f1780662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accountNumber': '746074413210',\n",
       " 'regionName': 'us-west-2',\n",
       " 'collectionArn': 'arn:aws:aoss:us-west-2:746074413210:collection/3f35uv3lze9bdothrm0c',\n",
       " 'collectionId': '3f35uv3lze9bdothrm0c',\n",
       " 'vectorIndexName': 'ws-index-',\n",
       " 'bedrockExecutionRoleArn': 'arn:aws:iam::746074413210:role/advanced-rag-workshop-bedrock_execution_role-us-west-2',\n",
       " 's3Bucket': '746074413210-us-west-2-advanced-rag-workshop',\n",
       " 'kbFixedChunk': 'WO4U6AWAU1',\n",
       " 'kbSemanticChunk': 'OUFEWBGEES',\n",
       " 'kbHierarchicalChunk': 'IHWIS6EP0H',\n",
       " 'ground_truth_path': 's3://746074413210-us-west-2-advanced-rag-workshop/ground_truth_data_files/kbqa_questions_answers.json'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open(\"variables.json\", \"r\") as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fb3777-e1ba-4686-9a76-7faed4fd9f8b",
   "metadata": {},
   "source": [
    "## Load Prompt json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5093f2d-ff0b-44d1-8dee-5415e039a49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_file_path = './dataset/prompt.json'\n",
    "with open(prompt_file_path, 'r') as f:\n",
    "    prompt = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aa1e671-1ce1-447a-829c-3c1916f3a29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking strategy\n",
    "chunking_strategies = ['fixed', 'hierarchical', 'semantic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42e1348c-2f36-4ba1-8a36-10efffb6d0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get kb id for a chunking strategy\n",
    "def get_kb_id(chunking_strategy):\n",
    "    if chunking_strategy == 'fixed':\n",
    "        return variables['kbFixedChunk']\n",
    "    elif chunking_strategy == 'hierarchical':\n",
    "        return variables['kbHierarchicalChunk']\n",
    "    elif chunking_strategy == 'semantic':\n",
    "        return variables['kbSemanticChunk']\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a199fd02-48ac-42d4-ad88-96682f3e3e01",
   "metadata": {},
   "source": [
    "## Sample experiment JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19762bd0-5991-4b6a-8f95-2017e8b786f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_config_data = {\n",
    "            \"temp_retrieval_llm\": \"0.1\",\n",
    "            \"gt_data\": variables[\"ground_truth_path\"],\n",
    "            \"rerank_model_id\": \"none\",\n",
    "            \"embedding_model\": \"amazon.titan-embed-text-v2:0\",\n",
    "            \"bedrock_knowledge_base\": True,\n",
    "            # \"kb_data\": variables['kbFixedChunk'],\n",
    "            \"retrieval_service\": \"bedrock\",\n",
    "            \"knn_num\": \"3\",\n",
    "            \"knowledge_base\": True,\n",
    "            \"retrieval_model\": \"us.amazon.nova-pro-v1:0\",\n",
    "            \"gateway_api_key\": \"\",\n",
    "            \"vector_dimension\": \"1024\",\n",
    "            \"gateway_enabled\": False,\n",
    "            \"gateway_url\": \"\",\n",
    "            # \"chunking_strategy\": \"Fixed\",\n",
    "            \"aws_region\": \"us-west-2\",\n",
    "            \"n_shot_prompt_guide_obj\": prompt,\n",
    "            \"n_shot_prompts\": 1\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12371288-c445-42e2-baee-8ac5ee3863aa",
   "metadata": {},
   "source": [
    "### Load Retriver function and other dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd348a6b-7fd2-4b11-ba90-f2a1a4a73cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pydantic/_internal/_fields.py:198: UserWarning: Field name \"json\" in \"MonitoringDatasetFormat\" shadows an attribute in parent \"Base\"\n",
      "  warnings.warn(\n",
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from flotorch_core.storage.storage_provider_factory import StorageProviderFactory\n",
    "from flotorch_core.reader.json_reader import JSONReader\n",
    "from flotorch_core.storage.db.vector.vector_storage_factory import VectorStorageFactory\n",
    "from flotorch_core.inferencer.inferencer_provider_factory import InferencerProviderFactory\n",
    "from flotorch_core.embedding.embedding_registry import embedding_registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19275dfc",
   "metadata": {},
   "source": [
    "### Initialize storage provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d922ecf0-a40f-4c19-8785-cea9e01670e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_data = exp_config_data['gt_data']\n",
    "storage = StorageProviderFactory.create_storage_provider(gt_data)\n",
    "gt_data_path = storage.get_path(gt_data)\n",
    "json_reader = JSONReader(storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f36e755",
   "metadata": {},
   "source": [
    "### Setting embedding to None if bedrock KB is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80bb60cb-63d0-4213-b997-77a1f7441566",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd97d675-b399-4761-b7a2-b0c197241c39",
   "metadata": {},
   "source": [
    "## üóÉÔ∏è Vector Storage Initialization\n",
    "\n",
    "This section initializes the `VectorStorage` component using a factory method that dynamically selects the appropriate vector storage backend (e.g., OpenSearch, Bedrock Knowledge Base) based on the experimental configuration.\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è `VectorStorageFactory.create_vector_storage(...)`\n",
    "\n",
    "Creates an instance of vector storage using configuration flags and credentials.\n",
    "\n",
    "- **Parameters:**\n",
    "  - `knowledge_base`: *(bool)* ‚Äì Whether a knowledge base is used as a backend.\n",
    "  - `use_bedrock_kb`: *(bool)* ‚Äì If set, uses AWS Bedrock Knowledge Base.\n",
    "  - `embedding`: *(BaseEmbedding)* ‚Äì Embedding generator to use for vector creation.\n",
    "  - `knowledge_base_id`: *(str | None)* ‚Äì ID of the Bedrock knowledge base.\n",
    "  - `aws_region`: *(str | None)* ‚Äì AWS region for Bedrock and related services.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Dynamic Backend Selection\n",
    "\n",
    "The factory method chooses the backend as follows:\n",
    "\n",
    "- If `bedrock_knowledge_base` is enabled ‚Üí connects to **Bedrock KB**.\n",
    "- Else if `knowledge_base` is enabled ‚Üí connects to **custom knowledge base**.\n",
    "\n",
    "---\n",
    "\n",
    "### üìù Result\n",
    "\n",
    "Returns a configured `VectorStorage` instance ready for:\n",
    "- KNN-based vector search\n",
    "- Bedrock KB search\n",
    "- Integration into QA or retrieval pipelines\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6ad13e",
   "metadata": {},
   "source": [
    "### Initialize vector storage with configuration for embedding and optional OpenSearch/Bedrock KB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79351c77-52b5-4238-8b7c-a33dae9f880d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_storage = {}\n",
    "for each_chunking_strategy in chunking_strategies:\n",
    "    vector_storage[each_chunking_strategy] = VectorStorageFactory.create_vector_storage(\n",
    "                    knowledge_base=exp_config_data.get(\"knowledge_base\", False),\n",
    "                    use_bedrock_kb=exp_config_data.get(\"bedrock_knowledge_base\", False),\n",
    "                    embedding=embedding,\n",
    "                    knowledge_base_id=get_kb_id(each_chunking_strategy),\n",
    "                    aws_region=exp_config_data.get(\"aws_region\")\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e2d0ee-4675-4afc-8218-1c2790bf480f",
   "metadata": {},
   "source": [
    "## ü§ñ Inferencer Initialization\n",
    "\n",
    "This block initializes the **Inferencer** using a factory method that configures the inference engine for text generation or question answering based on the experimental setup.\n",
    "\n",
    "---\n",
    "\n",
    "### üèóÔ∏è `InferencerProviderFactory.create_inferencer_provider(...)`\n",
    "\n",
    "Creates and returns an appropriate `Inferencer` instance depending on configuration such as API gateway usage, model settings, region, and credentials.\n",
    "\n",
    "---\n",
    "\n",
    "### üîß Parameters\n",
    "\n",
    "- `gateway_enabled`: *(bool)* ‚Äì Enables API gateway-based invocation if set to `True`.\n",
    "- `base_url`: *(str)* ‚Äì URL endpoint for the API Gateway (e.g., `/api/openai/v1`).\n",
    "- `api_key`: *(str)* ‚Äì API key for authenticating requests to the gateway.\n",
    "- `service`: *(str)* ‚Äì Name of the retrieval service (e.g., Bedrock, sagemaker).\n",
    "- `model_id`: *(str)* ‚Äì The model to use for inference (e.g., `anthropic.claude-v2`).\n",
    "- `region`: *(str)* ‚Äì AWS region for service provisioning (e.g., `us-east-1`).\n",
    "- `arn_role`: *(str)* ‚Äì IAM role ARN for Bedrock invocation permissions.\n",
    "- `n_shot_prompts`: *(int)* ‚Äì Number of few-shot examples to include in prompt.\n",
    "- `temp_retrieval_llm`: *(float)* ‚Äì Temperature setting for the language model.\n",
    "- `n_shot_prompt_guide_obj`: *(Any)* ‚Äì Few-shot guide object for prompt engineering.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Behavior\n",
    "\n",
    "- If `gateway_enabled` is `True`, connects to the specified API Gateway using credentials.\n",
    "- If disabled, falls back to direct model invocation through supported services like AWS Bedrock.\n",
    "- Supports dynamic few-shot prompting and custom temperature configuration.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Outcome\n",
    "\n",
    "Returns a fully configured `Inferencer` object capable of generating answers or completions for queries using the selected language model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe940888",
   "metadata": {},
   "source": [
    "### Initialize inferencer provider with configuration for gateway, retrieval service, and AWS integration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bddfc720-b994-41e2-8491-35881547f325",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferencer = InferencerProviderFactory.create_inferencer_provider(\n",
    "                gateway_enabled = False,\n",
    "                base_url = \"\",\n",
    "                api_key = \"\",\n",
    "                service = exp_config_data.get(\"retrieval_service\"),\n",
    "                model_id = exp_config_data.get(\"retrieval_model\"), \n",
    "                region = exp_config_data.get(\"aws_region\"), \n",
    "                arn_role = variables.get('bedrockExecutionRoleArn', 'arn:aws:iam::677276078734:role/flotorch-bedrock-role-qamain'),\n",
    "                n_shot_prompts = int(exp_config_data.get(\"n_shot_prompts\", 0)), \n",
    "                temperature = float(exp_config_data.get(\"temp_retrieval_llm\", 0)), \n",
    "                n_shot_prompt_guide_obj = exp_config_data.get(\"n_shot_prompt_guide_obj\")\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e74bb4-5639-4216-a1d1-2b2f0d830e3b",
   "metadata": {},
   "source": [
    "## üîÅ Reranker Initialization\n",
    "\n",
    "This code conditionally initializes the **`BedrockReranker`**, which reorders retrieved documents based on relevance using a reranking model.\n",
    "\n",
    "---\n",
    "\n",
    "### üèóÔ∏è `BedrockReranker(...)` Initialization\n",
    "\n",
    "The reranker is only instantiated if a valid rerank model ID is provided in the experiment configuration.\n",
    "\n",
    "---\n",
    "\n",
    "### üîß Parameters\n",
    "\n",
    "- `aws_region`: *(str)* ‚Äì AWS region where the Bedrock reranking model is hosted.\n",
    "- `rerank_model_id`: *(str)* ‚Äì ID of the Bedrock reranking model to be used.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Behavior\n",
    "\n",
    "- If `rerank_model_id` is **not** `\"none\"` (case-insensitive), a `BedrockReranker` is created.\n",
    "- If the value is `\"none\"`, no reranker is used and the value is set to `None`.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Outcome\n",
    "\n",
    "- A `BedrockReranker` object if reranking is enabled.\n",
    "- Otherwise, `reranker = None`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a20966",
   "metadata": {},
   "source": [
    "### Initialize reranker if a valid rerank model ID is provided in the configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38800c24-aa54-4d67-afcb-0fd5c41c4bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker = BedrockReranker(exp_config_data.get(\"aws_region\"), exp_config_data.get(\"rerank_model_id\")) \\\n",
    "                if exp_config_data.get(\"rerank_model_id\").lower() != \"none\" \\\n",
    "                else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c335b9f7",
   "metadata": {},
   "source": [
    "### Load ground truth data in JSON reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af7e2cd3-41d7-42a6-a588-5f7d06a5fcdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:flotorch_core.storage.s3_storage:Reading data from S3 storage\n"
     ]
    }
   ],
   "source": [
    "## Read ground truth json\n",
    "from pydantic import BaseModel\n",
    "from flotorch_core.chunking.chunking import Chunk\n",
    "class Question(BaseModel):\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "    def get_chunk(self) -> Chunk:\n",
    "        return Chunk(data=self.question)\n",
    "\n",
    "questions_list = json_reader.read_as_model(gt_data_path, Question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b1b9d42-eb35-4e49-99c6-67250e2205eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fixed': <flotorch_core.storage.db.vector.bedrock_knowledgebase_storage.BedrockKnowledgeBaseStorage at 0x7faffd31ab90>,\n",
       " 'hierarchical': <flotorch_core.storage.db.vector.bedrock_knowledgebase_storage.BedrockKnowledgeBaseStorage at 0x7fafc12ccfa0>,\n",
       " 'semantic': <flotorch_core.storage.db.vector.bedrock_knowledgebase_storage.BedrockKnowledgeBaseStorage at 0x7fafc12cc3d0>}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0992f6f5",
   "metadata": {},
   "source": [
    "### ü§ñ Perform vector search for each question chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e8c5c53-9308-4878-8cfc-378e718418c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector search with fixed completed\n",
      "Vector search with hierarchical completed\n",
      "Vector search with semantic completed\n"
     ]
    }
   ],
   "source": [
    "chunking_strategy_vector_esponses_dict = {}\n",
    "for each_chunking_strategy in chunking_strategies:\n",
    "    responses_list = []\n",
    "    for question in questions_list:\n",
    "        question_chunk = question.get_chunk()\n",
    "        vector_response = vector_storage[each_chunking_strategy].search(question_chunk, int(exp_config_data.get(\"knn_num\")), False)\n",
    "        vector_response_result = vector_response.to_json()['result']\n",
    "        responses_list.append({'question':question, 'question_chunk':question_chunk, 'vector_response':vector_response, 'vector_response_result':vector_response_result, 'response_status':vector_response.status})\n",
    "    print(f\"Vector search with {each_chunking_strategy} completed\")\n",
    "    chunking_strategy_vector_esponses_dict[each_chunking_strategy] = responses_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aba61c",
   "metadata": {},
   "source": [
    "### üîÅ Rerank vector responses using the reranker if enabled and response is valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a955930e-6e00-44e4-919d-6d6bb0953b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunking_strategy, vector_response_list in chunking_strategy_vector_esponses_dict.items():\n",
    "    for each_response in vector_response_list:\n",
    "        response_status = each_response['response_status']\n",
    "        vector_response_result = each_response['vector_response_result']\n",
    "        if reranker and response_status:\n",
    "            vector_response = reranker.rerank_documents(each_response['question_chunk'].data, vector_response_result)\n",
    "            each_response['vector_response'] = vector_response\n",
    "    print(f\"Reranking completed with {each_chunking_strategy} completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38cd325",
   "metadata": {},
   "source": [
    "### üß† Generate answers and extract metadata for each response, applying guardrail checks if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c739274-0df4-4708-b5e4-400a088cb928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 12:37:02,544 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:37:03,879 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:37:05,911 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:37:08,732 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:37:11,856 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:37:12,620 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:37:14,097 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:37:15,027 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:37:16,458 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:37:17,516 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:37:19,990 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:37:20,859 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:37:22,593 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:37:24,407 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:37:25,354 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:37:26,867 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:37:28,455 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:37:29,686 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:37:30,811 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:37:32,399 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:37:34,968 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:37:36,027 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:37:37,901 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:37:39,327 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:37:39,913 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:37:40,944 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:37:41,887 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:37:42,714 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:37:43,314 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:37:43,831 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:37:44,430 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:37:45,164 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:37:46,228 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:37:47,033 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:37:47,514 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:37:48,135 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:37:48,985 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:37:51,075 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:37:52,325 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:37:53,815 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:37:54,691 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:37:56,385 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:37:57,526 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:37:58,927 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:00,097 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:01,263 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:02,423 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:03,254 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:04,946 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:05,762 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:07,683 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:08,725 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:09,877 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:10,969 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:12,891 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:14,672 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:15,715 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:16,544 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:17,914 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:18,571 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:19,362 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:20,252 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:21,170 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:21,987 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:22,859 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:23,530 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:24,230 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:25,432 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:26,049 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:26,649 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:27,332 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:29,019 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:31,302 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:32,182 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:34,137 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:34,578 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:35,886 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:36,690 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:39,326 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:40,198 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:40,759 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:41,716 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:42,533 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:44,020 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:45,105 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:46,712 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:48,000 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:48,857 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:49,598 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:51,124 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:52,643 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:53,644 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:54,738 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:56,716 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:57,219 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:57,813 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:58,600 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:59,195 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:38:59,931 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:39:00,297 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:39:00,743 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:39:01,292 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:39:02,579 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:39:03,207 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n",
      "2025-04-20 12:39:03,599 - INFO - Using 1 shot prompt with 1 examples\n",
      "INFO:default:Using 1 shot prompt with 1 examples\n"
     ]
    }
   ],
   "source": [
    "for chunking_strategy, vector_response_list in chunking_strategy_vector_esponses_dict.items():\n",
    "    for each_response in vector_response_list:\n",
    "        response_status = each_response['response_status']\n",
    "        if response_status:\n",
    "            question = each_response['question']\n",
    "            vector_response = each_response['vector_response']\n",
    "            vector_response_result = each_response['vector_response_result']\n",
    "            metadata, answer = inferencer.generate_text(question.question, vector_response_result)\n",
    "            guardrail_blocked = metadata['guardrail_blocked'] if 'guardrail_blocked' in metadata else False\n",
    "            if guardrail_blocked:\n",
    "                answer_metadata = {}\n",
    "            else:\n",
    "                answer_metadata = metadata\n",
    "        else:\n",
    "            answer = metadata['guardrail_output']\n",
    "            metadata = {}\n",
    "            answer_metadata = {}\n",
    "            guardrail_blocked = vector_response.metadata['guardrail_blocked'] if 'guardrail_blocked' in vector_response.metadata else False\n",
    "        each_response['metadata'] = metadata\n",
    "        each_response['answer'] = answer\n",
    "        each_response['answer_metadata'] = answer_metadata\n",
    "        each_response['guardrail_blocked'] = guardrail_blocked\n",
    "    print(f\"Inferencing completed with {each_chunking_strategy} completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932195d0",
   "metadata": {},
   "source": [
    "### üì¶ Aggregate final results with question, answer, guardrail assessments, and reference context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3eeaedf-be15-4f47-9ac1-af1023ea8d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_dict = {}\n",
    "for chunking_strategy, vector_response_list in chunking_strategy_vector_esponses_dict.items():\n",
    "    result = []\n",
    "    for each_response in vector_response_list:\n",
    "        metadata = each_response['metadata']\n",
    "        vector_response = each_response['vector_response']\n",
    "        vector_response_result = each_response['vector_response_result']\n",
    "        result.append(\n",
    "                    {'question':each_response['question'].question,\n",
    "                    'answer':each_response['answer'],\n",
    "                    'guardrails_output_assessment':metadata['guardrail_output_assessment'] if 'guardrail_output_assessment' in metadata else None,\n",
    "                    'guardrails_context_assessment':vector_response.metadata['guardrail_context_assessment'] if 'guardrail_context_assessment' in vector_response.metadata else None,\n",
    "                    'guardrails_input_assessment':vector_response.metadata['guardrail_input_assessment'] if 'guardrail_input_assessment' in vector_response.metadata else None,\n",
    "                    'guardrails_blocked':each_response['guardrail_blocked'],\n",
    "                    'guardrails_block_level':vector_response.metadata['block_level'] if 'block_level' in vector_response.metadata else \"\",\n",
    "                    'answer_metadata':each_response['answer_metadata'],\n",
    "                    'reference_contexts':[res['text'] for res in vector_response_result] if vector_response_result else [],\n",
    "                    'gt_answer':each_response['question'].answer,\n",
    "                    'query_metadata':vector_response.metadata['embedding_metadata'].to_json() if 'embedding_metadata' in vector_response.metadata else None\n",
    "                    })\n",
    "    inference_dict[chunking_strategy] = result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84674286",
   "metadata": {},
   "source": [
    "### üíæ Save the aggregated results to a JSON file for inference metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0dc91fa0-5952-4d83-8372-ed46d47dd91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"results/{exp_config_data['retrieval_service']}_inference_metrics.json\", \"w\") as json_file:\n",
    "    json.dump(inference_dict, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f72648-1282-4fe0-b78b-6b9a1bd7dcad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01458e5e-7103-4c68-957a-c67e2213e20c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a903173-61d3-4bca-b423-cf92d3c0e5b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2fa652-dd59-4f5e-9b73-2af0b79020d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
