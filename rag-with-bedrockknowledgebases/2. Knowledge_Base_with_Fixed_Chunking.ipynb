{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aa74695-63c7-48a0-91b3-d3317035dd19",
   "metadata": {},
   "source": [
    "## Create a Knowledge Base with fixed chunking strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04e9f76",
   "metadata": {},
   "source": [
    "Chunking data is essential. If you are adding large documents with hundreds of pages to your knowledge base then you need to split them up and return only the relevant sections to use as context for your inference. If you are returning too much context it will increase costs (models charge based on input token count) and latency. It may also harm output quality. Shorter chunks will provide a better match but may lack the context necessary to answer a question.\n",
    "\n",
    "Bedrock Knowledge bases has a few different chunking strategies to choose from. They handle everything from splitting at semantic boundaries like paragraphs and hierarchical structures. However some document types can benefit from custom chunking. For example, any form of mark up can be used by a custom chunking approach.\n",
    "\n",
    "You can also create your own custom chunking approach using a Lambda function. If you want to add any custom metadata then you will need to add a Lambda function. You can either handle the chunking yourself, edit an existing chunk or just add metadata. Metadata can then be used for filtering.\n",
    "\n",
    "It is important to tune your chunking to the type of documents being ingested. Getting the wrong chunk size will affect the accuracy and response times. It will also increase the costs in both the vector storage and inference steps. The defaults supplied in Bedrock are pretty good but they may need tailored to your specific circumstances. Longer and more technical documents may need larger chunk sizes to make sure they include more context. Speech (like a chat transcript) can benefit from shorter chunks.\n",
    "\n",
    "![Chunking Strategies](./chunking-strategies.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e9c5e0",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this notebook, we will implement a knowledge base using a fixed chunking strategy. Here are the key steps we'll perform:\n",
    "\n",
    "1. **Create a Knowledge Base**: Set up an Amazon Bedrock Knowledge Base with fixed-size chunking configuration that will store and retrieve our vector embeddings.\n",
    "\n",
    "2. **Create a Data Source**: Connect our Knowledge Base to the documents we uploaded to S3 in the previous notebook.\n",
    "\n",
    "3. **Start Ingestion Job**: Begin the process of transforming our documents into chunks, creating embeddings, and storing them in our vector database.\n",
    "\n",
    "4. **Retrieve and Generate**: Test our Knowledge Base by retrieving relevant information based on a sample query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce312491-c1c5-4754-89bd-d28da5558005",
   "metadata": {},
   "source": [
    "#### Concept\n",
    "\n",
    "**Fixed Chunking**: Involves dividing your documents into fixed-size chunks, regardless of the content within them. Each chunk contains a predefined number of tokens or characters, and this method allows for more uniform data organization. \n",
    "\n",
    "Fixed chunking is useful when you want to ensure that your chunks are of a consistent size, making them easier to process and retrieve in a predictable manner. The document is split into sections of equal length, and each section becomes a separate chunk. This method works well when the content is relatively homogeneous, and the chunk boundaries are not as crucial to understanding the underlying context.\n",
    "\n",
    "#### Benefits\n",
    "\n",
    "- **Uniformity**: Each chunk has the same size, making the system more predictable. This helps with processing efficiency since you know that each chunk is of a consistent size, making batch operations and parallel processing easier.\n",
    "- **Simplified Retrieval**: Since the chunk sizes are uniform, searching through the data becomes straightforward. You can quickly determine the length of chunks, which can be useful for performance optimization and scalability in large datasets.\n",
    "- **Performance Optimization**: Fixed chunks are ideal when you want to control the computational cost of document retrieval and chunking. Having equal-sized chunks reduces the chance of computational bottlenecks in scenarios requiring large-scale document processing.\n",
    "\n",
    "> **Note:** While fixed chunking can be efficient for certain use cases, it may not preserve the natural semantic boundaries of the content, such as paragraphs or sections. This may lead to chunks that start or end at arbitrary places, potentially cutting off context in the middle of a sentence or idea.\n",
    "\n",
    "### **Best Use Cases**\n",
    "Fixed chunking is suitable for cases where:\n",
    "- **Homogeneous content**: The content is consistent, and boundaries are not as important.\n",
    "- **Performance**: You need uniform-sized chunks for predictable processing or optimization of large-scale systems.\n",
    "- **Simplified text processing**: When chunk boundaries do not need to match natural semantic structures like paragraphs or sentences.\n",
    "\n",
    "Examples include:\n",
    "- **General document indexing**: When large datasets are involved, and uniform chunk sizes optimize retrieval.\n",
    "- **Text summarization**: Fixed chunking is helpful when generating summaries from uniformly sized data pieces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b18e06c-98ef-41bc-99fd-d5e6c8977e4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accountNumber': '677276078734',\n",
       " 'regionName': 'us-east-1',\n",
       " 'collectionArn': 'arn:aws:aoss:us-east-1:677276078734:collection/rwomrqvg0hnseond2wob',\n",
       " 'collectionId': 'rwomrqvg0hnseond2wob',\n",
       " 'vectorIndexName': 'ws-index-fixed',\n",
       " 'bedrockExecutionRoleArn': 'arn:aws:iam::677276078734:role/advanced-rag-workshop-bedrock_execution_role-us-east-1',\n",
       " 's3Bucket': '677276078734-us-east-1-advanced-rag-workshop',\n",
       " 's3_ground_truth_path': 's3://677276078734-us-east-1-advanced-rag-workshop/ground_truth_data/kbqa_questions_answers.json'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load varibales json generated from prerequisites\n",
    "import json\n",
    "with open(\"./results/variables.json\", \"r\") as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbaee1f0-65f2-4f23-80f4-767dcf78a459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# naming bedrock KB\n",
    "knowledge_base_name = \"advanced-rag-kbs_demo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ceaa5e-2ddd-4b97-853f-527c92326ebd",
   "metadata": {},
   "source": [
    "### 1. Create a Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9933328d-76dc-44bb-a985-591a61c47fca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from retrying import retry\n",
    "import boto3\n",
    "\n",
    "# Initialize the Bedrock Agent client using the provided AWS region\n",
    "bedrock_agent = boto3.client(\"bedrock-agent\", region_name=variables[\"regionName\"])\n",
    "\n",
    "# Retry decorator: If the function fails, it will retry up to 3 times with a random wait time between 1-2 seconds\n",
    "@retry(wait_random_min=1000, wait_random_max=2000, stop_max_attempt_number=3)\n",
    "def create_knowledge_base_func(name, description, chunking_type):\n",
    "    \"\"\"\n",
    "    Creates a knowledge base in Amazon Bedrock with OpenSearch Serverless as the vector store.\n",
    "    \n",
    "    Parameters:\n",
    "        name (str): The name of the knowledge base.\n",
    "        description (str): A description of the knowledge base.\n",
    "        chunking_type (str): The type of chunking strategy applied to vector indexing.\n",
    "\n",
    "    Returns:\n",
    "        dict: The response containing details of the created knowledge base.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the ARN of the embedding model used for vectorization\n",
    "    embedding_model_arn = f\"arn:aws:bedrock:{variables['regionName']}::foundation-model/amazon.titan-embed-text-v2:0\"\n",
    "\n",
    "    # Configure OpenSearch Serverless for vector storage\n",
    "    opensearch_serverless_configuration = {\n",
    "        \"collectionArn\": variables[\"collectionArn\"],  # ARN of the OpenSearch collection\n",
    "        # \"vectorIndexName\": variables[\"vectorIndexName\"] + chunking_type,  # Index name based on chunking strategy\n",
    "        \"vectorIndexName\": variables[\"vectorIndexName\"],  # Index name based on chunking strategy\n",
    "        \"fieldMapping\": {  # Define field mappings for vectors, text, and metadata\n",
    "            \"vectorField\": \"vector\",\n",
    "            \"textField\": \"text\",\n",
    "            \"metadataField\": \"text-metadata\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    print(opensearch_serverless_configuration)  # Print configuration for debugging\n",
    "\n",
    "    # Create the knowledge base in Amazon Bedrock\n",
    "    create_kb_response = bedrock_agent.create_knowledge_base(\n",
    "        name=name,\n",
    "        description=description,\n",
    "        roleArn=variables[\"bedrockExecutionRoleArn\"],  # IAM Role ARN for Bedrock execution\n",
    "        knowledgeBaseConfiguration={\n",
    "            \"type\": \"VECTOR\",\n",
    "            \"vectorKnowledgeBaseConfiguration\": {\n",
    "                \"embeddingModelArn\": embedding_model_arn  # Reference to the embedding model\n",
    "            }\n",
    "        },\n",
    "        storageConfiguration={\n",
    "            \"type\": \"OPENSEARCH_SERVERLESS\",\n",
    "            \"opensearchServerlessConfiguration\": opensearch_serverless_configuration\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return create_kb_response[\"knowledgeBase\"]  # Return the created knowledge base details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68035c04-af12-4ae4-a48d-2e6391bb662f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'collectionArn': 'arn:aws:aoss:us-east-1:677276078734:collection/rwomrqvg0hnseond2wob', 'vectorIndexName': 'ws-index-fixed', 'fieldMapping': {'vectorField': 'vector', 'textField': 'text', 'metadataField': 'text-metadata'}}\n",
      "{'collectionArn': 'arn:aws:aoss:us-east-1:677276078734:collection/rwomrqvg0hnseond2wob', 'vectorIndexName': 'ws-index-fixed', 'fieldMapping': {'vectorField': 'vector', 'textField': 'text', 'metadataField': 'text-metadata'}}\n",
      "{'collectionArn': 'arn:aws:aoss:us-east-1:677276078734:collection/rwomrqvg0hnseond2wob', 'vectorIndexName': 'ws-index-fixed', 'fieldMapping': {'vectorField': 'vector', 'textField': 'text', 'metadataField': 'text-metadata'}}\n",
      "Knowledge Base already exists. Retrieving its ID...\n",
      "Could not find a knowledge base with the specified name.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "try:\n",
    "    # Create a knowledge base using the predefined function\n",
    "    kb = create_knowledge_base_func(\n",
    "        name=knowledge_base_name+\"_2\",\n",
    "        description=\"Knowledge base using Amazon OpenSearch Service as a vector store\",\n",
    "        chunking_type=\"fixed\"\n",
    "    )\n",
    "\n",
    "    # Retrieve details of the newly created knowledge base\n",
    "    get_kb_response = bedrock_agent.get_knowledge_base(knowledgeBaseId=kb['knowledgeBaseId'])\n",
    "\n",
    "    # Update the variables dictionary with the new knowledge base ID\n",
    "    variables[\"kbFixedChunk\"] = kb['knowledgeBaseId']\n",
    "\n",
    "    # Save updated variables to a JSON file, handling datetime serialization\n",
    "    with open(\"./results/variables.json\", \"w\") as f:\n",
    "        json.dump(variables, f, indent=4, default=str)  # Convert datetime to string\n",
    "\n",
    "    # Print the retrieved knowledge base response in a readable format\n",
    "    print(f'OpenSearch Knowledge Response: {json.dumps(get_kb_response, indent=4, default=str)}')\n",
    "\n",
    "except Exception as e:\n",
    "    # Check if error message indicates the knowledge base already exists\n",
    "    error_message = str(e).lower()\n",
    "    if any(phrase in error_message for phrase in [\"already exist\", \"duplicate\", \"already been created\"]):\n",
    "        print(\"Knowledge Base already exists. Retrieving its ID...\")\n",
    "        \n",
    "        # List all knowledge bases to find the one that already exists\n",
    "        list_kb_response = bedrock_agent.list_knowledge_bases()\n",
    "        \n",
    "        # Look for a knowledge base with the desired name\n",
    "        for kb in list_kb_response.get('knowledgeBaseSummaries', []):\n",
    "            if kb['name'] == knowledge_base_name:\n",
    "                kb_id = kb['knowledgeBaseId']\n",
    "                print(f\"Found existing knowledge base with ID: {kb_id}\")\n",
    "                \n",
    "                # Get the details of the existing knowledge base\n",
    "                get_kb_response = bedrock_agent.get_knowledge_base(knowledgeBaseId=kb_id)\n",
    "                \n",
    "                # With this code that reads existing values first:\n",
    "                try:\n",
    "                    # Read existing variables\n",
    "                    with open(\"variables.json\", \"r\") as f:\n",
    "                        existing_variables = json.load(f)\n",
    "                except (FileNotFoundError, json.JSONDecodeError):\n",
    "                    # If file doesn't exist or is invalid JSON\n",
    "                    existing_variables = {}\n",
    "                \n",
    "                # Update only the semantic chunking value\n",
    "                existing_variables[\"kbFixedChunk\"] = kb_id\n",
    "                                \n",
    "                # Write back all variables\n",
    "                with open(\"variables.json\", \"w\") as f:\n",
    "                    json.dump(existing_variables, f, indent=4, default=str)\n",
    "                \n",
    "                # Print the retrieved knowledge base response\n",
    "                print(f'OpenSearch Knowledge Response: {json.dumps(get_kb_response, indent=4, default=str)}')\n",
    "                break\n",
    "        else:\n",
    "            print(\"Could not find a knowledge base with the specified name.\")\n",
    "    else:\n",
    "        # If it's a different error, re-raise it\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d6a826-f3c8-40b5-aaa2-0c2a9662d5dc",
   "metadata": {},
   "source": [
    "### 2. Create Datasources for Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb183c15-662d-498e-9df2-6814560f471a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing data source 'advanced-rag-example'. Deleting it...\n",
      "Waiting for data source deletion to complete...\n",
      "Data source deleted successfully.\n",
      "Creating new data source 'advanced-rag-example'...\n",
      "Data source 'advanced-rag-example' still exists. Retrieving it...\n",
      "Retrieved existing data source: 6LDGU1ZGZU\n",
      "6LDGU1ZGZU\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Define the chunking strategy for data ingestion\n",
    "chunking_strategy_configuration = {\n",
    "    \"chunkingStrategy\": \"FIXED_SIZE\",\n",
    "    \"fixedSizeChunkingConfiguration\": {\n",
    "        \"maxTokens\": 1024,\n",
    "        \"overlapPercentage\": 20\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define the S3 bucket configuration for the data source\n",
    "s3_configuration = {\n",
    "    \"bucketArn\": f\"arn:aws:s3:::{variables['s3Bucket']}\",\n",
    "    \"inclusionPrefixes\": [\"data\"]  # Only include objects with the \"data\" prefix\n",
    "}\n",
    "\n",
    "data_source_name = \"advanced-rag-example\"\n",
    "\n",
    "# First, check if a data source with this name already exists in Bedrock (not just locally)\n",
    "try:\n",
    "    # List all data sources for the knowledge base\n",
    "    list_ds_response = bedrock_agent.list_data_sources(\n",
    "        knowledgeBaseId=kb['knowledgeBaseId']\n",
    "    )\n",
    "    \n",
    "    # Check if our named data source exists\n",
    "    existing_ds = None\n",
    "    for ds in list_ds_response.get('dataSourceSummaries', []):\n",
    "        if ds['name'] == data_source_name:\n",
    "            existing_ds = ds\n",
    "            break\n",
    "    \n",
    "    # If it exists, delete it\n",
    "    if existing_ds:\n",
    "        print(f\"Found existing data source '{data_source_name}'. Deleting it...\")\n",
    "        bedrock_agent.delete_data_source(\n",
    "            knowledgeBaseId=kb['knowledgeBaseId'],\n",
    "            dataSourceId=existing_ds[\"dataSourceId\"]\n",
    "        )\n",
    "        print(\"Waiting for data source deletion to complete...\")\n",
    "        time.sleep(10)\n",
    "        print(\"Data source deleted successfully.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error while checking or deleting data source: {e}\")\n",
    "\n",
    "# Now create a new data source\n",
    "try:\n",
    "    print(f\"Creating new data source '{data_source_name}'...\")\n",
    "    create_ds_response = bedrock_agent.create_data_source(\n",
    "        name=data_source_name,\n",
    "        description=\"A data source for Advanced RAG workshop\",\n",
    "        knowledgeBaseId=kb['knowledgeBaseId'],\n",
    "        dataSourceConfiguration={\n",
    "            \"type\": \"S3\",\n",
    "            \"s3Configuration\": s3_configuration\n",
    "        },\n",
    "        vectorIngestionConfiguration={\n",
    "            \"chunkingConfiguration\": chunking_strategy_configuration\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Store the created data source object\n",
    "    ds_fixed_chunk = create_ds_response[\"dataSource\"]\n",
    "    print(f\"Data source created successfully.\")\n",
    "    \n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'ConflictException':\n",
    "        print(f\"Data source '{data_source_name}' still exists. Retrieving it...\")\n",
    "        # Get the existing data source\n",
    "        list_ds_response = bedrock_agent.list_data_sources(\n",
    "            knowledgeBaseId=kb['knowledgeBaseId']\n",
    "        )\n",
    "        for ds in list_ds_response.get('dataSourceSummaries', []):\n",
    "            if ds['name'] == data_source_name:\n",
    "                ds_fixed_chunk = ds\n",
    "                print(f\"Retrieved existing data source: {ds['dataSourceId']}\")\n",
    "                break\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "# Print the data source information\n",
    "print(ds_fixed_chunk[\"dataSourceId\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322c1ddb-cfdc-4c54-a688-033e6967073b",
   "metadata": {},
   "source": [
    "### 3. Start Ingestion Job for Amazon Bedrock Knowledge base pointing to Amazon OpenSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b97c8d9",
   "metadata": {},
   "source": [
    "> **Note**: The ingestion process will take approximately 2-3 minutes to complete. During this time, the system is processing your documents by:\n",
    "> 1. Extracting text from the source files\n",
    "> 2. Chunking the content according to the defined strategy (Fixed / Semantic / Hierachical / Custom)\n",
    "> 3. Generating embeddings for each chunk\n",
    "> 4. Storing the embeddings and associated metadata in the OpenSearch vector database\n",
    ">\n",
    "> You'll see status updates as the process progresses. Please wait for the \"Ingestion job completed successfully\" message before proceeding to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49134993-3ce8-4da2-a5a5-9917c974c743",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion job started successfully.\n",
      "running...\n",
      "Job status: IN_PROGRESS\n",
      "running...\n",
      "Job status: IN_PROGRESS\n",
      "running...\n",
      "Job status: COMPLETE\n",
      "Ingestion job completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# List to keep track of all ingestion jobs\n",
    "ingest_jobs = []\n",
    "\n",
    "# Start an ingestion job for the data source\n",
    "try:\n",
    "    start_job_response = bedrock_agent.start_ingestion_job(\n",
    "        knowledgeBaseId=kb['knowledgeBaseId'],  # ID of the Knowledge Base\n",
    "        dataSourceId=ds_fixed_chunk[\"dataSourceId\"]  # ID of the associated data source\n",
    "    )\n",
    "    \n",
    "    # Extract job details\n",
    "    job = start_job_response[\"ingestionJob\"]\n",
    "    print(\"Ingestion job started successfully.\")\n",
    "\n",
    "    # Polling mechanism to check job status until it is complete\n",
    "    while job['status'] != 'COMPLETE':\n",
    "        # Sleep for a brief period to ensure the job is fully completed\n",
    "        print(\"running...\")\n",
    "        time.sleep(10)\n",
    "        get_job_response = bedrock_agent.get_ingestion_job(\n",
    "            knowledgeBaseId=kb['knowledgeBaseId'],  # ID of the Knowledge Base\n",
    "            dataSourceId=ds_fixed_chunk[\"dataSourceId\"],  # ID of the data source\n",
    "            ingestionJobId=job[\"ingestionJobId\"]  # ID of the running ingestion job\n",
    "        )\n",
    "        \n",
    "        # Update job status\n",
    "        job = get_job_response[\"ingestionJob\"]\n",
    "        print(f\"Job status: {job['status']}\")  # Log the current job status\n",
    "\n",
    "    print(\"Ingestion job completed successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error: Couldn't start ingestion job.\")\n",
    "    print(e)  # Print the exact error message for debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec8b006-b8af-46d9-b869-77fb08a00ee0",
   "metadata": {},
   "source": [
    "### 4. Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4256881d-7737-4d21-a007-7959b22e8fc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully retrieved relevant documents.\n",
      "[{'content': {'text': 'Category #Ex Acc     Ambiguous SELECT Column 171 90% Ambiguous WHERE Column 105 90% Ambiguous Filter Criteria 303 100% Ambiguous Values Within Column 122 80%     Nonexistent SELECT Column 482 95% Nonexistent WHERE Column 236 95% Unsupported Join 213 100% Nonexistent Filter Value 170 100%     Answerable (Spider Dev Set) 1034 100%     Total 2812 - Avg (excl. answerable) - 93.75%Table 4: Summary of Human Annotation Scores for Naturalness, Factuality, and Helpfulness.     Category Mean Std Krippendorff’s Alpha Naturalness 1.57 0.87 0.8207 Factuality 1.15 0.53 0.6829 Helpfulness 1.41 0.74 0.7602     4 Evaluation Task and Baselines     In this section, we describe the two evaluation tasks and corresponding metrics.     1. Question category classification: classify whether the question is answerable or one of the 8 ambiguous/unanswerable categories (9- way classification). We use classification ac- curacy for the ambiguous and unanswerable categories to measure the classification perfor- mance.     2. Clarification SQL Generation: predict the final SQL given the assistant’s clarification question and user’s clarification response. We use execution accuracy to measure the model performance (Li et al., 2024).     4.1 Question Category Classification We employ a few-shot prompting strategy for the question category classification task, experiment- ing with various numbers of shots (0-3) and dif- ferent LLMs via the litellm5 library as a baseline method. The prompt contains the definition of every category along with a variable number of in- context examples per category (see Prompt 9 & 11 for details). Each example includes an input com- prising the initial user question and relevant cell val- ues retrieved via a fuzzy matching approach, as de- scribed in (Lin et al., 2020; Wang et al., 2020) (de- noted by “lexicalOnly”). The in-context demonstra- tions contain human-curated step-by-step thoughts and classification of the question categories (Wei et al., 2022). To evaluate the impact of cell value retrieval on classification accuracy, we include a setting where oracle (perfect) cell values are pro- vided to the model (denoted by “lexicalAndOra- cle”). This setting allows us to assess how well the model performs if cell value retrieval is perfect.     4.2 SQL Prediction We use the DIN-SQL prompt-based framework, a SoTA method on the Spider dataset for predicting the final clarification SQL (Pourreza and Rafiei,     5https://github.com/BerriAI/litellm     Figure 2: Figure showing the classification accuracy of different models using different number of shots.     2024). The framework takes as input user ques- tions and the corresponding database schema and contains four modules that decompose the task of SQL generation into several sub-tasks following a chain-of-thought (Wei et al., 2022) approach for SQL generation.     5 Results and Discussions     Figure 2 shows the question category classification accuracy of different LLMs using varying numbers of examples. Claude 3.5 Sonnet6 achieves the best accuracy of 77.4% (75.9% excluding answerable category) across all categories when Oracle cell val- ues are included in the schema and 3 examples per question type are provided. Without oracle cell val- ues, the accuracy drops to 74.3% (72.4% excluding answerable). Mixtral-large-v27 performs similarly to Claude 3 Sonnet when at least 1 example is pro- vided per category but outperforms other models in the zero-shot setting, except Claude 3.5 Son- net. For the average accuracy across all categories, having lexical cell values improves performance by 0.7%, although the results are mixed. Across the three subcategories where cell values play a significant role (ambiguous VALUES within col- umn, ambiguous WHERE column, and ambiguous filter criteria), having oracle cell values boosts clas- sification accuracy by 1.5%.', 'type': 'TEXT'}, 'location': {'s3Location': {'uri': 's3://677276078734-us-east-1-advanced-rag-workshop/data/pdf_documents/practiq-a-practical-conversational-text-to-sql-dataset-with-ambiguous-and-unanswerable-queries.pdf'}, 'type': 'S3'}, 'metadata': {'x-amz-bedrock-kb-source-uri': 's3://677276078734-us-east-1-advanced-rag-workshop/data/pdf_documents/practiq-a-practical-conversational-text-to-sql-dataset-with-ambiguous-and-unanswerable-queries.pdf', 'x-amz-bedrock-kb-document-page-number': 6.0, 'year': 2025.0, 'docType': 'science', 'x-amz-bedrock-kb-data-source-id': 'T0YRYXWR89', 'company': 'Amazon', 'x-amz-bedrock-kb-chunk-id': '1%3A0%3AElllQpYBUFU5pE7mBhQ-', 'authors': ['Marvin Dong', 'Nischal Ashok Kumar', 'Yiqun Hu', 'Anuj Chauhan', 'Chung-Wei Hang', 'Shuaichen Chang', 'Lin Pan', 'Wuwei Lan', 'Henry Zhu', 'Jiarong Jiang', 'Patrick Ng', 'Zhiguo Wang']}, 'score': 0.42080534}, {'content': {'text': 'Did you ask a good     question? a cross-domain question intention classi- fication benchmark for text-to-sql. arXiv preprint arXiv:2010.12634.     Victor Zhong, Caiming Xiong, and Richard Socher. 2017. Seq2sql: Generating structured queries from natural language using reinforcement learning. arXiv preprint arXiv:1709.00103.           https://doi.org/10.18653/v1/2023.clinicalnlp-1.49         https://doi.org/10.18653/v1/2023.clinicalnlp-1.49         https://doi.org/10.18653/v1/2023.clinicalnlp-1.49         https://doi.org/10.18653/v1/D19-1204         https://doi.org/10.18653/v1/D19-1204         https://doi.org/10.18653/v1/D19-1204         https://doi.org/10.18653/v1/D18-1425         https://doi.org/10.18653/v1/D18-1425         https://doi.org/10.18653/v1/D18-1425         https://doi.org/10.18653/v1/2023.findings-emnlp.891         https://doi.org/10.18653/v1/2023.findings-emnlp.891A Dataset Examples     Table 6 and Table 7 show ambiguous and unanswer- able examples from our dataset respectively.     B Human Annotation     B.1 Question Category Classification For question category classification, we sampled 20 questions from each category and ask SQL experts to classify whether the category is correct or not given the pair of modified question and database (that includes values from the tables retrieved for the filter criteria) as input (binary classification). We employ 2 SQL experts for the question category classification annotation task. Each of the annota- tors has at least a bachelor’s degree in computer science. The annotators work as engineers/scien- tists in a private firm in the United States. The annotators performed their annotation task as a part of their service for which they were not specifically paid. To help with the annotations, we provide the definitions and a few examples of questions for each category.     Figure 3 shows the confusion matrix of the ques- tion category classification task of the human anno- tation. We see that in most cases the true label and the predicted labels are the same (diagonal entries in the matrix). Annotators classify ambiguous fil- ter criteria, ambiguous where column, non-existent select column, unsupported join, and answerable categories with high accuracy. Nonexistent filter value is often classified as answerable mostly be- cause annotators feel that the missing value is actu- ally present in the schema and might not have been retrieved in the example provided. On the contrary, some answerable data is classified as ambiguous filter criteria, as the filter values might not have been retrieved properly causing the annotators to believe that the data belongs to ambiguous filter criteria. Nonexistent Where Column data is some- times classified as Nonexistent Select Column as the annotators might believe that the column in the Select clause is missing for such examples. Am- biguous Values within Column is sometimes clas- sified as Nonexistent Filter Value indicating that the ambiguous cell values are not retrieved and the annotators believe that the exact value is missing even though the value can be similar to multiple values in the database. Ambiguous Values within Column is also sometimes classified as Answerable because the annotators might mistakenly believe that the value required to answer the question is     present in the database. Ambiguous Select Column is sometimes classified as answerable because the annotators might think that there exists another col- umn apart from the column that is removed which can be used to answer the user question.     B.2 Conversation Quality Evaluation We sampled 90 conversations across ambiguous, unanswerable, and answerable categories from dif- ferent databases for human annotation. Two SQL experts annotated each conversation on three crite- ria: factuality (correctness of SQL and natural lan- guage response), helpfulness (assistant’s responses in understanding user intent), and naturalness (con- versation flow) using a 1-5 Likert scale, where 1 denotes perfect/best quality and 5 denotes the worst quality.     Table 4 shows the mean, standard deviation, and Krippendorff’s Alpha for inter-annotator agree- ment. The high mean scores close to 1 (1.15-1.5) and substantial agreement (Alpha 0.68-0.82) indi- cate high-quality, natural conversations with factual and helpful responses.', 'type': 'TEXT'}, 'location': {'s3Location': {'uri': 's3://677276078734-us-east-1-advanced-rag-workshop/data/pdf_documents/practiq-a-practical-conversational-text-to-sql-dataset-with-ambiguous-and-unanswerable-queries.pdf'}, 'type': 'S3'}, 'metadata': {'x-amz-bedrock-kb-source-uri': 's3://677276078734-us-east-1-advanced-rag-workshop/data/pdf_documents/practiq-a-practical-conversational-text-to-sql-dataset-with-ambiguous-and-unanswerable-queries.pdf', 'x-amz-bedrock-kb-document-page-number': 10.0, 'year': 2025.0, 'docType': 'science', 'x-amz-bedrock-kb-data-source-id': 'T0YRYXWR89', 'company': 'Amazon', 'x-amz-bedrock-kb-chunk-id': '1%3A0%3AGFllQpYBUFU5pE7mBhQ-', 'authors': ['Marvin Dong', 'Nischal Ashok Kumar', 'Yiqun Hu', 'Anuj Chauhan', 'Chung-Wei Hang', 'Shuaichen Chang', 'Lin Pan', 'Wuwei Lan', 'Henry Zhu', 'Jiarong Jiang', 'Patrick Ng', 'Zhiguo Wang']}, 'score': 0.4160238}, {'content': {'text': 'The rephrased follow−up question should be clear , logical , and easy to understand , while avoiding unnecessary repetition of information from the initial conversation and technical jargon or complex words. Do not include unnecessary filler words like \"hey\" or \" hello \".     First , think step by step in <scratch /> XML tags. Then, write the rephrased concise follow−up question within the < result /> XML tags without any extra explanation .     Figure 7: System prompt for adding execution results explanation based on the SQL execution results.     You will be provided with a database schema containing multiple tables and their columns in a markdown format, along with 3 example values for each column enclosed within <schema/> XML tags.     After the schema, there will be a conversation between a USER and a DB EXPERT within <conversation/> XML tags. The conversation will include the USER\\'s query and the DB EXPERT\\'s SQL query to retrieve the requested information . At the end, the execution results of the DB EXPERT\\'s final SQL query will be presented within < execution_results /> XML tags.     Your task is to analyze the execution results and provide a short answer to the USER\\'s last question based on those results . The answer should be concise , self −contained , and no more than two sentences . Do not comment on the correctness of the query itself . Note that the short answer shall be self −explanatory becaues USER only sees the answer but not the execution results .     Your answer should be clear , logical , and easy for the USER to understand. Avoid using technical jargon or overly complex explanations . The goal is to provide a helpful and informative answer to the USER\\'s question.     Note that if the execution results contain more than 30 rows, only the first 30 rows will be shown, and the remaining rows will be skipped .     First , outline your thought process step by step within <scratch /> XML tags. Then, write the rephrased response within the < result /> XML tags without any additional explanation .     Figure 8: Binary classification Prompt for data filtering. \"{category_with_explanation}\" will be replaced with the name and definition of the corresponding question category in Figure 10. Few-shots examples are presented as conversation between the user and assistant in the format of message API of litellm.     You are a Database Expert (DB EXPERT) system that classifies user questions into one of the following two categories based on the given database schema:     { category_with_explanation }     − answerable: the database contains data needed to answer the question and the question has one and only one valid interpreation .     You will receive : 1. A database schema in markdown format with relevant column values enclosed in <schema/> tags . 2. The user \\' s question enclosed in <question/> tags .     Note that the \"answerable\" output shall only be provided if and only if : − the question posed is unambiguous, precise , and leaving no room for multiple interpretations or confusion . − the database contains the complete set of information required to formulate a comprehensive and accurate response to the query. If either of these conditions is not met, meaning the question lacks clarity or our data is insufficient , we shall refrain from classifying the query as \"answerable .\"     Provide your step−by−step thoughts within </ scratch > tags . Then, provide your final classification within < result /> tags as one of the categories above. Note that you result shall only be one of the categories specified at the beginning & Do not include any extra explanation in the result .Figure 9: Nine-way classification System Prompt. \"{category_with_explanation}\" will be replaced with the name and definition of four ambiguous and four unanswerable categories in Figure 10. Few-shots examples are presented as conversation between the user and assistant in the format of message API of litellm.     You are a Database Expert (DB EXPERT) system that classifies user questions into one of the following 9 categories based on the given database schema:     { category_with_explanation }     − answerable: the database contains data needed to answer the question and the question has one and only one valid interpreation .     You will receive : 1. A database schema in markdown format with relevant column values enclosed in <schema/> tags . 2. The user \\' s question enclosed in <question/> tags .     Your output should follow this format : <scratch> YOUR−STEP−BY−STEP−THOUGHTS </scratch> < result > ONE−OF−THE−9−QUESTION−CATEGORIES </result>     Note that the \"answerable\" output shall only be provided if and only if : − the question posed is unambiguous, precise , and leaving no room for multiple interpretations or confusion . − the database contains the complete set of information required to formulate a comprehensive and accurate response to the query.', 'type': 'TEXT'}, 'location': {'s3Location': {'uri': 's3://677276078734-us-east-1-advanced-rag-workshop/data/pdf_documents/practiq-a-practical-conversational-text-to-sql-dataset-with-ambiguous-and-unanswerable-queries.pdf'}, 'type': 'S3'}, 'metadata': {'x-amz-bedrock-kb-source-uri': 's3://677276078734-us-east-1-advanced-rag-workshop/data/pdf_documents/practiq-a-practical-conversational-text-to-sql-dataset-with-ambiguous-and-unanswerable-queries.pdf', 'x-amz-bedrock-kb-document-page-number': 15.0, 'year': 2025.0, 'docType': 'science', 'x-amz-bedrock-kb-data-source-id': 'T0YRYXWR89', 'company': 'Amazon', 'x-amz-bedrock-kb-chunk-id': '1%3A0%3AHlllQpYBUFU5pE7mBhQ-', 'authors': ['Marvin Dong', 'Nischal Ashok Kumar', 'Yiqun Hu', 'Anuj Chauhan', 'Chung-Wei Hang', 'Shuaichen Chang', 'Lin Pan', 'Wuwei Lan', 'Henry Zhu', 'Jiarong Jiang', 'Patrick Ng', 'Zhiguo Wang']}, 'score': 0.39568412}]\n",
      "\"Category #Ex Acc     Ambiguous SELECT Column 171 90% Ambiguous WHERE Column 105 90% Ambiguous Filter Criteria 303 100% Ambiguous Values Within Column 122 80%     Nonexistent SELECT Column 482 95% Nonexistent WHERE Column 236 95% Unsupported Join 213 100% Nonexistent Filter Value 170 100%     Answerable (Spider Dev Set) 1034 100%     Total 2812 - Avg (excl. answerable) - 93.75%Table 4: Summary of Human Annotation Scores for Naturalness, Factuality, and Helpfulness.     Category Mean Std Krippendorff\\u2019s Alpha Naturalness 1.57 0.87 0.8207 Factuality 1.15 0.53 0.6829 Helpfulness 1.41 0.74 0.7602     4 Evaluation Task and Baselines     In this section, we describe the two evaluation tasks and corresponding metrics.     1. Question category classification: classify whether the question is answerable or one of the 8 ambiguous/unanswerable categories (9- way classification). We use classification ac- curacy for the ambiguous and unanswerable categories to measure the classification perfor- mance.     2. Clarification SQL Generation: predict the final SQL given the assistant\\u2019s clarification question and user\\u2019s clarification response. We use execution accuracy to measure the model performance (Li et al., 2024).     4.1 Question Category Classification We employ a few-shot prompting strategy for the question category classification task, experiment- ing with various numbers of shots (0-3) and dif- ferent LLMs via the litellm5 library as a baseline method. The prompt contains the definition of every category along with a variable number of in- context examples per category (see Prompt 9 & 11 for details). Each example includes an input com- prising the initial user question and relevant cell val- ues retrieved via a fuzzy matching approach, as de- scribed in (Lin et al., 2020; Wang et al., 2020) (de- noted by \\u201clexicalOnly\\u201d). The in-context demonstra- tions contain human-curated step-by-step thoughts and classification of the question categories (Wei et al., 2022). To evaluate the impact of cell value retrieval on classification accuracy, we include a setting where oracle (perfect) cell values are pro- vided to the model (denoted by \\u201clexicalAndOra- cle\\u201d). This setting allows us to assess how well the model performs if cell value retrieval is perfect.     4.2 SQL Prediction We use the DIN-SQL prompt-based framework, a SoTA method on the Spider dataset for predicting the final clarification SQL (Pourreza and Rafiei,     5https://github.com/BerriAI/litellm     Figure 2: Figure showing the classification accuracy of different models using different number of shots.     2024). The framework takes as input user ques- tions and the corresponding database schema and contains four modules that decompose the task of SQL generation into several sub-tasks following a chain-of-thought (Wei et al., 2022) approach for SQL generation.     5 Results and Discussions     Figure 2 shows the question category classification accuracy of different LLMs using varying numbers of examples. Claude 3.5 Sonnet6 achieves the best accuracy of 77.4% (75.9% excluding answerable category) across all categories when Oracle cell val- ues are included in the schema and 3 examples per question type are provided. Without oracle cell val- ues, the accuracy drops to 74.3% (72.4% excluding answerable). Mixtral-large-v27 performs similarly to Claude 3 Sonnet when at least 1 example is pro- vided per category but outperforms other models in the zero-shot setting, except Claude 3.5 Son- net. For the average accuracy across all categories, having lexical cell values improves performance by 0.7%, although the results are mixed. Across the three subcategories where cell values play a significant role (ambiguous VALUES within col- umn, ambiguous WHERE column, and ambiguous filter criteria), having oracle cell values boosts clas- sification accuracy by 1.5%.\"\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize the Bedrock Agent Runtime client\n",
    "bedrock_agent_runtime = boto3.client(\"bedrock-agent-runtime\", region_name=variables[\"regionName\"])\n",
    "\n",
    "# Define the query for retrieving relevant documents\n",
    "query = 'What are three sub-tasks in question answering over knowledge bases?'\n",
    "\n",
    "try:\n",
    "    # Retrieve the top 3 most relevant documents from the knowledge base\n",
    "    relevant_documents_os = bedrock_agent_runtime.retrieve(\n",
    "        retrievalQuery={\n",
    "            'text': query  # Query text for document retrieval\n",
    "        },\n",
    "        knowledgeBaseId=kb['knowledgeBaseId'],  # ID of the Knowledge Base to search in\n",
    "        retrievalConfiguration={\n",
    "            'vectorSearchConfiguration': {\n",
    "                'numberOfResults': 3  # Fetch the top 3 most relevant documents\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Print the retrieved documents for debugging\n",
    "    print(\"Successfully retrieved relevant documents.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error: Unable to retrieve relevant documents.\")\n",
    "    print(e)  # Print the error details for debugging\n",
    "\n",
    "# Output the retrieved documents\n",
    "print(relevant_documents_os[\"retrievalResults\"])\n",
    "print(json.dumps([i[\"content\"][\"text\"] for i in relevant_documents_os[\"retrievalResults\"]][0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2641ae4b",
   "metadata": {},
   "source": [
    "> **Note**: After creating the knowledge base, you can explore its details and settings in the Amazon Bedrock console. This gives you a more visual interface to understand how the knowledge base is structured.\n",
    "> \n",
    "> **[➡️ View your Knowledge Bases in the AWS Console](https://us-west-2.console.aws.amazon.com/bedrock/home?region=us-west-2#/knowledge-bases)**\n",
    ">\n",
    "> In the console, you can:\n",
    "> - See all your knowledge bases in one place\n",
    "> - View ingestion status and statistics\n",
    "> - Test queries through the built-in chat interface\n",
    "> - Modify settings and configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5bc1cc-040b-462e-b27f-76657e891276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2440bd1-03ff-4ea9-b63b-efc2280e659b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a908760e-7cd2-42fe-82a1-6df7bb127912",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
