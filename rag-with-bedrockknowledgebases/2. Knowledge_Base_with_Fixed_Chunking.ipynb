{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aa74695-63c7-48a0-91b3-d3317035dd19",
   "metadata": {},
   "source": [
    "## Create a Knowledge Base with fixed chunking strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04e9f76",
   "metadata": {},
   "source": [
    "Chunking data is essential. If you are adding large documents with hundreds of pages to your knowledge base then you need to split them up and return only the relevant sections to use as context for your inference. If you are returning too much context it will increase costs (models charge based on input token count) and latency. It may also harm output quality. Shorter chunks will provide a better match but may lack the context necessary to answer a question.\n",
    "\n",
    "Bedrock Knowledge bases has a few different chunking strategies to choose from. They handle everything from splitting at semantic boundaries like paragraphs and hierarchical structures. However some document types can benefit from custom chunking. For example, any form of mark up can be used by a custom chunking approach.\n",
    "\n",
    "You can also create your own custom chunking approach using a Lambda function. If you want to add any custom metadata then you will need to add a Lambda function. You can either handle the chunking yourself, edit an existing chunk or just add metadata. Metadata can then be used for filtering.\n",
    "\n",
    "It is important to tune your chunking to the type of documents being ingested. Getting the wrong chunk size will affect the accuracy and response times. It will also increase the costs in both the vector storage and inference steps. The defaults supplied in Bedrock are pretty good but they may need tailored to your specific circumstances. Longer and more technical documents may need larger chunk sizes to make sure they include more context. Speech (like a chat transcript) can benefit from shorter chunks.\n",
    "\n",
    "![Chunking Strategies](./chunking-strategies.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e9c5e0",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this notebook, we will implement a knowledge base using a fixed chunking strategy. Here are the key steps we'll perform:\n",
    "\n",
    "1. **Create a Knowledge Base**: Set up an Amazon Bedrock Knowledge Base with fixed-size chunking configuration that will store and retrieve our vector embeddings.\n",
    "\n",
    "2. **Create a Data Source**: Connect our Knowledge Base to the documents we uploaded to S3 in the previous notebook.\n",
    "\n",
    "3. **Start Ingestion Job**: Begin the process of transforming our documents into chunks, creating embeddings, and storing them in our vector database.\n",
    "\n",
    "4. **Retrieve and Generate**: Test our Knowledge Base by retrieving relevant information based on a sample query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce312491-c1c5-4754-89bd-d28da5558005",
   "metadata": {},
   "source": [
    "#### Concept\n",
    "\n",
    "**Fixed Chunking**: Involves dividing your documents into fixed-size chunks, regardless of the content within them. Each chunk contains a predefined number of tokens or characters, and this method allows for more uniform data organization. \n",
    "\n",
    "Fixed chunking is useful when you want to ensure that your chunks are of a consistent size, making them easier to process and retrieve in a predictable manner. The document is split into sections of equal length, and each section becomes a separate chunk. This method works well when the content is relatively homogeneous, and the chunk boundaries are not as crucial to understanding the underlying context.\n",
    "\n",
    "#### Benefits\n",
    "\n",
    "- **Uniformity**: Each chunk has the same size, making the system more predictable. This helps with processing efficiency since you know that each chunk is of a consistent size, making batch operations and parallel processing easier.\n",
    "- **Simplified Retrieval**: Since the chunk sizes are uniform, searching through the data becomes straightforward. You can quickly determine the length of chunks, which can be useful for performance optimization and scalability in large datasets.\n",
    "- **Performance Optimization**: Fixed chunks are ideal when you want to control the computational cost of document retrieval and chunking. Having equal-sized chunks reduces the chance of computational bottlenecks in scenarios requiring large-scale document processing.\n",
    "\n",
    "> **Note:** While fixed chunking can be efficient for certain use cases, it may not preserve the natural semantic boundaries of the content, such as paragraphs or sections. This may lead to chunks that start or end at arbitrary places, potentially cutting off context in the middle of a sentence or idea.\n",
    "\n",
    "### **Best Use Cases**\n",
    "Fixed chunking is suitable for cases where:\n",
    "- **Homogeneous content**: The content is consistent, and boundaries are not as important.\n",
    "- **Performance**: You need uniform-sized chunks for predictable processing or optimization of large-scale systems.\n",
    "- **Simplified text processing**: When chunk boundaries do not need to match natural semantic structures like paragraphs or sentences.\n",
    "\n",
    "Examples include:\n",
    "- **General document indexing**: When large datasets are involved, and uniform chunk sizes optimize retrieval.\n",
    "- **Text summarization**: Fixed chunking is helpful when generating summaries from uniformly sized data pieces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b18e06c-98ef-41bc-99fd-d5e6c8977e4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load varibales json generated from prerequisites\n",
    "import json\n",
    "with open(\"./results/variables.json\", \"r\") as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaee1f0-65f2-4f23-80f4-767dcf78a459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# naming bedrock KB\n",
    "knowledge_base_name = \"advanced-rag-kbs_demo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ceaa5e-2ddd-4b97-853f-527c92326ebd",
   "metadata": {},
   "source": [
    "### 1. Create a Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9933328d-76dc-44bb-a985-591a61c47fca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from retrying import retry\n",
    "import boto3\n",
    "\n",
    "# Initialize the Bedrock Agent client using the provided AWS region\n",
    "bedrock_agent = boto3.client(\"bedrock-agent\", region_name=variables[\"regionName\"])\n",
    "\n",
    "# Retry decorator: If the function fails, it will retry up to 3 times with a random wait time between 1-2 seconds\n",
    "@retry(wait_random_min=1000, wait_random_max=2000, stop_max_attempt_number=3)\n",
    "def create_knowledge_base_func(name, description, chunking_type):\n",
    "    \"\"\"\n",
    "    Creates a knowledge base in Amazon Bedrock with OpenSearch Serverless as the vector store.\n",
    "    \n",
    "    Parameters:\n",
    "        name (str): The name of the knowledge base.\n",
    "        description (str): A description of the knowledge base.\n",
    "        chunking_type (str): The type of chunking strategy applied to vector indexing.\n",
    "\n",
    "    Returns:\n",
    "        dict: The response containing details of the created knowledge base.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the ARN of the embedding model used for vectorization\n",
    "    embedding_model_arn = f\"arn:aws:bedrock:{variables['regionName']}::foundation-model/amazon.titan-embed-text-v2:0\"\n",
    "\n",
    "    # Configure OpenSearch Serverless for vector storage\n",
    "    opensearch_serverless_configuration = {\n",
    "        \"collectionArn\": variables[\"collectionArn\"],  # ARN of the OpenSearch collection\n",
    "        # \"vectorIndexName\": variables[\"vectorIndexName\"] + chunking_type,  # Index name based on chunking strategy\n",
    "        \"vectorIndexName\": variables[\"vectorIndexName\"],  # Index name based on chunking strategy\n",
    "        \"fieldMapping\": {  # Define field mappings for vectors, text, and metadata\n",
    "            \"vectorField\": \"vector\",\n",
    "            \"textField\": \"text\",\n",
    "            \"metadataField\": \"text-metadata\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    print(opensearch_serverless_configuration)  # Print configuration for debugging\n",
    "\n",
    "    # Create the knowledge base in Amazon Bedrock\n",
    "    create_kb_response = bedrock_agent.create_knowledge_base(\n",
    "        name=name,\n",
    "        description=description,\n",
    "        roleArn=variables[\"bedrockExecutionRoleArn\"],  # IAM Role ARN for Bedrock execution\n",
    "        knowledgeBaseConfiguration={\n",
    "            \"type\": \"VECTOR\",\n",
    "            \"vectorKnowledgeBaseConfiguration\": {\n",
    "                \"embeddingModelArn\": embedding_model_arn  # Reference to the embedding model\n",
    "            }\n",
    "        },\n",
    "        storageConfiguration={\n",
    "            \"type\": \"OPENSEARCH_SERVERLESS\",\n",
    "            \"opensearchServerlessConfiguration\": opensearch_serverless_configuration\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return create_kb_response[\"knowledgeBase\"]  # Return the created knowledge base details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68035c04-af12-4ae4-a48d-2e6391bb662f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "try:\n",
    "    # Create a knowledge base using the predefined function\n",
    "    kb = create_knowledge_base_func(\n",
    "        name=knowledge_base_name+\"_2\",\n",
    "        description=\"Knowledge base using Amazon OpenSearch Service as a vector store\",\n",
    "        chunking_type=\"fixed\"\n",
    "    )\n",
    "\n",
    "    # Retrieve details of the newly created knowledge base\n",
    "    get_kb_response = bedrock_agent.get_knowledge_base(knowledgeBaseId=kb['knowledgeBaseId'])\n",
    "\n",
    "    # Update the variables dictionary with the new knowledge base ID\n",
    "    variables[\"kbFixedChunk\"] = kb['knowledgeBaseId']\n",
    "\n",
    "    # Save updated variables to a JSON file, handling datetime serialization\n",
    "    with open(\"./results/variables.json\", \"w\") as f:\n",
    "        json.dump(variables, f, indent=4, default=str)  # Convert datetime to string\n",
    "\n",
    "    # Print the retrieved knowledge base response in a readable format\n",
    "    print(f'OpenSearch Knowledge Response: {json.dumps(get_kb_response, indent=4, default=str)}')\n",
    "\n",
    "except Exception as e:\n",
    "    # Check if error message indicates the knowledge base already exists\n",
    "    error_message = str(e).lower()\n",
    "    if any(phrase in error_message for phrase in [\"already exist\", \"duplicate\", \"already been created\"]):\n",
    "        print(\"Knowledge Base already exists. Retrieving its ID...\")\n",
    "        \n",
    "        # List all knowledge bases to find the one that already exists\n",
    "        list_kb_response = bedrock_agent.list_knowledge_bases()\n",
    "        \n",
    "        # Look for a knowledge base with the desired name\n",
    "        for kb in list_kb_response.get('knowledgeBaseSummaries', []):\n",
    "            if kb['name'] == knowledge_base_name:\n",
    "                kb_id = kb['knowledgeBaseId']\n",
    "                print(f\"Found existing knowledge base with ID: {kb_id}\")\n",
    "                \n",
    "                # Get the details of the existing knowledge base\n",
    "                get_kb_response = bedrock_agent.get_knowledge_base(knowledgeBaseId=kb_id)\n",
    "                \n",
    "                # With this code that reads existing values first:\n",
    "                try:\n",
    "                    # Read existing variables\n",
    "                    with open(\"variables.json\", \"r\") as f:\n",
    "                        existing_variables = json.load(f)\n",
    "                except (FileNotFoundError, json.JSONDecodeError):\n",
    "                    # If file doesn't exist or is invalid JSON\n",
    "                    existing_variables = {}\n",
    "                \n",
    "                # Update only the semantic chunking value\n",
    "                existing_variables[\"kbFixedChunk\"] = kb_id\n",
    "                                \n",
    "                # Write back all variables\n",
    "                with open(\"variables.json\", \"w\") as f:\n",
    "                    json.dump(existing_variables, f, indent=4, default=str)\n",
    "                \n",
    "                # Print the retrieved knowledge base response\n",
    "                print(f'OpenSearch Knowledge Response: {json.dumps(get_kb_response, indent=4, default=str)}')\n",
    "                break\n",
    "        else:\n",
    "            print(\"Could not find a knowledge base with the specified name.\")\n",
    "    else:\n",
    "        # If it's a different error, re-raise it\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d6a826-f3c8-40b5-aaa2-0c2a9662d5dc",
   "metadata": {},
   "source": [
    "### 2. Create Datasources for Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb183c15-662d-498e-9df2-6814560f471a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Define the chunking strategy for data ingestion\n",
    "chunking_strategy_configuration = {\n",
    "    \"chunkingStrategy\": \"FIXED_SIZE\",\n",
    "    \"fixedSizeChunkingConfiguration\": {\n",
    "        \"maxTokens\": 1024,\n",
    "        \"overlapPercentage\": 20\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define the S3 bucket configuration for the data source\n",
    "s3_configuration = {\n",
    "    \"bucketArn\": f\"arn:aws:s3:::{variables['s3Bucket']}\",\n",
    "    \"inclusionPrefixes\": [\"data\"]  # Only include objects with the \"data\" prefix\n",
    "}\n",
    "\n",
    "data_source_name = \"advanced-rag-example\"\n",
    "\n",
    "# First, check if a data source with this name already exists in Bedrock (not just locally)\n",
    "try:\n",
    "    # List all data sources for the knowledge base\n",
    "    list_ds_response = bedrock_agent.list_data_sources(\n",
    "        knowledgeBaseId=kb['knowledgeBaseId']\n",
    "    )\n",
    "    \n",
    "    # Check if our named data source exists\n",
    "    existing_ds = None\n",
    "    for ds in list_ds_response.get('dataSourceSummaries', []):\n",
    "        if ds['name'] == data_source_name:\n",
    "            existing_ds = ds\n",
    "            break\n",
    "    \n",
    "    # If it exists, delete it\n",
    "    if existing_ds:\n",
    "        print(f\"Found existing data source '{data_source_name}'. Deleting it...\")\n",
    "        bedrock_agent.delete_data_source(\n",
    "            knowledgeBaseId=kb['knowledgeBaseId'],\n",
    "            dataSourceId=existing_ds[\"dataSourceId\"]\n",
    "        )\n",
    "        print(\"Waiting for data source deletion to complete...\")\n",
    "        time.sleep(10)\n",
    "        print(\"Data source deleted successfully.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error while checking or deleting data source: {e}\")\n",
    "\n",
    "# Now create a new data source\n",
    "try:\n",
    "    print(f\"Creating new data source '{data_source_name}'...\")\n",
    "    create_ds_response = bedrock_agent.create_data_source(\n",
    "        name=data_source_name,\n",
    "        description=\"A data source for Advanced RAG workshop\",\n",
    "        knowledgeBaseId=kb['knowledgeBaseId'],\n",
    "        dataSourceConfiguration={\n",
    "            \"type\": \"S3\",\n",
    "            \"s3Configuration\": s3_configuration\n",
    "        },\n",
    "        vectorIngestionConfiguration={\n",
    "            \"chunkingConfiguration\": chunking_strategy_configuration\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Store the created data source object\n",
    "    ds_fixed_chunk = create_ds_response[\"dataSource\"]\n",
    "    print(f\"Data source created successfully.\")\n",
    "    \n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'ConflictException':\n",
    "        print(f\"Data source '{data_source_name}' still exists. Retrieving it...\")\n",
    "        # Get the existing data source\n",
    "        list_ds_response = bedrock_agent.list_data_sources(\n",
    "            knowledgeBaseId=kb['knowledgeBaseId']\n",
    "        )\n",
    "        for ds in list_ds_response.get('dataSourceSummaries', []):\n",
    "            if ds['name'] == data_source_name:\n",
    "                ds_fixed_chunk = ds\n",
    "                print(f\"Retrieved existing data source: {ds['dataSourceId']}\")\n",
    "                break\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "# Print the data source information\n",
    "print(ds_fixed_chunk[\"dataSourceId\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322c1ddb-cfdc-4c54-a688-033e6967073b",
   "metadata": {},
   "source": [
    "### 3. Start Ingestion Job for Amazon Bedrock Knowledge base pointing to Amazon OpenSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b97c8d9",
   "metadata": {},
   "source": [
    "> **Note**: The ingestion process will take approximately 2-3 minutes to complete. During this time, the system is processing your documents by:\n",
    "> 1. Extracting text from the source files\n",
    "> 2. Chunking the content according to the defined strategy (Fixed / Semantic / Hierachical / Custom)\n",
    "> 3. Generating embeddings for each chunk\n",
    "> 4. Storing the embeddings and associated metadata in the OpenSearch vector database\n",
    ">\n",
    "> You'll see status updates as the process progresses. Please wait for the \"Ingestion job completed successfully\" message before proceeding to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49134993-3ce8-4da2-a5a5-9917c974c743",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# List to keep track of all ingestion jobs\n",
    "ingest_jobs = []\n",
    "\n",
    "# Start an ingestion job for the data source\n",
    "try:\n",
    "    start_job_response = bedrock_agent.start_ingestion_job(\n",
    "        knowledgeBaseId=kb['knowledgeBaseId'],  # ID of the Knowledge Base\n",
    "        dataSourceId=ds_fixed_chunk[\"dataSourceId\"]  # ID of the associated data source\n",
    "    )\n",
    "    \n",
    "    # Extract job details\n",
    "    job = start_job_response[\"ingestionJob\"]\n",
    "    print(\"Ingestion job started successfully.\")\n",
    "\n",
    "    # Polling mechanism to check job status until it is complete\n",
    "    while job['status'] != 'COMPLETE':\n",
    "        # Sleep for a brief period to ensure the job is fully completed\n",
    "        print(\"running...\")\n",
    "        time.sleep(10)\n",
    "        get_job_response = bedrock_agent.get_ingestion_job(\n",
    "            knowledgeBaseId=kb['knowledgeBaseId'],  # ID of the Knowledge Base\n",
    "            dataSourceId=ds_fixed_chunk[\"dataSourceId\"],  # ID of the data source\n",
    "            ingestionJobId=job[\"ingestionJobId\"]  # ID of the running ingestion job\n",
    "        )\n",
    "        \n",
    "        # Update job status\n",
    "        job = get_job_response[\"ingestionJob\"]\n",
    "        print(f\"Job status: {job['status']}\")  # Log the current job status\n",
    "\n",
    "    print(\"Ingestion job completed successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error: Couldn't start ingestion job.\")\n",
    "    print(e)  # Print the exact error message for debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec8b006-b8af-46d9-b869-77fb08a00ee0",
   "metadata": {},
   "source": [
    "### 4. Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4256881d-7737-4d21-a007-7959b22e8fc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize the Bedrock Agent Runtime client\n",
    "bedrock_agent_runtime = boto3.client(\"bedrock-agent-runtime\", region_name=variables[\"regionName\"])\n",
    "\n",
    "# Define the query for retrieving relevant documents\n",
    "query = 'What are three sub-tasks in question answering over knowledge bases?'\n",
    "\n",
    "try:\n",
    "    # Retrieve the top 3 most relevant documents from the knowledge base\n",
    "    relevant_documents_os = bedrock_agent_runtime.retrieve(\n",
    "        retrievalQuery={\n",
    "            'text': query  # Query text for document retrieval\n",
    "        },\n",
    "        knowledgeBaseId=kb['knowledgeBaseId'],  # ID of the Knowledge Base to search in\n",
    "        retrievalConfiguration={\n",
    "            'vectorSearchConfiguration': {\n",
    "                'numberOfResults': 3  # Fetch the top 3 most relevant documents\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Print the retrieved documents for debugging\n",
    "    print(\"Successfully retrieved relevant documents.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error: Unable to retrieve relevant documents.\")\n",
    "    print(e)  # Print the error details for debugging\n",
    "\n",
    "# Output the retrieved documents\n",
    "print(relevant_documents_os[\"retrievalResults\"])\n",
    "print(json.dumps([i[\"content\"][\"text\"] for i in relevant_documents_os[\"retrievalResults\"]][0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2641ae4b",
   "metadata": {},
   "source": [
    "> **Note**: After creating the knowledge base, you can explore its details and settings in the Amazon Bedrock console. This gives you a more visual interface to understand how the knowledge base is structured.\n",
    "> \n",
    "> **[➡️ View your Knowledge Bases in the AWS Console](https://us-west-2.console.aws.amazon.com/bedrock/home?region=us-west-2#/knowledge-bases)**\n",
    ">\n",
    "> In the console, you can:\n",
    "> - See all your knowledge bases in one place\n",
    "> - View ingestion status and statistics\n",
    "> - Test queries through the built-in chat interface\n",
    "> - Modify settings and configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5bc1cc-040b-462e-b27f-76657e891276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2440bd1-03ff-4ea9-b63b-efc2280e659b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a908760e-7cd2-42fe-82a1-6df7bb127912",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
