{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3afd887e-afe6-4dc8-8f45-328c1c0a86ef",
   "metadata": {},
   "source": [
    "# Retrieval and Generation with Bedrock Foundational Models\n",
    "\n",
    "### Overview  \n",
    "This notebook demonstrates how to perform retrieval-augmented generation (RAG) using Amazon Bedrock's foundational models. It covers retrieving relevant documents from a knowledge base and generating responses based on the retrieved context.\n",
    "\n",
    "### Build your own Retrieval Augmented Generation (RAG) system\n",
    "When constructing your own retrieval augmented generation (RAG) system, you can leverage a retriever system and a generator system. The retriever can be an embedding model that identifies the relevant chunks from the vector database based on similarity scores. The generator can be a Large Language Model (LLM) that utilizes the model's capability to answer questions based on the retrieved results (also known as chunks). In the following sections, we will provide additional tips on how to optimize the prompts for your RAG system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427d8982-69f2-4705-b402-590a0252e5b2",
   "metadata": {},
   "source": [
    "# üîç Retrieval in Flotorch\n",
    "\n",
    "[Flotorch](https://www.flotorch.ai/) is a real-time Retrieval-Augmented Generation (RAG) orchestration engine designed to streamline operational complexity and enhance observability in deploying AI workflows.\n",
    "\n",
    "In Flotorch, **retrieval** refers to the process of fetching relevant information from external knowledge bases to augment the responses generated by language models. This ensures that the AI system provides accurate, timely, and context-aware answers by combining its pre-trained knowledge with up-to-date external data.\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Key Components of Retrieval in Flotorch\n",
    "\n",
    "1. **Retriever**  \n",
    "   Searches external databases or knowledge sources to find relevant information based on the user's query.\n",
    "\n",
    "2. **Augmentation**  \n",
    "   Incorporates the retrieved data into the model's input to enhance the quality and relevance of the generated response.\n",
    "\n",
    "3. **Generator**  \n",
    "   Synthesizes a response by integrating the retrieved information with the model's existing knowledge.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Benefits of Retrieval in Flotorch\n",
    "\n",
    "- **Enhanced Accuracy**  \n",
    "  Accesses real-time data to minimize the risk of outdated or incorrect information.\n",
    "\n",
    "- **Contextual Understanding**  \n",
    "  Provides responses that are tailored to the specific query, ensuring relevance and usefulness.\n",
    "\n",
    "- **Scalability**  \n",
    "  Efficiently handles large datasets and complex queries.\n",
    "\n",
    "- **Cost-Effectiveness**  \n",
    "  Reduces the need for frequent retraining by dynamically pulling in fresh data.\n",
    "\n",
    "---\n",
    "\n",
    "This retrieval mechanism is integral to Flotorch's ability to deliver precise and context-aware AI solutions across various industries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c114e690-2fef-4dc8-8a37-f909e512dc56",
   "metadata": {},
   "source": [
    "## üîß Step 1: load aws variables created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5381fe1f-8477-421a-83ea-a498f1780662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accountNumber': '677276078734',\n",
       " 'regionName': 'us-east-1',\n",
       " 'collectionArn': 'arn:aws:aoss:us-east-1:677276078734:collection/h4x23xd1thd0kpl13b67',\n",
       " 'collectionId': 'h4x23xd1thd0kpl13b67',\n",
       " 'vectorIndexName': 'ws-index-fixed',\n",
       " 'bedrockExecutionRoleArn': 'arn:aws:iam::677276078734:role/advanced-rag-workshop-bedrock_execution_role-us-east-1',\n",
       " 's3Bucket': '677276078734-us-east-1-advanced-rag-workshop',\n",
       " 's3_ground_truth_path': 's3://677276078734-us-east-1-advanced-rag-workshop/ground_truth_data/kbqa_questions_answers.json',\n",
       " 'kbFixedChunk': 'TJSZIWHAIM'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open(\"./results/variables.json\", \"r\") as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fb3777-e1ba-4686-9a76-7faed4fd9f8b",
   "metadata": {},
   "source": [
    "## Load Prompt json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5093f2d-ff0b-44d1-8dee-5415e039a49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_file_path = './data/prompt.json'\n",
    "with open(prompt_file_path, 'r') as f:\n",
    "    prompt = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a199fd02-48ac-42d4-ad88-96682f3e3e01",
   "metadata": {},
   "source": [
    "## Sample experiment JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19762bd0-5991-4b6a-8f95-2017e8b786f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_config_data = {\n",
    "            \"temp_retrieval_llm\": \"0.1\",\n",
    "            \"gt_data\": variables[\"s3_ground_truth_path\"],\n",
    "            \"rerank_model_id\": \"none\",\n",
    "            \"embedding_model\": \"amazon.titan-embed-text-v2:0\",\n",
    "            \"bedrock_knowledge_base\": True,\n",
    "            \"kb_data\": variables['kbFixedChunk'],\n",
    "            \"retrieval_service\": \"sagemaker\",\n",
    "            \"knn_num\": \"3\",\n",
    "            \"knowledge_base\": True,\n",
    "            \"retrieval_model\": \"meta-textgeneration-llama-3-1-8b-instruct\",\n",
    "            \"index_id\": variables['vectorIndexName'],\n",
    "            \"gateway_api_key\": \"\",\n",
    "            \"vector_dimension\": \"1024\",\n",
    "            \"gateway_enabled\": False,\n",
    "            \"gateway_url\": \"\",\n",
    "            \"chunking_strategy\": \"Fixed\",\n",
    "            \"aws_region\": \"us-east-1\",\n",
    "            \"n_shot_prompt_guide_obj\": prompt,\n",
    "            \"n_shot_prompts\": 1\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f53614-8184-4a99-b5c0-a1dcbce28428",
   "metadata": {},
   "source": [
    "## üîç Load env config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21ed30e6-fe7b-49c4-b46e-97728c584837",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flotorch_core.config.env_config_provider import EnvConfigProvider\n",
    "from flotorch_core.config.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60ad8cc3-758f-4487-bffd-7de23fda906c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config_provider = EnvConfigProvider()\n",
    "config = Config(env_config_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12371288-c445-42e2-baee-8ac5ee3863aa",
   "metadata": {},
   "source": [
    "### Load Retriver function and other dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd348a6b-7fd2-4b11-ba90-f2a1a4a73cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "/Users/fl_lpt-301/Documents/projects/crag/crag_ravi/CRAG/crag_env/lib/python3.10/site-packages/pydantic/_internal/_fields.py:192: UserWarning: Field name \"json\" in \"MonitoringDatasetFormat\" shadows an attribute in parent \"Base\"\n",
      "  warnings.warn(\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/fl_lpt-301/Library/Application Support/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from flotorch_core.storage.storage_provider_factory import StorageProviderFactory\n",
    "from flotorch_core.reader.json_reader import JSONReader\n",
    "from flotorch_core.storage.db.vector.vector_storage_factory import VectorStorageFactory\n",
    "from flotorch_core.inferencer.inferencer_provider_factory import InferencerProviderFactory\n",
    "from flotorch_core.embedding.embedding_registry import embedding_registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19275dfc",
   "metadata": {},
   "source": [
    "### Initialize storage provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d922ecf0-a40f-4c19-8785-cea9e01670e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_data = exp_config_data['gt_data']\n",
    "storage = StorageProviderFactory.create_storage_provider(gt_data)\n",
    "gt_data_path = storage.get_path(gt_data)\n",
    "json_reader = JSONReader(storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f36e755",
   "metadata": {},
   "source": [
    "### Setting embedding to None if bedrock KB is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80bb60cb-63d0-4213-b997-77a1f7441566",
   "metadata": {},
   "outputs": [],
   "source": [
    "if exp_config_data.get(\"knowledge_base\", False) and not exp_config_data.get(\"bedrock_knowledge_base\", False):\n",
    "    embedding_class = embedding_registry.get_model(exp_config_data.get(\"embedding_model\"))\n",
    "    embedding = embedding_class(\n",
    "        exp_config_data.get(\"embedding_model\"), \n",
    "        exp_config_data.get(\"aws_region\"), \n",
    "        int(exp_config_data.get(\"vector_dimension\")))\n",
    "    is_opensearch_required = True\n",
    "else:\n",
    "    embedding = None\n",
    "    is_opensearch_required = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd97d675-b399-4761-b7a2-b0c197241c39",
   "metadata": {},
   "source": [
    "## üóÉÔ∏è Vector Storage Initialization\n",
    "\n",
    "This section initializes the `VectorStorage` component using a factory method that dynamically selects the appropriate vector storage backend (e.g., OpenSearch, Bedrock Knowledge Base) based on the experimental configuration.\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è `VectorStorageFactory.create_vector_storage(...)`\n",
    "\n",
    "Creates an instance of vector storage using configuration flags and credentials.\n",
    "\n",
    "- **Parameters:**\n",
    "  - `knowledge_base`: *(bool)* ‚Äì Whether a knowledge base is used as a backend.\n",
    "  - `use_bedrock_kb`: *(bool)* ‚Äì If set, uses AWS Bedrock Knowledge Base.\n",
    "  - `embedding`: *(BaseEmbedding)* ‚Äì Embedding generator to use for vector creation.\n",
    "  - `opensearch_host`: *(str | None)* ‚Äì OpenSearch host (set if required).\n",
    "  - `opensearch_port`: *(int | None)* ‚Äì OpenSearch port (set if required).\n",
    "  - `opensearch_username`: *(str | None)* ‚Äì OpenSearch authentication username.\n",
    "  - `opensearch_password`: *(str | None)* ‚Äì OpenSearch authentication password.\n",
    "  - `index_id`: *(str | None)* ‚Äì Identifier for the index to be used.\n",
    "  - `knowledge_base_id`: *(str | None)* ‚Äì ID of the Bedrock knowledge base.\n",
    "  - `aws_region`: *(str | None)* ‚Äì AWS region for Bedrock and related services.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Dynamic Backend Selection\n",
    "\n",
    "The factory method chooses the backend as follows:\n",
    "\n",
    "- If `bedrock_knowledge_base` is enabled ‚Üí connects to **Bedrock KB**.\n",
    "- Else if `knowledge_base` is enabled ‚Üí connects to **custom knowledge base**.\n",
    "- Else if `is_opensearch_required` is true ‚Üí initializes **OpenSearch** with provided credentials.\n",
    "\n",
    "---\n",
    "\n",
    "### üìù Result\n",
    "\n",
    "Returns a configured `VectorStorage` instance ready for:\n",
    "- KNN-based vector search\n",
    "- Bedrock KB search\n",
    "- Integration into QA or retrieval pipelines\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6ad13e",
   "metadata": {},
   "source": [
    "### Initialize vector storage with configuration for embedding and optional OpenSearch/Bedrock KB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79351c77-52b5-4238-8b7c-a33dae9f880d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_storage = VectorStorageFactory.create_vector_storage(\n",
    "                knowledge_base=exp_config_data.get(\"knowledge_base\", False),\n",
    "                use_bedrock_kb=exp_config_data.get(\"bedrock_knowledge_base\", False),\n",
    "                embedding=embedding,\n",
    "                opensearch_host=config.get_opensearch_host() if is_opensearch_required else None,\n",
    "                opensearch_port=config.get_opensearch_port() if is_opensearch_required else None,\n",
    "                opensearch_username='admin',\n",
    "                opensearch_password='Flotorch@123',\n",
    "                index_id=exp_config_data.get(\"index_id\"),\n",
    "                knowledge_base_id=exp_config_data.get(\"kb_data\"),\n",
    "                aws_region=exp_config_data.get(\"aws_region\")\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e2d0ee-4675-4afc-8218-1c2790bf480f",
   "metadata": {},
   "source": [
    "## ü§ñ Inferencer Initialization\n",
    "\n",
    "This block initializes the **Inferencer** using a factory method that configures the inference engine for text generation or question answering based on the experimental setup.\n",
    "\n",
    "---\n",
    "\n",
    "### üèóÔ∏è `InferencerProviderFactory.create_inferencer_provider(...)`\n",
    "\n",
    "Creates and returns an appropriate `Inferencer` instance depending on configuration such as API gateway usage, model settings, region, and credentials.\n",
    "\n",
    "---\n",
    "\n",
    "### üîß Parameters\n",
    "\n",
    "- `gateway_enabled`: *(bool)* ‚Äì Enables API gateway-based invocation if set to `True`.\n",
    "- `gateway_url`: *(str)* ‚Äì URL endpoint for the API Gateway (e.g., `/api/openai/v1`).\n",
    "- `gateway_api_key`: *(str)* ‚Äì API key for authenticating requests to the gateway.\n",
    "- `retrieval_service`: *(str)* ‚Äì Name of the retrieval service (e.g., Bedrock, sagemaker).\n",
    "- `retrieval_model`: *(str)* ‚Äì The model to use for inference (e.g., `anthropic.claude-v2`).\n",
    "- `aws_region`: *(str)* ‚Äì AWS region for service provisioning (e.g., `us-east-1`).\n",
    "- `iam_role`: *(str)* ‚Äì IAM role ARN for Bedrock invocation permissions.\n",
    "- `n_shot_prompts`: *(int)* ‚Äì Number of few-shot examples to include in prompt.\n",
    "- `temp_retrieval_llm`: *(float)* ‚Äì Temperature setting for the language model.\n",
    "- `n_shot_prompt_guide_obj`: *(Any)* ‚Äì Few-shot guide object for prompt engineering.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Behavior\n",
    "\n",
    "- If `gateway_enabled` is `True`, connects to the specified API Gateway using credentials.\n",
    "- If disabled, falls back to direct model invocation through supported services like AWS Bedrock.\n",
    "- Supports dynamic few-shot prompting and custom temperature configuration.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Outcome\n",
    "\n",
    "Returns a fully configured `Inferencer` object capable of generating answers or completions for queries using the selected language model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe940888",
   "metadata": {},
   "source": [
    "### Initialize inferencer provider with configuration for gateway, retrieval service, and AWS integration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bddfc720-b994-41e2-8491-35881547f325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 18:38:20,726 - INFO - Initializing SageMaker Generator for model: meta-textgeneration-llama-3-1-8b-instruct\n",
      "INFO:default:Initializing SageMaker Generator for model: meta-textgeneration-llama-3-1-8b-instruct\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint 'meta-textgeneration-llama-3-1-8b-instruct-inferencing-endpoint' does not exist.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model 'meta-textgeneration-llama-3-1-8b-instruct' requires accepting end-user license agreement (EULA). See https://jumpstart-cache-prod-us-east-1.s3.us-east-1.amazonaws.com/fmhMetadata/eula/llama3_1Eula.txt for terms of use.\n",
      "INFO:sagemaker.jumpstart:Model 'meta-textgeneration-llama-3-1-8b-instruct' requires accepting end-user license agreement (EULA). See https://jumpstart-cache-prod-us-east-1.s3.us-east-1.amazonaws.com/fmhMetadata/eula/llama3_1Eula.txt for terms of use.\n",
      "Using model 'meta-textgeneration-llama-3-1-8b-instruct' with wildcard version identifier '*'. You can pin to version '2.7.2' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "WARNING:sagemaker.jumpstart:Using model 'meta-textgeneration-llama-3-1-8b-instruct' with wildcard version identifier '*'. You can pin to version '2.7.2' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "No instance type selected for inference hosting endpoint. Defaulting to ml.g5.4xlarge.\n",
      "INFO:sagemaker.jumpstart:No instance type selected for inference hosting endpoint. Defaulting to ml.g5.4xlarge.\n",
      "2025-04-16 18:38:27,621 - INFO - Endpoint configuration 'meta-textgeneration-llama-3-1-8b-instruct-inferencing-endpoint' does not exist. Deploying using model.deploy().\n",
      "INFO:default:Endpoint configuration 'meta-textgeneration-llama-3-1-8b-instruct-inferencing-endpoint' does not exist. Deploying using model.deploy().\n",
      "INFO:sagemaker:Creating model with name: llama-3-1-8b-instruct-2025-04-16-13-08-26-585\n",
      "INFO:sagemaker:Creating endpoint-config with name meta-textgeneration-llama-3-1-8b-instruct-inferencing-endpoint\n",
      "INFO:sagemaker:Creating endpoint with name meta-textgeneration-llama-3-1-8b-instruct-inferencing-endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 18:47:38,998 - INFO - Successfully created endpoint 'meta-textgeneration-llama-3-1-8b-instruct-inferencing-endpoint' for model 'meta-textgeneration-llama-3-1-8b-instruct'.\n",
      "INFO:default:Successfully created endpoint 'meta-textgeneration-llama-3-1-8b-instruct-inferencing-endpoint' for model 'meta-textgeneration-llama-3-1-8b-instruct'.\n",
      "2025-04-16 18:47:39,002 - INFO - Waiting for endpoint 'meta-textgeneration-llama-3-1-8b-instruct-inferencing-endpoint' to be in service...\n",
      "INFO:default:Waiting for endpoint 'meta-textgeneration-llama-3-1-8b-instruct-inferencing-endpoint' to be in service...\n",
      "2025-04-16 18:47:40,316 - INFO - Endpoint 'meta-textgeneration-llama-3-1-8b-instruct-inferencing-endpoint' status: InService\n",
      "INFO:default:Endpoint 'meta-textgeneration-llama-3-1-8b-instruct-inferencing-endpoint' status: InService\n",
      "2025-04-16 18:47:40,318 - INFO - Endpoint 'meta-textgeneration-llama-3-1-8b-instruct-inferencing-endpoint' is now in service.\n",
      "INFO:default:Endpoint 'meta-textgeneration-llama-3-1-8b-instruct-inferencing-endpoint' is now in service.\n"
     ]
    }
   ],
   "source": [
    "inferencer = InferencerProviderFactory.create_inferencer_provider(\n",
    "                exp_config_data.get(\"gateway_enabled\", False),\n",
    "                f'{exp_config_data.get(\"gateway_url\", \"\")}/api/openai/v1',\n",
    "                exp_config_data.get(\"gateway_api_key\", \"\"),\n",
    "                exp_config_data.get(\"retrieval_service\"),\n",
    "                exp_config_data.get(\"retrieval_model\"),\n",
    "                exp_config_data.get(\"aws_region\"),\n",
    "                variables.get('bedrockExecutionRoleArn', 'arn:aws:iam::677276078734:role/flotorch-bedrock-role-qamain'),\n",
    "                # 'arn:aws:iam::677276078734:role/flotorch-bedrock-role-qamain',\n",
    "                int(exp_config_data.get(\"n_shot_prompts\", 0)), \n",
    "                float(exp_config_data.get(\"temp_retrieval_llm\", 0)), \n",
    "                exp_config_data.get(\"n_shot_prompt_guide_obj\")\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e74bb4-5639-4216-a1d1-2b2f0d830e3b",
   "metadata": {},
   "source": [
    "## üîÅ Reranker Initialization\n",
    "\n",
    "This code conditionally initializes the **`BedrockReranker`**, which reorders retrieved documents based on relevance using a reranking model.\n",
    "\n",
    "---\n",
    "\n",
    "### üèóÔ∏è `BedrockReranker(...)` Initialization\n",
    "\n",
    "The reranker is only instantiated if a valid rerank model ID is provided in the experiment configuration.\n",
    "\n",
    "---\n",
    "\n",
    "### üîß Parameters\n",
    "\n",
    "- `aws_region`: *(str)* ‚Äì AWS region where the Bedrock reranking model is hosted.\n",
    "- `rerank_model_id`: *(str)* ‚Äì ID of the Bedrock reranking model to be used.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Behavior\n",
    "\n",
    "- If `rerank_model_id` is **not** `\"none\"` (case-insensitive), a `BedrockReranker` is created.\n",
    "- If the value is `\"none\"`, no reranker is used and the value is set to `None`.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Outcome\n",
    "\n",
    "- A `BedrockReranker` object if reranking is enabled.\n",
    "- Otherwise, `reranker = None`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a20966",
   "metadata": {},
   "source": [
    "### Initialize reranker if a valid rerank model ID is provided in the configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38800c24-aa54-4d67-afcb-0fd5c41c4bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker = BedrockReranker(exp_config_data.get(\"aws_region\"), exp_config_data.get(\"rerank_model_id\")) \\\n",
    "                if exp_config_data.get(\"rerank_model_id\").lower() != \"none\" \\\n",
    "                else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c335b9f7",
   "metadata": {},
   "source": [
    "### Load ground truth data in JSON reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7e2cd3-41d7-42a6-a588-5f7d06a5fcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read ground truth json\n",
    "from pydantic import BaseModel\n",
    "from flotorch_core.chunking.chunking import Chunk\n",
    "class Question(BaseModel):\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "    def get_chunk(self) -> Chunk:\n",
    "        return Chunk(data=self.question)\n",
    "\n",
    "questions_list = json_reader.read_as_model(gt_data_path, Question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0992f6f5",
   "metadata": {},
   "source": [
    "### ü§ñ Perform vector search for each question chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8c5c53-9308-4878-8cfc-378e718418c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hierarchical = exp_config_data.get(\"chunking_strategy\") == 'hierarchical'\n",
    "\n",
    "responses_list = []\n",
    "for question in questions_list:\n",
    "    question_chunk = question.get_chunk()\n",
    "    vector_response = vector_storage.search(question_chunk, int(exp_config_data.get(\"knn_num\")), hierarchical)\n",
    "    vector_response_result = vector_response.to_json()['result']\n",
    "    responses_list.append({'question':question, 'question_chunk':question_chunk, 'vector_response':vector_response, 'vector_response_result':vector_response_result, 'response_status':vector_response.status})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aba61c",
   "metadata": {},
   "source": [
    "### üîÅ Rerank vector responses using the reranker if enabled and response is valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a955930e-6e00-44e4-919d-6d6bb0953b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for each_response in responses_list:\n",
    "    response_status = each_response['response_status']\n",
    "    vector_response_result = each_response['vector_response_result']\n",
    "    if reranker and response_status:\n",
    "        vector_response = reranker.rerank_documents(each_response['question_chunk'].data, vector_response_result)\n",
    "        each_response['vector_response'] = vector_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38cd325",
   "metadata": {},
   "source": [
    "### üß† Generate answers and extract metadata for each response, applying guardrail checks if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c739274-0df4-4708-b5e4-400a088cb928",
   "metadata": {},
   "outputs": [],
   "source": [
    "for each_response in responses_list:\n",
    "    response_status = each_response['response_status']\n",
    "    if response_status:\n",
    "        question = each_response['question']\n",
    "        vector_response = each_response['vector_response']\n",
    "        vector_response_result = each_response['vector_response_result']\n",
    "        metadata, answer = inferencer.generate_text(question.question, vector_response_result)\n",
    "        guardrail_blocked = metadata['guardrail_blocked'] if 'guardrail_blocked' in metadata else False\n",
    "        if guardrail_blocked:\n",
    "            answer_metadata = {}\n",
    "        else:\n",
    "            answer_metadata = metadata\n",
    "    else:\n",
    "        answer = metadata['guardrail_output']\n",
    "        metadata = {}\n",
    "        answer_metadata = {}\n",
    "        guardrail_blocked = vector_response.metadata['guardrail_blocked'] if 'guardrail_blocked' in vector_response.metadata else False\n",
    "    each_response['metadata'] = metadata\n",
    "    each_response['answer'] = answer\n",
    "    each_response['answer_metadata'] = answer_metadata\n",
    "    each_response['guardrail_blocked'] = guardrail_blocked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932195d0",
   "metadata": {},
   "source": [
    "### üì¶ Aggregate final results with question, answer, guardrail assessments, and reference context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eeaedf-be15-4f47-9ac1-af1023ea8d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for each_response in responses_list:\n",
    "    metadata = each_response['metadata']\n",
    "    vector_response = each_response['vector_response']\n",
    "    vector_response_result = each_response['vector_response_result']\n",
    "    # print(\"Hello\")\n",
    "    # print(each_response['question'])\n",
    "    result.append(\n",
    "                {'question':each_response['question'].question,\n",
    "                'answer':each_response['answer'],\n",
    "                'guardrails_output_assessment':metadata['guardrail_output_assessment'] if 'guardrail_output_assessment' in metadata else None,\n",
    "                'guardrails_context_assessment':vector_response.metadata['guardrail_context_assessment'] if 'guardrail_context_assessment' in vector_response.metadata else None,\n",
    "                'guardrails_input_assessment':vector_response.metadata['guardrail_input_assessment'] if 'guardrail_input_assessment' in vector_response.metadata else None,\n",
    "                'guardrails_blocked':each_response['guardrail_blocked'],\n",
    "                'guardrails_block_level':vector_response.metadata['block_level'] if 'block_level' in vector_response.metadata else \"\",\n",
    "                'answer_metadata':each_response['answer_metadata'],\n",
    "                'reference_contexts':[res['text'] for res in vector_response_result] if vector_response_result else [],\n",
    "                'gt_answer':each_response['question'].answer,\n",
    "                'query_metadata':vector_response.metadata['embedding_metadata'].to_json() if 'embedding_metadata' in vector_response.metadata else None\n",
    "                })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5927f7f-4c63-471d-981b-0fdbce7d8c4a",
   "metadata": {},
   "source": [
    "### üì¶ Calculate Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11541a9-f6b5-43a8-a6ed-567374784ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.cost_calculation import calculate_total_cost\n",
    "total_cost, results = calculate_total_cost(exp_config_data, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84674286",
   "metadata": {},
   "source": [
    "### üíæ Save the aggregated results to a JSON file for inference metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc91fa0-5952-4d83-8372-ed46d47dd91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./results/{exp_config_data['retrieval_service']}_inference_metrics.json\", \"w\") as json_file:\n",
    "    json.dump(results, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f72648-1282-4fe0-b78b-6b9a1bd7dcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01458e5e-7103-4c68-957a-c67e2213e20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "csv_file = './results/evaluation_output.csv'\n",
    "\n",
    "# Check if 'sagemaker_cost' exists in any item\n",
    "include_sagemaker_cost = any('sagemaker_cost' in item for item in results)\n",
    "include_inference_cost = any('inference_cost' in item for item in results)\n",
    "\n",
    "fieldnames=['question', 'answer', 'inputTokens', 'outputTokens', 'totalTokens', 'latencyMs', 'ground answer','message','score']\n",
    "\n",
    "if include_sagemaker_cost:\n",
    "    fieldnames.insert(fieldnames.index('message'), 'sagemaker_cost')  # Insert before 'ground answer'\n",
    "\n",
    "if include_inference_cost:\n",
    "    fieldnames.insert(fieldnames.index('message'), 'bedrock_input_cost')  # Insert before 'ground answer'\n",
    "    fieldnames.insert(fieldnames.index('message'), 'bedrock_output_cost')  # Insert before 'ground answer'\n",
    "    \n",
    "\n",
    "with open(csv_file, mode='w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for _id, item in enumerate(results):\n",
    "        answer_metadata = item.get('answer_metadata', {})\n",
    "        response = item.get('response', {})\n",
    "\n",
    "        row = {\n",
    "            'question': item.get('question', ''),\n",
    "            'answer': item.get('answer', ''),\n",
    "            'inputTokens': answer_metadata.get('inputTokens', ''),\n",
    "            'outputTokens': answer_metadata.get('outputTokens', ''),\n",
    "            'totalTokens': answer_metadata.get('totalTokens', ''),\n",
    "            'latencyMs': answer_metadata.get('latencyMs', ''),\n",
    "            'ground answer': item.get('gt_answer', ''),\n",
    "            'message': response.get('message', ''),\n",
    "            'score': response.get('score', ''),\n",
    "        }\n",
    "\n",
    "        if include_sagemaker_cost:\n",
    "            sagemaker_cost = item.get('sagemaker_cost', {})\n",
    "            row['sagemaker_cost'] = sagemaker_cost.get('sagemaker_inference_cost', '')\n",
    "        if include_inference_cost:\n",
    "            inference_cost = item.get('inference_cost', {})\n",
    "            row['bedrock_input_cost'] = inference_cost.get('inference_input_cost', '')\n",
    "            row['bedrock_output_cost'] = inference_cost.get('inference_output_cost', '')\n",
    "\n",
    "        writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a903173-61d3-4bca-b423-cf92d3c0e5b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2fa652-dd59-4f5e-9b73-2af0b79020d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
