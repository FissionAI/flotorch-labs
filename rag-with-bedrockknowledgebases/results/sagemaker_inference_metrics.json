[
    {
        "question": "What are the three main sub-tasks in Knowledge Base Question Answering (KBQA) as identified in the paper?",
        "answer": "The three main sub-tasks in Knowledge Base Question Answering (KBQA) as identified in the paper are: 1) Topic entity detection, 2) Entity linking, and 3) Relation detection. These sub-tasks are sequential and are used to find answers for natural language questions from structural knowledge bases (KB). The paper proposes a retrieve-and-rerank framework to access KBs via traditional information retrieval (IR) method, and re-rank retrieved candidates with more powerful neural networks such as the pre-trained BERT model. The unified model is then trained under a multi-task learning framework. The three sub-tasks are: 1) Topic entity detection: identifying the topic entity from the question, 2) Entity linking: linking the topic entity to an entity node from a list of candidate nodes, and 3) Relation detection: selecting the relation-chain leading to the final answer. These sub-tasks are essential for KBQA and are used to find answers for natural language questions from structural knowledge bases (KB). The paper proposes a unified model that shares a BERT encoder across all three sub-tasks and defines task-specific layers on top of the shared layer. The unified model is then trained under a multi-task learning framework.",
        "guardrails_output_assessment": null,
        "guardrails_context_assessment": null,
        "guardrails_input_assessment": null,
        "guardrails_blocked": false,
        "guardrails_block_level": "",
        "answer_metadata": {
            "inputTokens": 3516,
            "outputTokens": 313,
            "totalTokens": 3829,
            "latencyMs": 11089
        },
        "reference_contexts": [
            "We acknowledge that this method does not consider the already covered semantics in the first retrieval step, when we do the second step retrieval. Since the main goal of the retrieval step is to collect a list of high-quality candidates, we will perform better semantic matching in the re-ranking step with more powerful neural networks. If multi-hop relation- chains are needed, we can iterate this process until reaching the maximum steps. Usually, the number of max-hop is pre-computed on the target question sets. Another way is to utilize a model to decide when to stop (Chen et al., 2019b), however we will leave this option in the future work.     After collecting a list of relation-chains, we leverage another BERT model to compute the similarity between a question Q and each relation- chain r. Each pair of Q and r will be represented as a sequence of tokens with the format \u201c[CLS] question [SEP] topic-entity name [SEP] relation chain [SEP] answer name [SEP] answer types [SEP]\u201d, where topic-entity name is the name for the linked entity node, relation chain is the word sequence of a candidate relation-chain3, answer name is the name of the trailing node in the relation-chain, and answer types are all types of the trailing node. The hidden vector for the [CLS] token will be fed into a linear layer (with one output neuron) to predict the similarity between Q and r.     5 Multi-Task Learning for KBQA     5.1 Training Objectives     For the topic entity detection model, we define the objective function as the cross-entropy loss be- tween true distributions and predicted distributions. We sum up the cross-entropy losses for both start and end models, and average over all N training instances:     L(\u03b8t) = \u2212 1     N     N\u2211 i=1     log(P i s) + log(P i     e) (3)     where \u03b8t is the trainable parameter for topic entity detection model.     3A relation-chain is split into a word sequence based on delimiters such as periods, hyphens and underscores.     Both entity linking and relation detection tasks are ranking tasks, therefore we leverage a hinge loss function for both tasks:     L(\u03b8) = \u2212 1     N     N\u2211 i=1     max(0, l+ s(Q, c\u2212)\u2212 s(Q, c+)     (4) Where \u03b8 is the trainable parameter, l is a margin, s(Q, c) can be the model of Pl or Pr, c+ is a correct candidate, and c\u2212 is an incorrect candidate. We set l = 1.0 in this work.     5.2 Multi-Task Learning A naive approach would be to use three different BERT encoders for the topic entity detection, entity linking and relation detection sub-tasks individu- ally. Since BERT model is a very large model, it is expensive to host three BERT models in real ap- plications. To address this, we propose to share a BERT encoder across all three sub-tasks, and define lean layers for each individual sub-task on top of the shared layer. This unified model is then trained under the multi-task learning framework proposed by Liu et al. (2019). First, training instances for each sub-tasks are packed into mini-batches sep- arately. At the beginning of each training epoch, mini-batches from all three sub-tasks are mixed together and randomly shuffled. During training, a mini-batch is selected, and the model is updated according to the task-specific objective for the se- lected mini-batch.     6 Experiments     We evaluate the effectiveness of our model on stan- dard benchmarks in this section. We first conduct experiments on each sub-task with a separate BERT model in Section 6.2, 6.3 and 6.4, then evaluate the influence of sharing a BERT encoder for all three models in Section 6.5. Finally, we benchmark our method on full Freebase in Section 6.6.     6.1 Datasets and Basic Settings We evaluate our proposed model on two large-scale benchmarks: SimpleQuestions and FreebaseQA. Other existing datasets, such as WebQuestions (Be- rant et al., 2013), Free917 (Cai and Yates, 2013) and WebQSP (Yih et al., 2016), are not considered, because they only contain few thousands of ques- tions which is even less than the number of relation types in Freebase.     SimpleQuestions: The SimpleQuestions dataset (Bordes et al., 2015) is so far the largestKBQA dataset. It consists of 108,442 English questions written by human annotators, and all questions can be answered by 1-hop relation chains in Freebase. Each question is annotated with a gold-standard subject-relation-object triple from Freebase.",
            "Previous methods usually worked on a subset of KB in order to fit KB into memory. For entity linking, some sophisticated heuristics were com- monly used to collect entity candidates. For rela- tion detection, previous work usually enumerated all possible 1-hop and 2-hop relation-chains (start- ing from linked entity nodes) as candidates. AllWho wrote the book Beau Geste ?     MID name type     m.04wxy8 Beau Geste book     m.0dl_h4 Beau Geste film     m.051vvdc Beau Geste music     \u2026 \u2026 \u2026     (2) Entity linking     predicate object MID     book.written_work.author m.05f834     book.written_work.subjects m.0322m     book.book.genre m.05hgj     \u2026 \u2026     (3) Relation detection     (1) Topic entity detection     Node candidates for entity linking. Relation-chain candidates for relation detection.     Figure 1: A typical workflow for KBQA. Given a question \u201cWho wrote the book Beau Geste?\u201d, the topic entity detection model first identifies a topic entity \u201cBeau Geste\u201d from the question. Then, the entity linking model links the topic entity into an entity node (m.04wxy8) in the KB. Finally, the relation book.written work.author is selected as the relation-chain leading to the final answer node (m.05f834).     these workarounds may prevent their methods from generalizing well to other datasets, and scaling up to bigger KBs.     To tackle these issues, we leverage a retrieve- and-rerank strategy to access KBs. In the retrieval step, we ingest KBs into two inverted indices: one that stores all entity nodes for entity linking, and the other one that stores all subject-predicate-object triples for relation detection. Then, we use TF-IDF algorithm to retrieve candidates for both entity link- ing and relation detection sub-tasks. This method naturally overcomes the memory overhead when dealing with large-scale KBs, therefore makes our method easily scale up to large-scale tasks. In the re-ranking step, we leverage the advanced BERT model to re-rank all candidates by fine-grained se- mantic matching. For the topic entity detection sub-task, we utilize another BERT model to predict the start and end positions of a topic entity within a question. Since assigning a different BERT model for each sub-task may incur prohibitive costs, we therefore propose to share a BERT encoder across sub-tasks and define task-specific layers for each individual sub-task on top of the shared layer. This unified BERT model is then trained under the multi- task learning framework. Experiments on two stan- dard benchmarks show that: (1) Our IR-based re- trieval method is able to collect high-quality candi- dates efficiently; (2) the BERT model improves the accuracy across all three sub-tasks; and (3) bene- fiting from multi-task learning, the unified model obtains further improvements with only 1/3 of the original parameters. Our final model achieves com- petitive results on the SimpleQuestions dataset and superior performance on the FreebaseQA dataset.     2 Task Definition     Knowledge-base question answering (KBQA) aims to find answers for natural language questions from structural knowledge bases (KB). We assume a KB K is a collection of subject-predicate-object triples \u3008e1, p, e2\u3009, where e1, e2 \u2208 E are entities, and p \u2208 P is a relation type between two entities, E is the set of all entities, and P is the set of all relation types. Given a question Q, the goal of KBQA is to find an entity node a \u2208 E from the KB as the final answer, thus can be formulated as     a\u0302 = argmax a\u2208E     Pr(a|Q,K) (1)     where Pr(a|Q,K) is the probability of a to be the answer for Q. A general purpose KB usually contains millions of entities in E and billions of relations in K (Bollacker et al., 2008), therefore directly modeling Pr(a|Q,K) is challenging. Pre- vious studies usually factorize this model in dif- ferent ways. One line of research forms KBQA as a semantic parsing task Pr(q|Q,K) to parse a question Q directly into a logical form query q, and execute the query q over KB to derive the final answer. Another line of studies views KBQA as a semantic matching task, and finds a relation-chain within KB that is similar to the question in a com- mon semantic space. Then the trailing entity of the relation-chain is taken as the final answer.",
            "Retrieval, Re-ranking and Multi-task Learning for Knowledge-Base Question Answering     Zhiguo Wang, Patrick Ng, Ramesh Nallapati, Bing Xiang AWS AI Labs     {zhiguow, patricng, rnallapa, bxiang}@amazon.com     Abstract     Question answering over knowledge bases (KBQA) usually involves three sub-tasks, namely topic entity detection, entity linking and relation detection. Due to the large num- ber of entities and relations inside knowledge bases (KB), previous work usually utilized so- phisticated rules to narrow down the search space and managed only a subset of KBs in memory. In this work, we leverage a retrieve- and-rerank framework to access KBs via tradi- tional information retrieval (IR) method, and re-rank retrieved candidates with more pow- erful neural networks such as the pre-trained BERT model. Considering the fact that di- rectly assigning a different BERT model for each sub-task may incur prohibitive costs, we propose to share a BERT encoder across all three sub-tasks and define task-specific layers on top of the shared layer. The unified model is then trained under a multi-task learning frame- work. Experiments show that: (1) Our IR- based retrieval method is able to collect high- quality candidates efficiently, thus enables our method adapt to large-scale KBs easily; (2) the BERT model improves the accuracy across all three sub-tasks; and (3) benefiting from multi- task learning, the unified model obtains fur- ther improvements with only 1/3 of the origi- nal parameters. Our final model achieves com- petitive results on the SimpleQuestions dataset and superior performance on the FreebaseQA dataset.     1 Introduction     Answering natural language questions by search- ing over large-scale knowledge bases (KBQA) is highly demanded by real-life applications, such as Google Assistant, Siri, and Alexa. Owing to the availability of large-scale KBs, significant ad- vancements have been made over the years. One main research direction views KBQA as a seman- tic matching task (Bordes et al., 2014; Dong et al.,     2015; Dai et al., 2016; Hao et al., 2017; Mohammed et al., 2018; Yu et al., 2018; Wu et al., 2019; Chen et al., 2019a; Petrochuk and Zettlemoyer, 2018), and finds a relation-chain within KBs that is most similar to the question in a common semantic space, where the relation-chain can be 1-hop, 2-hop or multi-hop (Chen et al., 2019b). Another research direction formulates KBQA as a semantic parsing task (Berant et al., 2013; Bao et al., 2016; Luo et al., 2018), and tackles questions that involve complex reasoning, such as ordinal (e.g. What is the sec- ond largest fulfillment center of Amazon?), and aggregation (e.g. How many fulfillment centers does Amazon have?). Most recently, some stud- ies proposed to derive answers from both KBs and free-text corpus to deal with the low-coverage issue of KBs (Xu et al., 2016; Sun et al., 2018; Xiong et al., 2019; Sun et al., 2019). In this paper, we follow the first research direction since the relation- chain type of questions counts the vast majority of real-life questions (Berant et al., 2013; Bordes et al., 2015; Jiang et al., 2019).     Previous semantic matching methods for KBQA usually decompose the task into sequential sub- tasks consisting of topic entity detection, entity linking, and relation detection. For example in Fig- ure 1, given the question \u201cWho wrote the book Beau Geste?\u201d, a KBQA system first identifies the topic entity \u201cBeau Geste\u201d from the question, then the topic entity is linked to an entity node (m.04wxy8) from a list of candidate nodes, and finally the relation book.written work.author is se- lected as the relation-chain leading to the final an- swer. Previous methods usually worked on a subset of KB in order to fit KB into memory. For entity linking, some sophisticated heuristics were com- monly used to collect entity candidates. For rela- tion detection, previous work usually enumerated all possible 1-hop and 2-hop relation-chains (start- ing from linked entity nodes) as candidates. AllWho wrote the book Beau Geste ?"
        ],
        "gt_answer": "The three main sub-tasks in KBQA are topic entity detection, entity linking, and relation detection.",
        "query_metadata": {
            "input_token": 0,
            "latency_ms": 0
        },
        "sagemaker_cost": {
            "sagemaker_inference_cost": 0.003727136111111111
        }
    },
    {
        "question": "How does the proposed method handle large-scale knowledge bases efficiently?",
        "answer": "Sorry, I don't have sufficient information to provide an answer. There is no need to explain the reasoning behind your answers.",
        "guardrails_output_assessment": null,
        "guardrails_context_assessment": null,
        "guardrails_input_assessment": null,
        "guardrails_blocked": false,
        "guardrails_block_level": "",
        "answer_metadata": {
            "inputTokens": 3375,
            "outputTokens": 32,
            "totalTokens": 3407,
            "latencyMs": 1912
        },
        "reference_contexts": [
            "of relation-chains per question. Comparing the \u201cIR\u201d columns with \u201cAll\u201d columns, our IR-based method retrieves fewer relation-chains but maintains better recall.     Re-ranking step: We feed top-100 relation- chain candidates from the retrieval step into our relation detection model Pr to re-rank all candi- dates. Table 6 shows the results from previous state-of-the-art models as well as our relation de- tection model Pr. We can see that our Pr model obtains very competitive results on the SimpleQues- tions dataset, and outperforms previous models by a large margin in the FreebaseQA dataset. We also create a model PtPlPr to multiply scores from our topic entity detection model, entity linking model and relation detection model. By considering the in- fluence of all three components, our PtPlPr model achieves even better accuracy on the FreebaseQA dataset.     6.5 Multi-task Learning Experiments     Our method achieves very strong performance by leveraging three BERT encoders for each model component. In this section, we share a BERT en- coder for all three models, and jointly train the unified model with the multi-task learning method described in Section 5.2. Experimental results from this model are shown in rows with the prefix \u201cMulti- task\u201d in Table 1, 3, 4, and 6. Although the multi- task model only has about 1/3 of the original pa- rameters, it is able to achieve better end-to-end accuracy in Table 6, and retain similar performance as before on the other two sub-tasks.     6.6 KBQA over Full Freebase     Most of the previous studies conducted KBQA ex- periments with a subset of Freebase, because it is     Models SimpleQ. FreebaseQA     Dai et al. (2016) 75.7 N/A Yin et al. (2016) 76.4 N/A Yu et al. (2017) 77.0 N/A Wu et al. (2019) 77.3 37.0 Hao et al. (2018) 80.2 N/A Petrochuk (2018) 78.1 N/A     Pr 79.4 45.4 PtPlPr 79.4 49.1     Multi-task Pr 79.7 47.9 Multi-task PtPlPr 79.7 51.7     Full Freebase 74.1 35.4     Table 6: Relation detection accuracy in the end-to-end manner.     hard to fit the full Freebase into memory (Bordes et al., 2014; Dong et al., 2015). Our method ingests Freebase into inverted indices on hard disk storage, thus naturally overcomes the memory overhead. This advantage enables us to evaluate our method on the full Freebase. The last rows of Table 3, 4, and 6 show the results of running our \u201cMulti-task\u201d model over the full Freebase. Significant degra- dations are observed in entity linking and relation detection tasks on both datasets. This phenomenon reveals that previous studies may overestimate the capacity of their KBQA models. We suggest that researchers evaluate their models on the full Free- base in the future.     7 Conclusion     In this work, we proposed a retrieve-and-rerank strategy to access large-scale KBs in two steps. First, we leveraged traditional IR methods to col- lect high-quality candidates from KBs for entity linking and relation detection. Second, we utilized the advanced BERT model to re-rank candidates by fine-grained semantic matching. We also employed a BERT model to predict the start and end posi- tions of the topic entity in a question. To reduce the model size, we proposed a joint model to share BERT encoder across all three sub-tasks, and create task-specific layers on the top. We then trained this joint model with multi-task learning. Experimental results show that our method achieves superior re- sults on standard benchmarks, and is able to scale up to large-scale KBs.References Junwei Bao, Nan Duan, Zhao Yan, Ming Zhou, and     Tiejun Zhao. 2016. Constraint-based question an- swering with knowledge graph. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2503\u20132514, Osaka, Japan. The COLING 2016 Organizing Committee.     Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Lan- guage Processing, pages 1533\u20131544.",
            "Retrieval, Re-ranking and Multi-task Learning for Knowledge-Base Question Answering     Zhiguo Wang, Patrick Ng, Ramesh Nallapati, Bing Xiang AWS AI Labs     {zhiguow, patricng, rnallapa, bxiang}@amazon.com     Abstract     Question answering over knowledge bases (KBQA) usually involves three sub-tasks, namely topic entity detection, entity linking and relation detection. Due to the large num- ber of entities and relations inside knowledge bases (KB), previous work usually utilized so- phisticated rules to narrow down the search space and managed only a subset of KBs in memory. In this work, we leverage a retrieve- and-rerank framework to access KBs via tradi- tional information retrieval (IR) method, and re-rank retrieved candidates with more pow- erful neural networks such as the pre-trained BERT model. Considering the fact that di- rectly assigning a different BERT model for each sub-task may incur prohibitive costs, we propose to share a BERT encoder across all three sub-tasks and define task-specific layers on top of the shared layer. The unified model is then trained under a multi-task learning frame- work. Experiments show that: (1) Our IR- based retrieval method is able to collect high- quality candidates efficiently, thus enables our method adapt to large-scale KBs easily; (2) the BERT model improves the accuracy across all three sub-tasks; and (3) benefiting from multi- task learning, the unified model obtains fur- ther improvements with only 1/3 of the origi- nal parameters. Our final model achieves com- petitive results on the SimpleQuestions dataset and superior performance on the FreebaseQA dataset.     1 Introduction     Answering natural language questions by search- ing over large-scale knowledge bases (KBQA) is highly demanded by real-life applications, such as Google Assistant, Siri, and Alexa. Owing to the availability of large-scale KBs, significant ad- vancements have been made over the years. One main research direction views KBQA as a seman- tic matching task (Bordes et al., 2014; Dong et al.,     2015; Dai et al., 2016; Hao et al., 2017; Mohammed et al., 2018; Yu et al., 2018; Wu et al., 2019; Chen et al., 2019a; Petrochuk and Zettlemoyer, 2018), and finds a relation-chain within KBs that is most similar to the question in a common semantic space, where the relation-chain can be 1-hop, 2-hop or multi-hop (Chen et al., 2019b). Another research direction formulates KBQA as a semantic parsing task (Berant et al., 2013; Bao et al., 2016; Luo et al., 2018), and tackles questions that involve complex reasoning, such as ordinal (e.g. What is the sec- ond largest fulfillment center of Amazon?), and aggregation (e.g. How many fulfillment centers does Amazon have?). Most recently, some stud- ies proposed to derive answers from both KBs and free-text corpus to deal with the low-coverage issue of KBs (Xu et al., 2016; Sun et al., 2018; Xiong et al., 2019; Sun et al., 2019). In this paper, we follow the first research direction since the relation- chain type of questions counts the vast majority of real-life questions (Berant et al., 2013; Bordes et al., 2015; Jiang et al., 2019).     Previous semantic matching methods for KBQA usually decompose the task into sequential sub- tasks consisting of topic entity detection, entity linking, and relation detection. For example in Fig- ure 1, given the question \u201cWho wrote the book Beau Geste?\u201d, a KBQA system first identifies the topic entity \u201cBeau Geste\u201d from the question, then the topic entity is linked to an entity node (m.04wxy8) from a list of candidate nodes, and finally the relation book.written work.author is se- lected as the relation-chain leading to the final an- swer. Previous methods usually worked on a subset of KB in order to fit KB into memory. For entity linking, some sophisticated heuristics were com- monly used to collect entity candidates. For rela- tion detection, previous work usually enumerated all possible 1-hop and 2-hop relation-chains (start- ing from linked entity nodes) as candidates. AllWho wrote the book Beau Geste ?",
            "Previous methods usually worked on a subset of KB in order to fit KB into memory. For entity linking, some sophisticated heuristics were com- monly used to collect entity candidates. For rela- tion detection, previous work usually enumerated all possible 1-hop and 2-hop relation-chains (start- ing from linked entity nodes) as candidates. AllWho wrote the book Beau Geste ?     MID name type     m.04wxy8 Beau Geste book     m.0dl_h4 Beau Geste film     m.051vvdc Beau Geste music     \u2026 \u2026 \u2026     (2) Entity linking     predicate object MID     book.written_work.author m.05f834     book.written_work.subjects m.0322m     book.book.genre m.05hgj     \u2026 \u2026     (3) Relation detection     (1) Topic entity detection     Node candidates for entity linking. Relation-chain candidates for relation detection.     Figure 1: A typical workflow for KBQA. Given a question \u201cWho wrote the book Beau Geste?\u201d, the topic entity detection model first identifies a topic entity \u201cBeau Geste\u201d from the question. Then, the entity linking model links the topic entity into an entity node (m.04wxy8) in the KB. Finally, the relation book.written work.author is selected as the relation-chain leading to the final answer node (m.05f834).     these workarounds may prevent their methods from generalizing well to other datasets, and scaling up to bigger KBs.     To tackle these issues, we leverage a retrieve- and-rerank strategy to access KBs. In the retrieval step, we ingest KBs into two inverted indices: one that stores all entity nodes for entity linking, and the other one that stores all subject-predicate-object triples for relation detection. Then, we use TF-IDF algorithm to retrieve candidates for both entity link- ing and relation detection sub-tasks. This method naturally overcomes the memory overhead when dealing with large-scale KBs, therefore makes our method easily scale up to large-scale tasks. In the re-ranking step, we leverage the advanced BERT model to re-rank all candidates by fine-grained se- mantic matching. For the topic entity detection sub-task, we utilize another BERT model to predict the start and end positions of a topic entity within a question. Since assigning a different BERT model for each sub-task may incur prohibitive costs, we therefore propose to share a BERT encoder across sub-tasks and define task-specific layers for each individual sub-task on top of the shared layer. This unified BERT model is then trained under the multi- task learning framework. Experiments on two stan- dard benchmarks show that: (1) Our IR-based re- trieval method is able to collect high-quality candi- dates efficiently; (2) the BERT model improves the accuracy across all three sub-tasks; and (3) bene- fiting from multi-task learning, the unified model obtains further improvements with only 1/3 of the original parameters. Our final model achieves com- petitive results on the SimpleQuestions dataset and superior performance on the FreebaseQA dataset.     2 Task Definition     Knowledge-base question answering (KBQA) aims to find answers for natural language questions from structural knowledge bases (KB). We assume a KB K is a collection of subject-predicate-object triples \u3008e1, p, e2\u3009, where e1, e2 \u2208 E are entities, and p \u2208 P is a relation type between two entities, E is the set of all entities, and P is the set of all relation types. Given a question Q, the goal of KBQA is to find an entity node a \u2208 E from the KB as the final answer, thus can be formulated as     a\u0302 = argmax a\u2208E     Pr(a|Q,K) (1)     where Pr(a|Q,K) is the probability of a to be the answer for Q. A general purpose KB usually contains millions of entities in E and billions of relations in K (Bollacker et al., 2008), therefore directly modeling Pr(a|Q,K) is challenging. Pre- vious studies usually factorize this model in dif- ferent ways. One line of research forms KBQA as a semantic parsing task Pr(q|Q,K) to parse a question Q directly into a logical form query q, and execute the query q over KB to derive the final answer. Another line of studies views KBQA as a semantic matching task, and finds a relation-chain within KB that is similar to the question in a com- mon semantic space. Then the trailing entity of the relation-chain is taken as the final answer."
        ],
        "gt_answer": "The method uses an IR-based retrieval approach to collect high-quality candidates efficiently, enabling adaptation to large-scale KBs.",
        "query_metadata": {
            "input_token": 0,
            "latency_ms": 0
        },
        "sagemaker_cost": {
            "sagemaker_inference_cost": 0.0006426444444444444
        }
    },
    {
        "question": "What is the role of BERT in the proposed KBQA framework?",
        "answer": "The BERT model is used in the proposed KBQA framework for semantic matching tasks, specifically for re-ranking candidates in the retrieval step and for predicting the start and end positions of a topic entity within a question in the topic entity detection sub-task. Additionally, a unified BERT model is proposed to share a BERT encoder across sub-tasks and define task-specific layers for each individual sub-task on top of the shared layer, which is trained under the multi-task learning framework.",
        "guardrails_output_assessment": null,
        "guardrails_context_assessment": null,
        "guardrails_input_assessment": null,
        "guardrails_blocked": false,
        "guardrails_block_level": "",
        "answer_metadata": {
            "inputTokens": 3511,
            "outputTokens": 125,
            "totalTokens": 3636,
            "latencyMs": 4502
        },
        "reference_contexts": [
            "We acknowledge that this method does not consider the already covered semantics in the first retrieval step, when we do the second step retrieval. Since the main goal of the retrieval step is to collect a list of high-quality candidates, we will perform better semantic matching in the re-ranking step with more powerful neural networks. If multi-hop relation- chains are needed, we can iterate this process until reaching the maximum steps. Usually, the number of max-hop is pre-computed on the target question sets. Another way is to utilize a model to decide when to stop (Chen et al., 2019b), however we will leave this option in the future work.     After collecting a list of relation-chains, we leverage another BERT model to compute the similarity between a question Q and each relation- chain r. Each pair of Q and r will be represented as a sequence of tokens with the format \u201c[CLS] question [SEP] topic-entity name [SEP] relation chain [SEP] answer name [SEP] answer types [SEP]\u201d, where topic-entity name is the name for the linked entity node, relation chain is the word sequence of a candidate relation-chain3, answer name is the name of the trailing node in the relation-chain, and answer types are all types of the trailing node. The hidden vector for the [CLS] token will be fed into a linear layer (with one output neuron) to predict the similarity between Q and r.     5 Multi-Task Learning for KBQA     5.1 Training Objectives     For the topic entity detection model, we define the objective function as the cross-entropy loss be- tween true distributions and predicted distributions. We sum up the cross-entropy losses for both start and end models, and average over all N training instances:     L(\u03b8t) = \u2212 1     N     N\u2211 i=1     log(P i s) + log(P i     e) (3)     where \u03b8t is the trainable parameter for topic entity detection model.     3A relation-chain is split into a word sequence based on delimiters such as periods, hyphens and underscores.     Both entity linking and relation detection tasks are ranking tasks, therefore we leverage a hinge loss function for both tasks:     L(\u03b8) = \u2212 1     N     N\u2211 i=1     max(0, l+ s(Q, c\u2212)\u2212 s(Q, c+)     (4) Where \u03b8 is the trainable parameter, l is a margin, s(Q, c) can be the model of Pl or Pr, c+ is a correct candidate, and c\u2212 is an incorrect candidate. We set l = 1.0 in this work.     5.2 Multi-Task Learning A naive approach would be to use three different BERT encoders for the topic entity detection, entity linking and relation detection sub-tasks individu- ally. Since BERT model is a very large model, it is expensive to host three BERT models in real ap- plications. To address this, we propose to share a BERT encoder across all three sub-tasks, and define lean layers for each individual sub-task on top of the shared layer. This unified model is then trained under the multi-task learning framework proposed by Liu et al. (2019). First, training instances for each sub-tasks are packed into mini-batches sep- arately. At the beginning of each training epoch, mini-batches from all three sub-tasks are mixed together and randomly shuffled. During training, a mini-batch is selected, and the model is updated according to the task-specific objective for the se- lected mini-batch.     6 Experiments     We evaluate the effectiveness of our model on stan- dard benchmarks in this section. We first conduct experiments on each sub-task with a separate BERT model in Section 6.2, 6.3 and 6.4, then evaluate the influence of sharing a BERT encoder for all three models in Section 6.5. Finally, we benchmark our method on full Freebase in Section 6.6.     6.1 Datasets and Basic Settings We evaluate our proposed model on two large-scale benchmarks: SimpleQuestions and FreebaseQA. Other existing datasets, such as WebQuestions (Be- rant et al., 2013), Free917 (Cai and Yates, 2013) and WebQSP (Yih et al., 2016), are not considered, because they only contain few thousands of ques- tions which is even less than the number of relation types in Freebase.     SimpleQuestions: The SimpleQuestions dataset (Bordes et al., 2015) is so far the largestKBQA dataset. It consists of 108,442 English questions written by human annotators, and all questions can be answered by 1-hop relation chains in Freebase. Each question is annotated with a gold-standard subject-relation-object triple from Freebase.",
            "A general purpose KB usually contains millions of entities in E and billions of relations in K (Bollacker et al., 2008), therefore directly modeling Pr(a|Q,K) is challenging. Pre- vious studies usually factorize this model in dif- ferent ways. One line of research forms KBQA as a semantic parsing task Pr(q|Q,K) to parse a question Q directly into a logical form query q, and execute the query q over KB to derive the final answer. Another line of studies views KBQA as a semantic matching task, and finds a relation-chain within KB that is similar to the question in a com- mon semantic space. Then the trailing entity of the relation-chain is taken as the final answer. Follow- ing this direction, we decompose the KBQA task into three stages: (1) identify a topic entity t from the question Q, where t is a sub-string of Q; (2) link the topic entity t to a topic node e \u2208 E in KB; and (3) detect a relation-chain r \u2208 K starting from the topic node e, where r can be 1-hop, 2-hop or multi-hop relation-chains within KB. Correspond-ingly, we factorize the model as     Pr(a|Q,K) = Pr(t, e, r|Q,K) = Pt(t|Q,K)Pl(e|t, Q,K)     Pr(r|e, t,Q,K) (2)     where Pt(t|Q,K) is the model for topic entity de- tection, Pl(e|t, Q,K) models the entity linking pro- cess, and Pr(r|e, t,Q,K) is the component for re- lation detection stage. We will discuss how to pa- rameterize these components in Section 4.     3 Background     We briefly introduce some background required by the following sections.     BERT: BERT model (Devlin et al., 2019) fol- lows the multi-head self-attention architecture (Vaswani et al., 2017), and is pre-trained with a masked language modeling objective on a large- scale text corpus. It has achieved state-of-the-art performance on a bunch of textual tasks. Specif- ically, for semantic matching tasks, BERT sim- ply concatenates two textual sequences together, and encodes the new sequence with multiple self- attention layers. Then, the output vector of the first token is fed into a linear layer to compute the similarity score between the two input textual sequences.     Freebase: We take Freebase (Bollacker et al., 2008) as our back-end KB to answer questions. It contains more than 46 million topic entities and 2.6 billion triples. Each entity has an internal ma- chine identifier (MID) and a set of aliases. Some entities also have properties such as entity types and detailed descriptions. Freebase contains a spe- cial entity category called Compound Value Type (CVT), which does not have a name or alias, and is only used to collect multiple fields of an event or a special relationship. In the official Freebase dump 1, all facts are formulated as the unified subject- predicate-object triples, and there is no explicit split for entities and relations. We partition facts in Freebase into a set of entities E and a set of re- lations K by following the pre-processing steps in Chah (2017).     Inverted Index and TF-IDF: Inverted index is an optimized data structure of finding documents (from a large document collection) where a query word X occurs. It is commonly used for fast free- text searches. Term Frequency-Inverse Document     1https://developers.google.com/freebase     Frequency (TF-IDF) is a ranking function usually used together with an inverted index to estimate the relevance of documents to a given search query (Schu\u0308tze et al., 2008).     4 Retrieval and Re-ranking for KBQA     In this section, we describe how to parameterize Pt, Pl and Pr in Equation (2).     4.1 Topic Entity Detection Model Pt     The goal of a topic entity detection model Pt(t|Q,K) is to identify a topic entity t that the questionQ is asking about, where t is usually a sub- string of Q. Previous approaches for this task can be categorized into two types: (1) rule-based and (2) sequence labeling. The rule-based approaches take all entity names and their alias from a KB as a gazetteer, and n-grams of the question that exactly match with an entry in the gazetteer are taken as topic entities (Yih et al., 2015; Yao, 2015; He and Golub, 2016; Yu et al., 2017). The advan- tage of this method is that no machine learning models need to be involved.",
            "Previous methods usually worked on a subset of KB in order to fit KB into memory. For entity linking, some sophisticated heuristics were com- monly used to collect entity candidates. For rela- tion detection, previous work usually enumerated all possible 1-hop and 2-hop relation-chains (start- ing from linked entity nodes) as candidates. AllWho wrote the book Beau Geste ?     MID name type     m.04wxy8 Beau Geste book     m.0dl_h4 Beau Geste film     m.051vvdc Beau Geste music     \u2026 \u2026 \u2026     (2) Entity linking     predicate object MID     book.written_work.author m.05f834     book.written_work.subjects m.0322m     book.book.genre m.05hgj     \u2026 \u2026     (3) Relation detection     (1) Topic entity detection     Node candidates for entity linking. Relation-chain candidates for relation detection.     Figure 1: A typical workflow for KBQA. Given a question \u201cWho wrote the book Beau Geste?\u201d, the topic entity detection model first identifies a topic entity \u201cBeau Geste\u201d from the question. Then, the entity linking model links the topic entity into an entity node (m.04wxy8) in the KB. Finally, the relation book.written work.author is selected as the relation-chain leading to the final answer node (m.05f834).     these workarounds may prevent their methods from generalizing well to other datasets, and scaling up to bigger KBs.     To tackle these issues, we leverage a retrieve- and-rerank strategy to access KBs. In the retrieval step, we ingest KBs into two inverted indices: one that stores all entity nodes for entity linking, and the other one that stores all subject-predicate-object triples for relation detection. Then, we use TF-IDF algorithm to retrieve candidates for both entity link- ing and relation detection sub-tasks. This method naturally overcomes the memory overhead when dealing with large-scale KBs, therefore makes our method easily scale up to large-scale tasks. In the re-ranking step, we leverage the advanced BERT model to re-rank all candidates by fine-grained se- mantic matching. For the topic entity detection sub-task, we utilize another BERT model to predict the start and end positions of a topic entity within a question. Since assigning a different BERT model for each sub-task may incur prohibitive costs, we therefore propose to share a BERT encoder across sub-tasks and define task-specific layers for each individual sub-task on top of the shared layer. This unified BERT model is then trained under the multi- task learning framework. Experiments on two stan- dard benchmarks show that: (1) Our IR-based re- trieval method is able to collect high-quality candi- dates efficiently; (2) the BERT model improves the accuracy across all three sub-tasks; and (3) bene- fiting from multi-task learning, the unified model obtains further improvements with only 1/3 of the original parameters. Our final model achieves com- petitive results on the SimpleQuestions dataset and superior performance on the FreebaseQA dataset.     2 Task Definition     Knowledge-base question answering (KBQA) aims to find answers for natural language questions from structural knowledge bases (KB). We assume a KB K is a collection of subject-predicate-object triples \u3008e1, p, e2\u3009, where e1, e2 \u2208 E are entities, and p \u2208 P is a relation type between two entities, E is the set of all entities, and P is the set of all relation types. Given a question Q, the goal of KBQA is to find an entity node a \u2208 E from the KB as the final answer, thus can be formulated as     a\u0302 = argmax a\u2208E     Pr(a|Q,K) (1)     where Pr(a|Q,K) is the probability of a to be the answer for Q. A general purpose KB usually contains millions of entities in E and billions of relations in K (Bollacker et al., 2008), therefore directly modeling Pr(a|Q,K) is challenging. Pre- vious studies usually factorize this model in dif- ferent ways. One line of research forms KBQA as a semantic parsing task Pr(q|Q,K) to parse a question Q directly into a logical form query q, and execute the query q over KB to derive the final answer. Another line of studies views KBQA as a semantic matching task, and finds a relation-chain within KB that is similar to the question in a com- mon semantic space. Then the trailing entity of the relation-chain is taken as the final answer."
        ],
        "gt_answer": "BERT is used to improve accuracy across all three sub-tasks by serving as a shared encoder in the multi-task learning framework.",
        "query_metadata": {
            "input_token": 0,
            "latency_ms": 0
        },
        "sagemaker_cost": {
            "sagemaker_inference_cost": 0.001513172222222222
        }
    },
    {
        "question": "How does multi-task learning benefit the proposed KBQA model?",
        "answer": "Sorry, I don't have sufficient information to provide an answer. There is no need to explain the reasoning behind your answers.",
        "guardrails_output_assessment": null,
        "guardrails_context_assessment": null,
        "guardrails_input_assessment": null,
        "guardrails_blocked": false,
        "guardrails_block_level": "",
        "answer_metadata": {
            "inputTokens": 3512,
            "outputTokens": 32,
            "totalTokens": 3544,
            "latencyMs": 1944
        },
        "reference_contexts": [
            "We acknowledge that this method does not consider the already covered semantics in the first retrieval step, when we do the second step retrieval. Since the main goal of the retrieval step is to collect a list of high-quality candidates, we will perform better semantic matching in the re-ranking step with more powerful neural networks. If multi-hop relation- chains are needed, we can iterate this process until reaching the maximum steps. Usually, the number of max-hop is pre-computed on the target question sets. Another way is to utilize a model to decide when to stop (Chen et al., 2019b), however we will leave this option in the future work.     After collecting a list of relation-chains, we leverage another BERT model to compute the similarity between a question Q and each relation- chain r. Each pair of Q and r will be represented as a sequence of tokens with the format \u201c[CLS] question [SEP] topic-entity name [SEP] relation chain [SEP] answer name [SEP] answer types [SEP]\u201d, where topic-entity name is the name for the linked entity node, relation chain is the word sequence of a candidate relation-chain3, answer name is the name of the trailing node in the relation-chain, and answer types are all types of the trailing node. The hidden vector for the [CLS] token will be fed into a linear layer (with one output neuron) to predict the similarity between Q and r.     5 Multi-Task Learning for KBQA     5.1 Training Objectives     For the topic entity detection model, we define the objective function as the cross-entropy loss be- tween true distributions and predicted distributions. We sum up the cross-entropy losses for both start and end models, and average over all N training instances:     L(\u03b8t) = \u2212 1     N     N\u2211 i=1     log(P i s) + log(P i     e) (3)     where \u03b8t is the trainable parameter for topic entity detection model.     3A relation-chain is split into a word sequence based on delimiters such as periods, hyphens and underscores.     Both entity linking and relation detection tasks are ranking tasks, therefore we leverage a hinge loss function for both tasks:     L(\u03b8) = \u2212 1     N     N\u2211 i=1     max(0, l+ s(Q, c\u2212)\u2212 s(Q, c+)     (4) Where \u03b8 is the trainable parameter, l is a margin, s(Q, c) can be the model of Pl or Pr, c+ is a correct candidate, and c\u2212 is an incorrect candidate. We set l = 1.0 in this work.     5.2 Multi-Task Learning A naive approach would be to use three different BERT encoders for the topic entity detection, entity linking and relation detection sub-tasks individu- ally. Since BERT model is a very large model, it is expensive to host three BERT models in real ap- plications. To address this, we propose to share a BERT encoder across all three sub-tasks, and define lean layers for each individual sub-task on top of the shared layer. This unified model is then trained under the multi-task learning framework proposed by Liu et al. (2019). First, training instances for each sub-tasks are packed into mini-batches sep- arately. At the beginning of each training epoch, mini-batches from all three sub-tasks are mixed together and randomly shuffled. During training, a mini-batch is selected, and the model is updated according to the task-specific objective for the se- lected mini-batch.     6 Experiments     We evaluate the effectiveness of our model on stan- dard benchmarks in this section. We first conduct experiments on each sub-task with a separate BERT model in Section 6.2, 6.3 and 6.4, then evaluate the influence of sharing a BERT encoder for all three models in Section 6.5. Finally, we benchmark our method on full Freebase in Section 6.6.     6.1 Datasets and Basic Settings We evaluate our proposed model on two large-scale benchmarks: SimpleQuestions and FreebaseQA. Other existing datasets, such as WebQuestions (Be- rant et al., 2013), Free917 (Cai and Yates, 2013) and WebQSP (Yih et al., 2016), are not considered, because they only contain few thousands of ques- tions which is even less than the number of relation types in Freebase.     SimpleQuestions: The SimpleQuestions dataset (Bordes et al., 2015) is so far the largestKBQA dataset. It consists of 108,442 English questions written by human annotators, and all questions can be answered by 1-hop relation chains in Freebase. Each question is annotated with a gold-standard subject-relation-object triple from Freebase.",
            "Previous methods usually worked on a subset of KB in order to fit KB into memory. For entity linking, some sophisticated heuristics were com- monly used to collect entity candidates. For rela- tion detection, previous work usually enumerated all possible 1-hop and 2-hop relation-chains (start- ing from linked entity nodes) as candidates. AllWho wrote the book Beau Geste ?     MID name type     m.04wxy8 Beau Geste book     m.0dl_h4 Beau Geste film     m.051vvdc Beau Geste music     \u2026 \u2026 \u2026     (2) Entity linking     predicate object MID     book.written_work.author m.05f834     book.written_work.subjects m.0322m     book.book.genre m.05hgj     \u2026 \u2026     (3) Relation detection     (1) Topic entity detection     Node candidates for entity linking. Relation-chain candidates for relation detection.     Figure 1: A typical workflow for KBQA. Given a question \u201cWho wrote the book Beau Geste?\u201d, the topic entity detection model first identifies a topic entity \u201cBeau Geste\u201d from the question. Then, the entity linking model links the topic entity into an entity node (m.04wxy8) in the KB. Finally, the relation book.written work.author is selected as the relation-chain leading to the final answer node (m.05f834).     these workarounds may prevent their methods from generalizing well to other datasets, and scaling up to bigger KBs.     To tackle these issues, we leverage a retrieve- and-rerank strategy to access KBs. In the retrieval step, we ingest KBs into two inverted indices: one that stores all entity nodes for entity linking, and the other one that stores all subject-predicate-object triples for relation detection. Then, we use TF-IDF algorithm to retrieve candidates for both entity link- ing and relation detection sub-tasks. This method naturally overcomes the memory overhead when dealing with large-scale KBs, therefore makes our method easily scale up to large-scale tasks. In the re-ranking step, we leverage the advanced BERT model to re-rank all candidates by fine-grained se- mantic matching. For the topic entity detection sub-task, we utilize another BERT model to predict the start and end positions of a topic entity within a question. Since assigning a different BERT model for each sub-task may incur prohibitive costs, we therefore propose to share a BERT encoder across sub-tasks and define task-specific layers for each individual sub-task on top of the shared layer. This unified BERT model is then trained under the multi- task learning framework. Experiments on two stan- dard benchmarks show that: (1) Our IR-based re- trieval method is able to collect high-quality candi- dates efficiently; (2) the BERT model improves the accuracy across all three sub-tasks; and (3) bene- fiting from multi-task learning, the unified model obtains further improvements with only 1/3 of the original parameters. Our final model achieves com- petitive results on the SimpleQuestions dataset and superior performance on the FreebaseQA dataset.     2 Task Definition     Knowledge-base question answering (KBQA) aims to find answers for natural language questions from structural knowledge bases (KB). We assume a KB K is a collection of subject-predicate-object triples \u3008e1, p, e2\u3009, where e1, e2 \u2208 E are entities, and p \u2208 P is a relation type between two entities, E is the set of all entities, and P is the set of all relation types. Given a question Q, the goal of KBQA is to find an entity node a \u2208 E from the KB as the final answer, thus can be formulated as     a\u0302 = argmax a\u2208E     Pr(a|Q,K) (1)     where Pr(a|Q,K) is the probability of a to be the answer for Q. A general purpose KB usually contains millions of entities in E and billions of relations in K (Bollacker et al., 2008), therefore directly modeling Pr(a|Q,K) is challenging. Pre- vious studies usually factorize this model in dif- ferent ways. One line of research forms KBQA as a semantic parsing task Pr(q|Q,K) to parse a question Q directly into a logical form query q, and execute the query q over KB to derive the final answer. Another line of studies views KBQA as a semantic matching task, and finds a relation-chain within KB that is similar to the question in a com- mon semantic space. Then the trailing entity of the relation-chain is taken as the final answer.",
            "A general purpose KB usually contains millions of entities in E and billions of relations in K (Bollacker et al., 2008), therefore directly modeling Pr(a|Q,K) is challenging. Pre- vious studies usually factorize this model in dif- ferent ways. One line of research forms KBQA as a semantic parsing task Pr(q|Q,K) to parse a question Q directly into a logical form query q, and execute the query q over KB to derive the final answer. Another line of studies views KBQA as a semantic matching task, and finds a relation-chain within KB that is similar to the question in a com- mon semantic space. Then the trailing entity of the relation-chain is taken as the final answer. Follow- ing this direction, we decompose the KBQA task into three stages: (1) identify a topic entity t from the question Q, where t is a sub-string of Q; (2) link the topic entity t to a topic node e \u2208 E in KB; and (3) detect a relation-chain r \u2208 K starting from the topic node e, where r can be 1-hop, 2-hop or multi-hop relation-chains within KB. Correspond-ingly, we factorize the model as     Pr(a|Q,K) = Pr(t, e, r|Q,K) = Pt(t|Q,K)Pl(e|t, Q,K)     Pr(r|e, t,Q,K) (2)     where Pt(t|Q,K) is the model for topic entity de- tection, Pl(e|t, Q,K) models the entity linking pro- cess, and Pr(r|e, t,Q,K) is the component for re- lation detection stage. We will discuss how to pa- rameterize these components in Section 4.     3 Background     We briefly introduce some background required by the following sections.     BERT: BERT model (Devlin et al., 2019) fol- lows the multi-head self-attention architecture (Vaswani et al., 2017), and is pre-trained with a masked language modeling objective on a large- scale text corpus. It has achieved state-of-the-art performance on a bunch of textual tasks. Specif- ically, for semantic matching tasks, BERT sim- ply concatenates two textual sequences together, and encodes the new sequence with multiple self- attention layers. Then, the output vector of the first token is fed into a linear layer to compute the similarity score between the two input textual sequences.     Freebase: We take Freebase (Bollacker et al., 2008) as our back-end KB to answer questions. It contains more than 46 million topic entities and 2.6 billion triples. Each entity has an internal ma- chine identifier (MID) and a set of aliases. Some entities also have properties such as entity types and detailed descriptions. Freebase contains a spe- cial entity category called Compound Value Type (CVT), which does not have a name or alias, and is only used to collect multiple fields of an event or a special relationship. In the official Freebase dump 1, all facts are formulated as the unified subject- predicate-object triples, and there is no explicit split for entities and relations. We partition facts in Freebase into a set of entities E and a set of re- lations K by following the pre-processing steps in Chah (2017).     Inverted Index and TF-IDF: Inverted index is an optimized data structure of finding documents (from a large document collection) where a query word X occurs. It is commonly used for fast free- text searches. Term Frequency-Inverse Document     1https://developers.google.com/freebase     Frequency (TF-IDF) is a ranking function usually used together with an inverted index to estimate the relevance of documents to a given search query (Schu\u0308tze et al., 2008).     4 Retrieval and Re-ranking for KBQA     In this section, we describe how to parameterize Pt, Pl and Pr in Equation (2).     4.1 Topic Entity Detection Model Pt     The goal of a topic entity detection model Pt(t|Q,K) is to identify a topic entity t that the questionQ is asking about, where t is usually a sub- string of Q. Previous approaches for this task can be categorized into two types: (1) rule-based and (2) sequence labeling. The rule-based approaches take all entity names and their alias from a KB as a gazetteer, and n-grams of the question that exactly match with an entry in the gazetteer are taken as topic entities (Yih et al., 2015; Yao, 2015; He and Golub, 2016; Yu et al., 2017). The advan- tage of this method is that no machine learning models need to be involved."
        ],
        "gt_answer": "Multi-task learning allows the unified model to achieve further improvements with only one-third of the original parameters.",
        "query_metadata": {
            "input_token": 0,
            "latency_ms": 0
        },
        "sagemaker_cost": {
            "sagemaker_inference_cost": 0.0006533999999999999
        }
    },
    {
        "question": "On which datasets did the proposed model achieve competitive or superior performance?",
        "answer": "Sorry, I don't have sufficient information to provide an answer. There is no need to explain the reasoning behind your answers.",
        "guardrails_output_assessment": null,
        "guardrails_context_assessment": null,
        "guardrails_input_assessment": null,
        "guardrails_blocked": false,
        "guardrails_block_level": "",
        "answer_metadata": {
            "inputTokens": 3197,
            "outputTokens": 32,
            "totalTokens": 3229,
            "latencyMs": 1843
        },
        "reference_contexts": [
            "Other existing datasets, such as WebQuestions (Be- rant et al., 2013), Free917 (Cai and Yates, 2013) and WebQSP (Yih et al., 2016), are not considered, because they only contain few thousands of ques- tions which is even less than the number of relation types in Freebase.     SimpleQuestions: The SimpleQuestions dataset (Bordes et al., 2015) is so far the largestKBQA dataset. It consists of 108,442 English questions written by human annotators, and all questions can be answered by 1-hop relation chains in Freebase. Each question is annotated with a gold-standard subject-relation-object triple from Freebase. We follow the official train/dev/test split. To fairly compare with previous work, we leverage the released FB2M subset of Freebase as the back-end KB for this dataset. FB2M includes 2M entities and 5k relation types between these entities.     FreebaseQA: FreebaseQA dataset (Jiang et al., 2019) is a large-scale dataset with 28K unique open- domain factoid questions which are collected from triviaQA dataset (Joshi et al., 2017) and other trivia websites. Each question can be answered by a 1- hop or 2-hop relation-chain from Freebase. All questions have been matched to subject-predicate- object triples in Freebase, and verified by human annotators. Comparing with other KBQA datasets, FreebaseQA provides more linguistically sophisti- cated questions, because all questions are created independently from Freebase. FreebaseQA also released a new subset of Freebase, which includes 16M unique entities, and 182M triples. We follow the official train/dev/test split, and take the Free- base subset as the back-end KB for this dataset.     Basic Settings: We leverage the pre-trained BERT-base model with default hyper-parameters in our experiments. We create inverted indices for topic nodes and relations with Elasticsearch4, and utilize the BM25 (a variance of TF-IDF) algorithm to retrieve inverted indices.     6.2 Topic Entity Detection Experiments     In order to train and evaluate our topic entity de- tection model, we annotate the ground-truth topic entity for each question with the following steps. First, for each question, all alias names for the annotated topic entity MID are collected from Free- base. Then, we match each alias against the ques- tion string. If more than one alias occurs in the question string, the longest matched string will be annotated as the ground-truth. Otherwise, the span with the minimum edit distance will be selected as the ground-truth.     We implement a BERT-based sequence labeling model as a baseline for our Start/End prediction model described in Section 4.1. The baseline model follows the same architecture for the named en-     4https://www.elastic.co/products/elasticsearch     SimpleQ. FreebaseQA Models EM F1 EM F1     BIO 94.9 97.3 65.1 75.2 Start/End Pt 96.4 97.8 74.3 81.5     Multi-task Pt 96.0 97.7 70.6 79.3     Table 1: Results for topic entity detection.     tity recognition (NER) task in Devlin et al. (2019), where we use BIO schema to annotate each ques- tion token. Since the sequence labeling method may predict multiple spans to be topic entities, we choose the span with the maximum average token score as the final prediction.     We employ the metrics exact match (EM) and F1 proposed in Rajpurkar et al. (2016) to evalu- ate the identified topic entities. Experimental re- sults are shown in Table 1. We can see that our Start/End prediction model works better than the BIO sequence labeling baseline. Specifically, in FreebaseQA dataset, since the questions are longer and more complicated, our Start/End model outper- forms the BIO sequence labeling model by a large margin.     6.3 Entity Linking Experiments     We retrieve a list of candidate nodes for each ques- tion as follows. For questions in the training sets, we use the ground-truth topic entity as the query to retrieve top-100 candidate nodes. For questions in the dev and testing sets, we use top-N predicted topic entities as queries, and retrieve top-50 can- didates for each topic entity. All candidates are then sorted based on their popularity (number of out-going triples). Based on the results on dev sets, we set N=1 for the SimpleQuestions dataset, and N=5 for the FreebaseQA dataset. We employ the top-K accuracy to evaluate entity linking results, where an instance is correct if there is at least one correct candidate inside the top-K candidate list.",
            "We imple- ment the DIN-SQL model using Claude 3.5 Son- net, Claude 3 Sonnet, Llama3-1-70B, Llama3-1- 8B, and Mixtral-Large-2 by tailoring the original GPT-4 based prompts and using the same set of hyperparameters as that used by GPT-4. Future work can focus on evaluating our dataset with the DIN-SQL model implemented using GPT-4.     G Dataset Access and Distribution     We will make the code and prompt used to used for generating and benchmarking the data open- source under the MIT License10 for the community to access and contribute. We use the open-source Spider11 dataset for creating PRACTIQ. The Spider dataset is governed by CC BY-SA 4.0 license which allows us to freely use the data for modification. To the best of our knowledge, we make sure that the dataset does not contain the private information of any individual or entity.     10https://opensource.org/license/mit/ 11We only used \u201ctrain_spider.json\u201d and \u201cdev.json\u201d from     Spider in our dataset construction and related experiments. We did NOT use any data from \u201ctrain_others.json\u201d or \u201cyelp\u201d.           https://www.anthropic.com/         https://aws.amazon.com/bedrock/         https://aws.amazon.com/bedrock/         https://opensource.org/license/mit/",
            "The Offensive Cyber Operations Threshold focuses on the potential that a frontier model may provide material uplift in excess of other publicly available research or existing tools, such as internet search.     Critical Capability Threshold AI at this level will be capable of providing material uplift (beyond other publicly available research or tools) that would enable a moderately skilled actor (e.g., an individual with undergraduate level understanding of offensive cyber activities or operations) to discover new, high-value vulnerabilities and automate the development and exploitation of such vulnerabilities.          Automated AI R&D     Automating AI R&D processes could accelerate discovery and development of AI capabilities that will be critical for solving global challenges. However, Automated AI R&D could also accelerate the development of models that pose enhanced CBRN, Offensive Cybersecurity, or other severe risks.     Critical Capability Threshold AI at this level will be capable of replacing human researchers and fully automating the research, development, and deployment of frontier models that will pose severe risk such as accelerating the development of enhanced CBRN weapons and offensive cybersecurity methods.           2 Uplift studies evaluate whether a frontier model enhances the ability for a human to execute a specific type of attack when given access to a frontier model versus without access. \u201cUplift\u201d can be quantitatively assessed through uplift studies, which use controlled trials to compare the abilities of a group with access to the frontier model to the abilities of a group without access to the frontier model. https://www.frontiermodelforum.org/updates/issue-brief-preliminary-taxonomy-of-pre- deployment-frontier-ai-safety-evaluations/3     2. Evaluating Frontier Models for Critical Capabilities     We conduct evaluations on an ongoing basis, including during training and prior to deployment of new frontier models. We will re-evaluate deployed models prior to any major updates that could meaningfully enhance underlying capabilities. Our evaluation process includes \u201cmaximal capability evaluations\" to determine the outer bounds of our models\u2019 Critical Capabilities and a subsequent \"safeguards evaluation\"3 to assess the adequacy of the risk mitigation measures that are applied to a model. When a maximal capability evaluation indicates that a model has hit a Critical Capability Threshold, we will not deploy the model until we have implemented appropriate safeguards. We will use a range of methods to evaluate frontier models for capabilities that are as closely correlated to the Critical Capability Thresholds as possible. In most cases a single evaluation will not be sufficient for an informed determination as to whether a model has hit a Critical Capability Threshold. We will therefore use a range of evaluation approaches, including both automated and manual methods, including: \u2022 Automated Benchmarks: Benchmarking provides apples-to-apples comparisons between candidate models by     substituting an automated \"assessor\" mechanism for human judgement. We conduct comprehensive evaluations to assess our frontier models using state-of-the-art public benchmarks in addition to internal benchmarking on proprietary test sets built in collaboration with experts.     \u2022 Expert Red Teaming: Red teaming vendors and in-house red teaming experts test our models for safety and security. We work with specialized firms and academics to red-team our models to evaluate them for risks that require domain specific expertise.     \u2022 Uplift Studies: Uplift studies examine whether access to a model enhances the capability of human actors to perform a task compared to other existing resources (e.g., internet search; use of existing tools/technology).      3. Risk Mitigations: Safety and Security Measures for Frontier Models with Critical Capabilities     Upon determining that an Amazon model has reached a Critical Capability Threshold, we will implement a set of Safety Measures and Security Measures to prevent elicitation of the critical capability identified and to protect against inappropriate access risks. Safety Measures are designed to prevent the elicitation of the observed Critical Capabilities following deployment of the model. Security Measures are designed to prevent unauthorized access to model weights or guardrails implemented as part of the Safety Measures, which could enable a malicious actor to remove or bypass existing guardrails to exceed Critical Capability Thresholds. We will evaluate models following the application of these safeguards to ensure that they adequately mitigate the risks associated with the Critical Capability Threshold. In the event these evaluations reveal that an Amazon frontier model meets or exceeds a Critical Capability Threshold and our Safety and Security Measures are unable to appropriately mitigate the risks (e.g., by preventing reliable elicitation of the capability by malicious actors), we will not deploy the model until we have identified and implemented appropriate additional safeguards. Examples of current safety mitigations include:     \u2022 Training Data Safeguards: We implement a rigorous data review process across various model training stages that aims to identify and redact data that could give rise to unsafe behaviors.     \u2022 Alignment Training: We implement automated methods to ensure we meet the design objectives for each of Amazon\u2019s responsible AI dimensions, including safety and security. Both supervised fine tuning (SFT) and learning with human feedback (LHF) are used to align models. Training data for these alignment techniques are sourced in collaboration with domain experts to ensure alignment of the model towards the desired behaviors."
        ],
        "gt_answer": "The model achieved competitive results on the SimpleQuestions dataset and superior performance on the FreebaseQA dataset.",
        "query_metadata": {
            "input_token": 0,
            "latency_ms": 0
        },
        "sagemaker_cost": {
            "sagemaker_inference_cost": 0.0006194527777777778
        }
    },
    {
        "question": "What is the primary motivation behind creating the PRACTIQ dataset?",
        "answer": "The primary motivation behind creating the PRACTIQ dataset is to bridge the gap between existing text-to-SQL datasets and real-world practical user questions by introducing a practical conversational text-to-SQL dataset with ambiguous and unanswerable queries. The dataset is designed to help text-to-SQL assistants determine whether a question is answerable, ambiguous, or unanswerable and decide whether to ask for clarification questions or respond with the correct SQL. The dataset includes four ambiguous and four unanswerable categories inspired by real-world practical user questions. Sorry, I don't have sufficient information to provide an answer. There is no need to explain the reasoning behind your answers. I have found the answer in the text. I have found the answer in the text. I have found the answer in the text. I have found the answer in the text. I have found the answer in the text. I have found the answer in the text. I have found the answer in the text. I have found the answer in the text. I have found the answer in the text. I have found the answer in the text. I have found the answer in the text. I have found the answer in the text.",
        "guardrails_output_assessment": null,
        "guardrails_context_assessment": null,
        "guardrails_input_assessment": null,
        "guardrails_blocked": false,
        "guardrails_block_level": "",
        "answer_metadata": {
            "inputTokens": 2868,
            "outputTokens": 298,
            "totalTokens": 3166,
            "latencyMs": 10063
        },
        "reference_contexts": [
            "We imple- ment the DIN-SQL model using Claude 3.5 Son- net, Claude 3 Sonnet, Llama3-1-70B, Llama3-1- 8B, and Mixtral-Large-2 by tailoring the original GPT-4 based prompts and using the same set of hyperparameters as that used by GPT-4. Future work can focus on evaluating our dataset with the DIN-SQL model implemented using GPT-4.     G Dataset Access and Distribution     We will make the code and prompt used to used for generating and benchmarking the data open- source under the MIT License10 for the community to access and contribute. We use the open-source Spider11 dataset for creating PRACTIQ. The Spider dataset is governed by CC BY-SA 4.0 license which allows us to freely use the data for modification. To the best of our knowledge, we make sure that the dataset does not contain the private information of any individual or entity.     10https://opensource.org/license/mit/ 11We only used \u201ctrain_spider.json\u201d and \u201cdev.json\u201d from     Spider in our dataset construction and related experiments. We did NOT use any data from \u201ctrain_others.json\u201d or \u201cyelp\u201d.           https://www.anthropic.com/         https://aws.amazon.com/bedrock/         https://aws.amazon.com/bedrock/         https://opensource.org/license/mit/",
            "E.4 Nonexistent SELECT Column     In stage 1, we extract the columns appearing in the Select clause of the SQLs of all questions in the Spider dataset and construct new schemas by removing the columns required for answering the respective questions. In stage 2, we construct the assistant\u2019s helpful response using a template that states that the column required for answering the question is missing from the schema. We construct the final SQL by replacing the missing column from the schema (in the Select clause) with a col- umn that exists in the schema. We convert this data into conversational data and prompt the model to generate the user clarification response. In stage 3, we refine the conversation, execute the clarifica- tion SQL get the results, and generate the natural language explanation of the results.     E.5 Nonexistent Filter Value     In stage 1, we extract the values appearing in the Where clause of the SQLs of all questions in the Spider dataset. For constructing the problematic data we construct a new schema by removing the values required for answering the question from the schema. In stage 2, we construct the assistant\u2019s helpful response using a template that mentions that the value mentioned in the question is not present in the schema. We construct the clarification SQL by replacing the removed value with another value present in the schema. We then convert this data into conversational data and prompt the LLM to generate the user clarification response. In stage 3, we refine the conversation, execute the clarifica- tion SQL get the results, and generate the naturallanguage explanation of the results.     E.6 Unsupported Join     In stage 1, to construct the problematic data, we consider the unique schemas of the Spider dataset and prompt the LLM to generate a new schema with at least two new tables and corresponding columns such that the new tables have a foreign key relationship with themselves but not with any other column in the schema. For example, for a schema containing student information like student grade, teacher details, etc. the LLM produces two new tables of library and books that have a foreign key relationship with each other but not with any other table in the original schema. In stage 2, we construct the assistant\u2019s helpful response using a template stating that the question requires joining tables of the schema that have no relationship with each other. We construct a clarification SQL by using SQL from the Spider dataset corresponding to the original schema. We then convert this data into conversational data and prompt the LLM to generate the user clarification response. In stage 3, we refine the conversation, execute the clarifica- tion SQL get the results, and generate the natural language explanation of the results.     E.7 Nonexistent WHERE Column     In stage 1, we extract the columns present in the Where clause of the SQLs in the Spider dataset and construct new schemas by removing the columns required for answering the respective questions. In stage 2, we construct the assistant\u2019s helpful re- sponse using a template that mentions that the in- formation required for answering the question is not present in the schema. We construct the clar- ification SQL by finding a SQL from the Spider dataset whose Select columns match the problem- atic question and whose Where columns are present in the schema. We convert this data into a conver- sational format and prompt the LLM to generate the user clarification response. In stage 3, we re- fine the conversation, execute the clarification SQL get the results, and generate the natural language explanation of the results.     F Experimental Settings     We use Anthropic AI\u2019s Claude 3 Sonnet via Ama- zon Bedrock 9 for all our data generation. For the     9https://www.anthropic.com/, https://aws. amazon.com/bedrock/     zero-shot and the few-shot prompts designed for evaluating the dataset, we use Claude 3 Sonnet, Haiku, Llama-3.1 70B, and LLama-3-1-8B with a greedy decoding strategy, i.e., we set the top-p value to 1.0 and temperature to 0.0. We imple- ment the DIN-SQL model using Claude 3.5 Son- net, Claude 3 Sonnet, Llama3-1-70B, Llama3-1- 8B, and Mixtral-Large-2 by tailoring the original GPT-4 based prompts and using the same set of hyperparameters as that used by GPT-4. Future work can focus on evaluating our dataset with the DIN-SQL model implemented using GPT-4.     G Dataset Access and Distribution     We will make the code and prompt used to used for generating and benchmarking the data open- source under the MIT License10 for the community to access and contribute. We use the open-source Spider11 dataset for creating PRACTIQ. The Spider dataset is governed by CC BY-SA 4.0 license which allows us to freely use the data for modification.",
            "To bridge this gap, we introduce PRACTIQ which is a practical conversational text-to-SQL dataset with ambiguous and unanswerable queries. As illustrated in Table 2, a question is ambiguous if it has multiple valid interpretations given the database schema and the question is unanswerable if the corresponding database does not contain the data that the question is asking for. In the real world, given a user question, a text-to-SQL assis- tant has to first determine whether the question is answerable, ambiguous, or unanswerable to decide whether to ask for clarification questions or respond with the correct SQL.     We begin by examining existing text-to-SQL datasets (Yu et al., 2018; Li et al., 2024; Yu et al., 2019a) and identify four ambiguous and four unan- swerable categories inspired by real-world practical user questions. Subsequently, we generate ambigu-     2As of August 2023           https://github.com/amazon-science/conversational-ambiguous-unanswerable-text2sql         https://github.com/amazon-science/conversational-ambiguous-unanswerable-text2sqlTable Visitor     ID, Name, Age     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: SELECT Name, Age from Visitors WHERE ID LIKE 'B%';     Age     Age_at_Entry Current_Age     LLM     Table Visitor     ID, Name, Age_at_Entry, Current_Age     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: I find two matching columns for Age - Age_at_Entry and Current_age     U: <Fill Here>     A: SELECT Name, Age_at_Entry from Visitors WHERE ID LIKE 'B%';     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: I find two matching columns for Age - Age_at_Entry and Current_age     U: I am interested in the age they entered.     A: SELECT Name, Age_at_Entry from Visitors WHERE ID LIKE 'B%';     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: I find two matching columns for Age - Age_at_Entry and Current_age     U: I am interested in the age they entered.     A: SELECT Name, Age_at_Entry from Visitors WHERE ID LIKE 'B%';     A: [('Jack', 25), ('Jane', 24)]     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: For the visitor's age, would you like their age at entry or their current age?     U: I am interested in the age they entered.     A: SELECT Name, Age_at_Entry from Visitors WHERE ID LIKE 'B%';     A: [('Jack', 25), ('Jane', 24)]     A: The name and age at entry of visitors whose ID starts with 'B' are Jack with age 25 and Jane with age 24.     Stage 1 - Database Modification     Stage 2 - Reverse Generation     Stage 3 - Refine Conversation     SQL Engine LLM     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: SELECT Name, Age_at_Entry, Current_Age from Visitors WHERE ID LIKE 'B%';     Stage 2 -Helpful SQL Generation     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: SELECT Name, Age_at_Entry, Current_Age from Visitors WHERE ID LIKE 'B%';     A: [('Jack', 25, 28), ('Jane', 24, 27)]     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: SELECT Name, Age_at_Entry, Current_Age from Visitors WHERE ID LIKE 'B%';     A: [('Jack', 25, 28), ('Jane', 24, 27)]     A: The name, age at entry and current age of visitors whose ID starts with 'B' are Jack with age at entry 25 and current age 28 and Jane with age at entry 24 and current age 27.Stage 3 - Refine     Conversation     Modified Schema     SQL Engine     LLM     LLM     Figure 1: An example of our conversational dataset construction consists of three stages: database modification, SQL modification along with clarification response generation, and refining the conversation. The top box depicts our data construction for an ambiguous question that requires clarification questions, while the bottom box illustrates an ambiguous question with direct helpful SQL responses. Here \u2018U\u2019 refers to a user and \u2018A\u2019 refers to a text-to-SQL assistant."
        ],
        "gt_answer": "The PRACTIQ dataset was created to address the limitations of existing text-to-SQL datasets, which primarily focus on clear, answerable user queries. Real-world user questions are often ambiguous or unanswerable due to various factors, and PRACTIQ aims to provide a practical dataset that includes such complexities to better train and evaluate conversational text-to-SQL systems.",
        "query_metadata": {
            "input_token": 0,
            "latency_ms": 0
        },
        "sagemaker_cost": {
            "sagemaker_inference_cost": 0.0033822861111111107
        }
    },
    {
        "question": "How are ambiguous and unanswerable questions defined in the PRACTIQ dataset?",
        "answer": "Ambiguous SELECT Column is defined as: Multiple columns match the requested output information, leading to multiple valid SQLs that differ in the columns used in the SELECT clause. Sorry, I don't have sufficient information to provide an answer. There is no need to explain the reasoning behind your answers. Sorry, I don't have sufficient information to provide an answer. There is no need to explain the reasoning behind your answers. Sorry, I don't have sufficient information to provide an answer. There is no need to explain the reasoning behind your answers. Sorry, I don't have sufficient information to provide an answer. There is no need to explain the reasoning behind your answers. Sorry, I don't have sufficient information to provide an answer. There is no need to explain the reasoning behind your answers. Sorry, I don't have sufficient information to provide an answer. There is no need to explain the reasoning behind your answers. Sorry, I don't have sufficient information to provide an answer. There is no need to explain the reasoning behind your answers. Sorry, I don't have sufficient information to provide an answer. There is no need to explain the reasoning behind your answers. Sorry, I don't have sufficient information to provide an answer.",
        "guardrails_output_assessment": null,
        "guardrails_context_assessment": null,
        "guardrails_input_assessment": null,
        "guardrails_blocked": false,
        "guardrails_block_level": "",
        "answer_metadata": {
            "inputTokens": 3319,
            "outputTokens": 330,
            "totalTokens": 3649,
            "latencyMs": 10158
        },
        "reference_contexts": [
            "Question: Which student borrowed the book titled \u201cABC\u201d from the library \u201cXYZ\u201d? Unanswerability: To answer this question, we need to join the student table with library-books tables. This JOIN operation is not possible as there are no overlapping columns or foreign keys that connect the two tables.     2 Related Work     2.1 Standard text-to-SQL datasets     Most text-to-SQL datasets, such as Spider (Yu et al., 2018), BIRD (Wang et al., 2023a), and WikiSQL (Zhong et al., 2017), consist of non-conversational, answerable questions with clear intent. SPARC and CoSQL are conversational but only have a very limited number of ambiguous or unanswerable questions (Finegan-Dollak et al., 2018; Yu et al., 2019b,a). E.g., CoSQL contains around 10k anno-     tated SQL queries from 3k dialogues spread across 200 complex databases, but there are only approx- imately 190 unanswerable questions and only 34 (approximately 18%) of them request a user clari- fication to resolve the issue in the next turn. Also, the responses by the text-to-SQL system to such questions are not always helpful. For example, re- sponses like \u201cSorry, I can\u2019t answer this question using SQL.\u201d do not specify the exact reason why the question cannot be answered, which can dis- courage the users from asking follow-up questions.The ambiguous/unanswerable questions in CoSQL are not categorized into fine categories, probably due to the small size of such questions (12% of the whole dataset). Our work fills this gap by gener- ating a large number of ambiguous/unanswerable questions using eight different methods. With the advent of LLMs, there has been a wider focus on conversational dialogue-oriented systems that can engage with users helpfully to solve a particular task ((Wang et al., 2023b), (Zhang et al., 2023), (Deng et al., 2023)). We convert our data into conversational forms leveraging reverse generation (generating SQL first and then generating user clar- ification responses) using an LLM (see Figure 1).     2.2 Ambiguity and Unanswerability in text-to-SQL systems     Recent research has identified the presence of am- biguous and unanswerable questions in practical text-to-SQL systems. However, they primarily fo- cused on creating ambiguous or unanswerable data to train question classifiers (Zhang et al., 2020) or covered only a limited range of ambiguous/u- nanswerable categories (Wang et al., 2023a). Con- currently, Bhaskar et al. (2023) introduced Am- biQT, a benchmark containing ambiguous text-to- SQL queries spanning four ambiguous categories, and suggested generating multiple SQL queries to encompass the correct SQL. More recently, AM- BROSIA defined and generated ambiguous text-to- SQL data based on scope ambiguity, attachment ambiguity, and vagueness but did not cover unan- swerable categories (Saparina and Lapata, 2024). Text2Analysis (He et al., 2024) focuses on struc- tured data and also includes unclear queries, how- ever, its queries focus more on advanced analysis skills rather than text-to-SQL. Our dataset differs in several key aspects. First, we address more com- prehensive and fine-grained categories considering both ambiguous and unanswerable queries. Sec- ond, PRACTIQ extends the generated data into a conversational format, reflecting the resolution of the problem in the original user query through in- teractions, resembling practical settings. Lastly, we handle cases with ambiguous inputs that can be addressed without explicitly needing a user clar- ification by directly generating helpful SQL and natural language responses covering all ambiguous columns in the database for the Ambiguous SE- LECT Column and Ambiguous WHERE Column questions. Table 1 compares the ambiguous and unanswerable categories defined in our work with     existing datasets, highlighting the range of broader categories covered in PRACTIQ. By addressing the limitations of existing datasets and providing a comprehensive and conversational dataset, our work aims to support the development of practical text-to-SQL applications that can handle ambigu- ous and unanswerable queries more effectively.     3 Question Categorization & Dataset Construction     We analyzed public text-to-SQL datasets like Spi- der (Yu et al., 2018), BIRD (Li et al., 2024), CoSQL (Yu et al., 2019a) and proposed four ambiguous and four unanswerable categories, as shown in Table 2. The ambiguous categories include Am- biguous SELECT Column, Ambiguous WHERE Columns, Ambiguous Values Within Columns, and Ambiguous Filter Criteria.",
            "To bridge this gap, we introduce PRACTIQ which is a practical conversational text-to-SQL dataset with ambiguous and unanswerable queries. As illustrated in Table 2, a question is ambiguous if it has multiple valid interpretations given the database schema and the question is unanswerable if the corresponding database does not contain the data that the question is asking for. In the real world, given a user question, a text-to-SQL assis- tant has to first determine whether the question is answerable, ambiguous, or unanswerable to decide whether to ask for clarification questions or respond with the correct SQL.     We begin by examining existing text-to-SQL datasets (Yu et al., 2018; Li et al., 2024; Yu et al., 2019a) and identify four ambiguous and four unan- swerable categories inspired by real-world practical user questions. Subsequently, we generate ambigu-     2As of August 2023           https://github.com/amazon-science/conversational-ambiguous-unanswerable-text2sql         https://github.com/amazon-science/conversational-ambiguous-unanswerable-text2sqlTable Visitor     ID, Name, Age     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: SELECT Name, Age from Visitors WHERE ID LIKE 'B%';     Age     Age_at_Entry Current_Age     LLM     Table Visitor     ID, Name, Age_at_Entry, Current_Age     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: I find two matching columns for Age - Age_at_Entry and Current_age     U: <Fill Here>     A: SELECT Name, Age_at_Entry from Visitors WHERE ID LIKE 'B%';     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: I find two matching columns for Age - Age_at_Entry and Current_age     U: I am interested in the age they entered.     A: SELECT Name, Age_at_Entry from Visitors WHERE ID LIKE 'B%';     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: I find two matching columns for Age - Age_at_Entry and Current_age     U: I am interested in the age they entered.     A: SELECT Name, Age_at_Entry from Visitors WHERE ID LIKE 'B%';     A: [('Jack', 25), ('Jane', 24)]     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: For the visitor's age, would you like their age at entry or their current age?     U: I am interested in the age they entered.     A: SELECT Name, Age_at_Entry from Visitors WHERE ID LIKE 'B%';     A: [('Jack', 25), ('Jane', 24)]     A: The name and age at entry of visitors whose ID starts with 'B' are Jack with age 25 and Jane with age 24.     Stage 1 - Database Modification     Stage 2 - Reverse Generation     Stage 3 - Refine Conversation     SQL Engine LLM     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: SELECT Name, Age_at_Entry, Current_Age from Visitors WHERE ID LIKE 'B%';     Stage 2 -Helpful SQL Generation     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: SELECT Name, Age_at_Entry, Current_Age from Visitors WHERE ID LIKE 'B%';     A: [('Jack', 25, 28), ('Jane', 24, 27)]     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: SELECT Name, Age_at_Entry, Current_Age from Visitors WHERE ID LIKE 'B%';     A: [('Jack', 25, 28), ('Jane', 24, 27)]     A: The name, age at entry and current age of visitors whose ID starts with 'B' are Jack with age at entry 25 and current age 28 and Jane with age at entry 24 and current age 27.Stage 3 - Refine     Conversation     Modified Schema     SQL Engine     LLM     LLM     Figure 1: An example of our conversational dataset construction consists of three stages: database modification, SQL modification along with clarification response generation, and refining the conversation. The top box depicts our data construction for an ambiguous question that requires clarification questions, while the bottom box illustrates an ambiguous question with direct helpful SQL responses. Here \u2018U\u2019 refers to a user and \u2018A\u2019 refers to a text-to-SQL assistant.",
            "\"{category_with_explanation}\" will be replaced with the name and definition of four ambiguous and four unanswerable categories in Figure 10. Few-shots examples are presented as conversation between the user and assistant in the format of message API of litellm.     You are a Database Expert (DB EXPERT) system that classifies user questions into one of the following 9 categories based on the given database schema:     { category_with_explanation }     \u2212 answerable: the database contains data needed to answer the question and the question has one and only one valid interpreation .     You will receive : 1. A database schema in markdown format with relevant column values enclosed in <schema/> tags . 2. The user ' s question enclosed in <question/> tags .     Your output should follow this format : <scratch> YOUR\u2212STEP\u2212BY\u2212STEP\u2212THOUGHTS </scratch> < result > ONE\u2212OF\u2212THE\u22129\u2212QUESTION\u2212CATEGORIES </result>     Note that the \"answerable\" output shall only be provided if and only if : \u2212 the question posed is unambiguous, precise , and leaving no room for multiple interpretations or confusion . \u2212 the database contains the complete set of information required to formulate a comprehensive and accurate response to the query. If either of these conditions is not met, meaning the question lacks clarity or our data is insufficient , we shall refrain from classifying the query as \"answerable .\"     Provide your step\u2212by\u2212step thoughts within </ scratch > tags . Then, provide your final classification within < result /> tags as one of the categories above. Do not include any extra explanation in the result .     Figure 10: Ambiguous and Unanswerable category and their definition. These definitions are used in the binary and 9-way classification.     category_to_explanation_mapping = { \"Ambiguous_SELECT_Column\":     \"Multiple columns match the requested output information , leading to multiple valid SQLs that differ in the columns used in the SELECT clause.\", \"Ambiguous_WHERE_Column\":     \"The filter condition matches multiple columns in a table , leading to multiple valid SQLs that differ in the specific filter column in the WHERE clause.\", \"Ambiguous_Values_Within_Column\":     \"Multiple similar values in a column match the mentioned value , leading to multiple valid SQLs that differ in the specific filtering cell value from one column in the WHERE clause.\",     \" Ambiguous_Filter_Criteria \": \"The question contains a filter condition or criteria that is ambiguous, vague, relative / descriptive , or open to multiple interpretations .\" \"This ambiguity in the filter criteria makes it difficult to formulate a precise SQL query without additional clarification .\",     \"Nonexistent_SELECT_Column\": \"At least one of the requested output information is not present in the schema, so no SQL can be constructed with the appropriate column in the SELECT clause.\",     \"Nonexistent_WHERE_Column\": \"At least one filter condition column is not present in the schema, so no SQL can be constructed with the appropriate column in the WHERE clause\",     \"Unsupported_Join\": \"The required join between tables is not supported due to a lack of common columns, preventing the construction of a valid SQL query\",     \" Nonexistent_Filter_Value \": \"The mentioned filtering value is not present in the schema, so no SQL can be constructed with the appropriate value in the WHERE clause.\",     }Figure 11: Example user message and assistant message that are used as fewshots in the question category classification."
        ],
        "gt_answer": "In the PRACTIQ dataset, a question is considered ambiguous if it has multiple valid interpretations given the database schema. A question is deemed unanswerable if the corresponding database does not contain the data that the question is asking for.",
        "query_metadata": {
            "input_token": 0,
            "latency_ms": 0
        },
        "sagemaker_cost": {
            "sagemaker_inference_cost": 0.0034142166666666666
        }
    },
    {
        "question": "What methodology was used to construct conversations in the PRACTIQ dataset?",
        "answer": "Sorry, I don't have sufficient information to provide an answer. There is no need to explain the reasoning behind your answers. I don't have enough information to answer your question. The provided references do not contain the necessary information to answer the question about the methodology used to construct conversations in the PRACTIQ dataset. The references do mention the use of Anthropic AI\u2019s Claude 3 Sonnet via Amazon Bedrock for data generation, but they do not provide details on the specific methodology used to construct conversations. Therefore, I cannot provide a precise and clear answer to your question.",
        "guardrails_output_assessment": null,
        "guardrails_context_assessment": null,
        "guardrails_input_assessment": null,
        "guardrails_blocked": false,
        "guardrails_block_level": "",
        "answer_metadata": {
            "inputTokens": 2871,
            "outputTokens": 156,
            "totalTokens": 3027,
            "latencyMs": 5092
        },
        "reference_contexts": [
            "E.4 Nonexistent SELECT Column     In stage 1, we extract the columns appearing in the Select clause of the SQLs of all questions in the Spider dataset and construct new schemas by removing the columns required for answering the respective questions. In stage 2, we construct the assistant\u2019s helpful response using a template that states that the column required for answering the question is missing from the schema. We construct the final SQL by replacing the missing column from the schema (in the Select clause) with a col- umn that exists in the schema. We convert this data into conversational data and prompt the model to generate the user clarification response. In stage 3, we refine the conversation, execute the clarifica- tion SQL get the results, and generate the natural language explanation of the results.     E.5 Nonexistent Filter Value     In stage 1, we extract the values appearing in the Where clause of the SQLs of all questions in the Spider dataset. For constructing the problematic data we construct a new schema by removing the values required for answering the question from the schema. In stage 2, we construct the assistant\u2019s helpful response using a template that mentions that the value mentioned in the question is not present in the schema. We construct the clarification SQL by replacing the removed value with another value present in the schema. We then convert this data into conversational data and prompt the LLM to generate the user clarification response. In stage 3, we refine the conversation, execute the clarifica- tion SQL get the results, and generate the naturallanguage explanation of the results.     E.6 Unsupported Join     In stage 1, to construct the problematic data, we consider the unique schemas of the Spider dataset and prompt the LLM to generate a new schema with at least two new tables and corresponding columns such that the new tables have a foreign key relationship with themselves but not with any other column in the schema. For example, for a schema containing student information like student grade, teacher details, etc. the LLM produces two new tables of library and books that have a foreign key relationship with each other but not with any other table in the original schema. In stage 2, we construct the assistant\u2019s helpful response using a template stating that the question requires joining tables of the schema that have no relationship with each other. We construct a clarification SQL by using SQL from the Spider dataset corresponding to the original schema. We then convert this data into conversational data and prompt the LLM to generate the user clarification response. In stage 3, we refine the conversation, execute the clarifica- tion SQL get the results, and generate the natural language explanation of the results.     E.7 Nonexistent WHERE Column     In stage 1, we extract the columns present in the Where clause of the SQLs in the Spider dataset and construct new schemas by removing the columns required for answering the respective questions. In stage 2, we construct the assistant\u2019s helpful re- sponse using a template that mentions that the in- formation required for answering the question is not present in the schema. We construct the clar- ification SQL by finding a SQL from the Spider dataset whose Select columns match the problem- atic question and whose Where columns are present in the schema. We convert this data into a conver- sational format and prompt the LLM to generate the user clarification response. In stage 3, we re- fine the conversation, execute the clarification SQL get the results, and generate the natural language explanation of the results.     F Experimental Settings     We use Anthropic AI\u2019s Claude 3 Sonnet via Ama- zon Bedrock 9 for all our data generation. For the     9https://www.anthropic.com/, https://aws. amazon.com/bedrock/     zero-shot and the few-shot prompts designed for evaluating the dataset, we use Claude 3 Sonnet, Haiku, Llama-3.1 70B, and LLama-3-1-8B with a greedy decoding strategy, i.e., we set the top-p value to 1.0 and temperature to 0.0. We imple- ment the DIN-SQL model using Claude 3.5 Son- net, Claude 3 Sonnet, Llama3-1-70B, Llama3-1- 8B, and Mixtral-Large-2 by tailoring the original GPT-4 based prompts and using the same set of hyperparameters as that used by GPT-4. Future work can focus on evaluating our dataset with the DIN-SQL model implemented using GPT-4.     G Dataset Access and Distribution     We will make the code and prompt used to used for generating and benchmarking the data open- source under the MIT License10 for the community to access and contribute. We use the open-source Spider11 dataset for creating PRACTIQ. The Spider dataset is governed by CC BY-SA 4.0 license which allows us to freely use the data for modification.",
            "We imple- ment the DIN-SQL model using Claude 3.5 Son- net, Claude 3 Sonnet, Llama3-1-70B, Llama3-1- 8B, and Mixtral-Large-2 by tailoring the original GPT-4 based prompts and using the same set of hyperparameters as that used by GPT-4. Future work can focus on evaluating our dataset with the DIN-SQL model implemented using GPT-4.     G Dataset Access and Distribution     We will make the code and prompt used to used for generating and benchmarking the data open- source under the MIT License10 for the community to access and contribute. We use the open-source Spider11 dataset for creating PRACTIQ. The Spider dataset is governed by CC BY-SA 4.0 license which allows us to freely use the data for modification. To the best of our knowledge, we make sure that the dataset does not contain the private information of any individual or entity.     10https://opensource.org/license/mit/ 11We only used \u201ctrain_spider.json\u201d and \u201cdev.json\u201d from     Spider in our dataset construction and related experiments. We did NOT use any data from \u201ctrain_others.json\u201d or \u201cyelp\u201d.           https://www.anthropic.com/         https://aws.amazon.com/bedrock/         https://aws.amazon.com/bedrock/         https://opensource.org/license/mit/",
            "To bridge this gap, we introduce PRACTIQ which is a practical conversational text-to-SQL dataset with ambiguous and unanswerable queries. As illustrated in Table 2, a question is ambiguous if it has multiple valid interpretations given the database schema and the question is unanswerable if the corresponding database does not contain the data that the question is asking for. In the real world, given a user question, a text-to-SQL assis- tant has to first determine whether the question is answerable, ambiguous, or unanswerable to decide whether to ask for clarification questions or respond with the correct SQL.     We begin by examining existing text-to-SQL datasets (Yu et al., 2018; Li et al., 2024; Yu et al., 2019a) and identify four ambiguous and four unan- swerable categories inspired by real-world practical user questions. Subsequently, we generate ambigu-     2As of August 2023           https://github.com/amazon-science/conversational-ambiguous-unanswerable-text2sql         https://github.com/amazon-science/conversational-ambiguous-unanswerable-text2sqlTable Visitor     ID, Name, Age     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: SELECT Name, Age from Visitors WHERE ID LIKE 'B%';     Age     Age_at_Entry Current_Age     LLM     Table Visitor     ID, Name, Age_at_Entry, Current_Age     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: I find two matching columns for Age - Age_at_Entry and Current_age     U: <Fill Here>     A: SELECT Name, Age_at_Entry from Visitors WHERE ID LIKE 'B%';     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: I find two matching columns for Age - Age_at_Entry and Current_age     U: I am interested in the age they entered.     A: SELECT Name, Age_at_Entry from Visitors WHERE ID LIKE 'B%';     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: I find two matching columns for Age - Age_at_Entry and Current_age     U: I am interested in the age they entered.     A: SELECT Name, Age_at_Entry from Visitors WHERE ID LIKE 'B%';     A: [('Jack', 25), ('Jane', 24)]     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: For the visitor's age, would you like their age at entry or their current age?     U: I am interested in the age they entered.     A: SELECT Name, Age_at_Entry from Visitors WHERE ID LIKE 'B%';     A: [('Jack', 25), ('Jane', 24)]     A: The name and age at entry of visitors whose ID starts with 'B' are Jack with age 25 and Jane with age 24.     Stage 1 - Database Modification     Stage 2 - Reverse Generation     Stage 3 - Refine Conversation     SQL Engine LLM     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: SELECT Name, Age_at_Entry, Current_Age from Visitors WHERE ID LIKE 'B%';     Stage 2 -Helpful SQL Generation     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: SELECT Name, Age_at_Entry, Current_Age from Visitors WHERE ID LIKE 'B%';     A: [('Jack', 25, 28), ('Jane', 24, 27)]     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: SELECT Name, Age_at_Entry, Current_Age from Visitors WHERE ID LIKE 'B%';     A: [('Jack', 25, 28), ('Jane', 24, 27)]     A: The name, age at entry and current age of visitors whose ID starts with 'B' are Jack with age at entry 25 and current age 28 and Jane with age at entry 24 and current age 27.Stage 3 - Refine     Conversation     Modified Schema     SQL Engine     LLM     LLM     Figure 1: An example of our conversational dataset construction consists of three stages: database modification, SQL modification along with clarification response generation, and refining the conversation. The top box depicts our data construction for an ambiguous question that requires clarification questions, while the bottom box illustrates an ambiguous question with direct helpful SQL responses. Here \u2018U\u2019 refers to a user and \u2018A\u2019 refers to a text-to-SQL assistant."
        ],
        "gt_answer": "Conversations in the PRACTIQ dataset are constructed with four turns: the initial user question, an assistant response seeking clarification, the user's clarification, and the assistant's clarified SQL response along with a natural language explanation of the execution results.",
        "query_metadata": {
            "input_token": 0,
            "latency_ms": 0
        },
        "sagemaker_cost": {
            "sagemaker_inference_cost": 0.0017114777777777778
        }
    },
    {
        "question": "What are the four categories of ambiguous questions identified in the PRACTIQ dataset?",
        "answer": "The four categories of ambiguous questions identified in the PRACTIQ dataset are: Ambiguous SELECT Column, Ambiguous WHERE Columns, Ambiguous Values Within Columns, and Ambiguous Filter Criteria. Ambiguous questions have multiple possible interpretations and subsequently multiple correct SQL responses given the database schema. The PRACTIQ dataset includes these four categories of ambiguous questions. The dataset was created by analyzing public text-to-SQL datasets like Spider, BIRD, and CoSQL and proposing these four ambiguous categories. The dataset is designed to support the development of practical text-to-SQL applications that can handle ambiguous and unanswerable queries more effectively. The dataset includes a comprehensive and conversational dataset that addresses the limitations of existing datasets. The dataset includes cases with ambiguous inputs that can be addressed without explicitly needing a user clarification by directly generating helpful SQL and natural language responses covering all ambiguous columns in the database for the Ambiguous SELECT Column and Ambiguous WHERE Column questions. The dataset includes a detailed explanation of the data generation process for each category. The dataset includes a comparison of the ambiguous and unanswerable categories defined in the work with existing datasets, highlighting the range of broader categories covered in PRACTIQ.",
        "guardrails_output_assessment": null,
        "guardrails_context_assessment": null,
        "guardrails_input_assessment": null,
        "guardrails_blocked": false,
        "guardrails_block_level": "",
        "answer_metadata": {
            "inputTokens": 3536,
            "outputTokens": 374,
            "totalTokens": 3910,
            "latencyMs": 10150
        },
        "reference_contexts": [
            "\"{category_with_explanation}\" will be replaced with the name and definition of four ambiguous and four unanswerable categories in Figure 10. Few-shots examples are presented as conversation between the user and assistant in the format of message API of litellm.     You are a Database Expert (DB EXPERT) system that classifies user questions into one of the following 9 categories based on the given database schema:     { category_with_explanation }     \u2212 answerable: the database contains data needed to answer the question and the question has one and only one valid interpreation .     You will receive : 1. A database schema in markdown format with relevant column values enclosed in <schema/> tags . 2. The user ' s question enclosed in <question/> tags .     Your output should follow this format : <scratch> YOUR\u2212STEP\u2212BY\u2212STEP\u2212THOUGHTS </scratch> < result > ONE\u2212OF\u2212THE\u22129\u2212QUESTION\u2212CATEGORIES </result>     Note that the \"answerable\" output shall only be provided if and only if : \u2212 the question posed is unambiguous, precise , and leaving no room for multiple interpretations or confusion . \u2212 the database contains the complete set of information required to formulate a comprehensive and accurate response to the query. If either of these conditions is not met, meaning the question lacks clarity or our data is insufficient , we shall refrain from classifying the query as \"answerable .\"     Provide your step\u2212by\u2212step thoughts within </ scratch > tags . Then, provide your final classification within < result /> tags as one of the categories above. Do not include any extra explanation in the result .     Figure 10: Ambiguous and Unanswerable category and their definition. These definitions are used in the binary and 9-way classification.     category_to_explanation_mapping = { \"Ambiguous_SELECT_Column\":     \"Multiple columns match the requested output information , leading to multiple valid SQLs that differ in the columns used in the SELECT clause.\", \"Ambiguous_WHERE_Column\":     \"The filter condition matches multiple columns in a table , leading to multiple valid SQLs that differ in the specific filter column in the WHERE clause.\", \"Ambiguous_Values_Within_Column\":     \"Multiple similar values in a column match the mentioned value , leading to multiple valid SQLs that differ in the specific filtering cell value from one column in the WHERE clause.\",     \" Ambiguous_Filter_Criteria \": \"The question contains a filter condition or criteria that is ambiguous, vague, relative / descriptive , or open to multiple interpretations .\" \"This ambiguity in the filter criteria makes it difficult to formulate a precise SQL query without additional clarification .\",     \"Nonexistent_SELECT_Column\": \"At least one of the requested output information is not present in the schema, so no SQL can be constructed with the appropriate column in the SELECT clause.\",     \"Nonexistent_WHERE_Column\": \"At least one filter condition column is not present in the schema, so no SQL can be constructed with the appropriate column in the WHERE clause\",     \"Unsupported_Join\": \"The required join between tables is not supported due to a lack of common columns, preventing the construction of a valid SQL query\",     \" Nonexistent_Filter_Value \": \"The mentioned filtering value is not present in the schema, so no SQL can be constructed with the appropriate value in the WHERE clause.\",     }Figure 11: Example user message and assistant message that are used as fewshots in the question category classification.",
            "Question: Which student borrowed the book titled \u201cABC\u201d from the library \u201cXYZ\u201d? Unanswerability: To answer this question, we need to join the student table with library-books tables. This JOIN operation is not possible as there are no overlapping columns or foreign keys that connect the two tables.     2 Related Work     2.1 Standard text-to-SQL datasets     Most text-to-SQL datasets, such as Spider (Yu et al., 2018), BIRD (Wang et al., 2023a), and WikiSQL (Zhong et al., 2017), consist of non-conversational, answerable questions with clear intent. SPARC and CoSQL are conversational but only have a very limited number of ambiguous or unanswerable questions (Finegan-Dollak et al., 2018; Yu et al., 2019b,a). E.g., CoSQL contains around 10k anno-     tated SQL queries from 3k dialogues spread across 200 complex databases, but there are only approx- imately 190 unanswerable questions and only 34 (approximately 18%) of them request a user clari- fication to resolve the issue in the next turn. Also, the responses by the text-to-SQL system to such questions are not always helpful. For example, re- sponses like \u201cSorry, I can\u2019t answer this question using SQL.\u201d do not specify the exact reason why the question cannot be answered, which can dis- courage the users from asking follow-up questions.The ambiguous/unanswerable questions in CoSQL are not categorized into fine categories, probably due to the small size of such questions (12% of the whole dataset). Our work fills this gap by gener- ating a large number of ambiguous/unanswerable questions using eight different methods. With the advent of LLMs, there has been a wider focus on conversational dialogue-oriented systems that can engage with users helpfully to solve a particular task ((Wang et al., 2023b), (Zhang et al., 2023), (Deng et al., 2023)). We convert our data into conversational forms leveraging reverse generation (generating SQL first and then generating user clar- ification responses) using an LLM (see Figure 1).     2.2 Ambiguity and Unanswerability in text-to-SQL systems     Recent research has identified the presence of am- biguous and unanswerable questions in practical text-to-SQL systems. However, they primarily fo- cused on creating ambiguous or unanswerable data to train question classifiers (Zhang et al., 2020) or covered only a limited range of ambiguous/u- nanswerable categories (Wang et al., 2023a). Con- currently, Bhaskar et al. (2023) introduced Am- biQT, a benchmark containing ambiguous text-to- SQL queries spanning four ambiguous categories, and suggested generating multiple SQL queries to encompass the correct SQL. More recently, AM- BROSIA defined and generated ambiguous text-to- SQL data based on scope ambiguity, attachment ambiguity, and vagueness but did not cover unan- swerable categories (Saparina and Lapata, 2024). Text2Analysis (He et al., 2024) focuses on struc- tured data and also includes unclear queries, how- ever, its queries focus more on advanced analysis skills rather than text-to-SQL. Our dataset differs in several key aspects. First, we address more com- prehensive and fine-grained categories considering both ambiguous and unanswerable queries. Sec- ond, PRACTIQ extends the generated data into a conversational format, reflecting the resolution of the problem in the original user query through in- teractions, resembling practical settings. Lastly, we handle cases with ambiguous inputs that can be addressed without explicitly needing a user clar- ification by directly generating helpful SQL and natural language responses covering all ambiguous columns in the database for the Ambiguous SE- LECT Column and Ambiguous WHERE Column questions. Table 1 compares the ambiguous and unanswerable categories defined in our work with     existing datasets, highlighting the range of broader categories covered in PRACTIQ. By addressing the limitations of existing datasets and providing a comprehensive and conversational dataset, our work aims to support the development of practical text-to-SQL applications that can handle ambigu- ous and unanswerable queries more effectively.     3 Question Categorization & Dataset Construction     We analyzed public text-to-SQL datasets like Spi- der (Yu et al., 2018), BIRD (Li et al., 2024), CoSQL (Yu et al., 2019a) and proposed four ambiguous and four unanswerable categories, as shown in Table 2. The ambiguous categories include Am- biguous SELECT Column, Ambiguous WHERE Columns, Ambiguous Values Within Columns, and Ambiguous Filter Criteria.",
            "Lastly, we handle cases with ambiguous inputs that can be addressed without explicitly needing a user clar- ification by directly generating helpful SQL and natural language responses covering all ambiguous columns in the database for the Ambiguous SE- LECT Column and Ambiguous WHERE Column questions. Table 1 compares the ambiguous and unanswerable categories defined in our work with     existing datasets, highlighting the range of broader categories covered in PRACTIQ. By addressing the limitations of existing datasets and providing a comprehensive and conversational dataset, our work aims to support the development of practical text-to-SQL applications that can handle ambigu- ous and unanswerable queries more effectively.     3 Question Categorization & Dataset Construction     We analyzed public text-to-SQL datasets like Spi- der (Yu et al., 2018), BIRD (Li et al., 2024), CoSQL (Yu et al., 2019a) and proposed four ambiguous and four unanswerable categories, as shown in Table 2. The ambiguous categories include Am- biguous SELECT Column, Ambiguous WHERE Columns, Ambiguous Values Within Columns, and Ambiguous Filter Criteria. Ambiguous questions have multiple possible interpretations and subse- quently multiple correct SQL responses given the database schema. The unanswerable categories in- clude Nonexistent SELECT Column, Nonexistent WHERE Column, Nonexistent Filter Value, and Unsupported Join. Unanswerable questions are those for which a valid SQL cannot be produced given the database schema.     The data generation process consists of three main stages, as shown in Figure 1. We describe the main procedure and illustrate it with a detailed explanation for one category. For convenience, we use \"assistant\" to indicate the text-to-SQL system in the remaining text. Please see Appendix E for a detailed explanation of the data generation process for each category.     3.1 Stage 1: SQL parsing & Database modification     We first extract the columns and cell values by pars- ing the SQL queries using a custom parser on top of SQLGLOT4. Then, we select a column or cell value of interest and modify the database schemas using an LLM so that the question becomes ambiguous or unanswerable. Since users are often unaware of database details, modifying the databases instead of the user questions, when plausible, is a natural way to create ambiguous and unanswerable questions. For example, for Ambiguous SELECT Column questions, we asked the LLM to generate two al- ternative columns to replace the original column mentioned in the question, such that either column     4https://github.com/tobymao/sqlglotis a valid interpretation of the question (see Prompt 4 for details). For Nonexistent Filter Value ques- tions, we remove the mentioned cell values from the database, making the question unanswerable. For example, given the user question \"What is the maximum capacity of all stadiums?\" and the origi- nal database schema with the column \"Capacity\", we prompt the LLM to generate two semantically similar but non-equivalent columns, \"Standing Ca- pacity\" and \"Seating Capacity\". We then remove the original \"Capacity\" column and add the newly generated columns to the database.     3.2 Stage 2: SQL modification and clarification response generation     Based on the user question, the modified database, and the original SQL, we generate the text-to-SQL assistant\u2019s initial response to the ambiguous/unan- swerable question, the following user clarification response, and the assistant\u2019s SQL response to the clarified question. First, we generate the assistant\u2019s response to the initial user question using either a template-based method or a prompting method. For example, for Ambiguous SELECT Column questions, the template is \"I find two conflicting information in the data. Which one would you like to know about? Ambiguous_SELECT_Column_1 or Ambiguous_SELECT_Column_2\".     Next, we follow a reverse-generation process (Hu et al., 2023) to first generate the assistant\u2019s final SQL response and then generate the user\u2019s clarification question. The assistant\u2019s final SQL response is generated by modifying the original SQL programmatically. Then, we prompt the LLM to fill in the user\u2019s clarification response based on the conversation context (initial user question, as- sistant\u2019s clarification question, and final SQL re- sponses). For example, for the Ambiguous SE- LECT Column question, we generate the assis- tant\u2019s clarified SQL by replacing the column in the SELECT clause of the original SQL with one of the ambiguous SELECT columns generated in the above stage. Then, given the user\u2019s ini- tial question, the assistant\u2019s clarification question, \"empty_user_clarification_response\", and the assis- tant\u2019s final SQL response, we prompt the LLM to fill in the \"empty_user_clarification_response\" so that the user clarification response matches the as- sistant\u2019s SQL response and rest of the conversation (see Prompt 5 for details)."
        ],
        "gt_answer": "The four categories of ambiguous questions identified in the PRACTIQ dataset are: (1) Ambiguity due to multiple columns with similar names, (2) Ambiguity due to multiple tables containing similar information, (3) Ambiguity arising from unspecified aggregation operations, and (4) Ambiguity caused by vague temporal references.",
        "query_metadata": {
            "input_token": 0,
            "latency_ms": 0
        },
        "sagemaker_cost": {
            "sagemaker_inference_cost": 0.0034115277777777776
        }
    },
    {
        "question": "What are the four categories of unanswerable questions identified in the PRACTIQ dataset?",
        "answer": "The four categories of unanswerable questions identified in the PRACTIQ dataset are: Nonexistent_SELECT_Column, Nonexistent_WHERE_Column, Unsupported_Join, and Nonexistent_Filter_Value.",
        "guardrails_output_assessment": null,
        "guardrails_context_assessment": null,
        "guardrails_input_assessment": null,
        "guardrails_blocked": false,
        "guardrails_block_level": "",
        "answer_metadata": {
            "inputTokens": 3355,
            "outputTokens": 46,
            "totalTokens": 3401,
            "latencyMs": 2415
        },
        "reference_contexts": [
            "Question: Which student borrowed the book titled \u201cABC\u201d from the library \u201cXYZ\u201d? Unanswerability: To answer this question, we need to join the student table with library-books tables. This JOIN operation is not possible as there are no overlapping columns or foreign keys that connect the two tables.     2 Related Work     2.1 Standard text-to-SQL datasets     Most text-to-SQL datasets, such as Spider (Yu et al., 2018), BIRD (Wang et al., 2023a), and WikiSQL (Zhong et al., 2017), consist of non-conversational, answerable questions with clear intent. SPARC and CoSQL are conversational but only have a very limited number of ambiguous or unanswerable questions (Finegan-Dollak et al., 2018; Yu et al., 2019b,a). E.g., CoSQL contains around 10k anno-     tated SQL queries from 3k dialogues spread across 200 complex databases, but there are only approx- imately 190 unanswerable questions and only 34 (approximately 18%) of them request a user clari- fication to resolve the issue in the next turn. Also, the responses by the text-to-SQL system to such questions are not always helpful. For example, re- sponses like \u201cSorry, I can\u2019t answer this question using SQL.\u201d do not specify the exact reason why the question cannot be answered, which can dis- courage the users from asking follow-up questions.The ambiguous/unanswerable questions in CoSQL are not categorized into fine categories, probably due to the small size of such questions (12% of the whole dataset). Our work fills this gap by gener- ating a large number of ambiguous/unanswerable questions using eight different methods. With the advent of LLMs, there has been a wider focus on conversational dialogue-oriented systems that can engage with users helpfully to solve a particular task ((Wang et al., 2023b), (Zhang et al., 2023), (Deng et al., 2023)). We convert our data into conversational forms leveraging reverse generation (generating SQL first and then generating user clar- ification responses) using an LLM (see Figure 1).     2.2 Ambiguity and Unanswerability in text-to-SQL systems     Recent research has identified the presence of am- biguous and unanswerable questions in practical text-to-SQL systems. However, they primarily fo- cused on creating ambiguous or unanswerable data to train question classifiers (Zhang et al., 2020) or covered only a limited range of ambiguous/u- nanswerable categories (Wang et al., 2023a). Con- currently, Bhaskar et al. (2023) introduced Am- biQT, a benchmark containing ambiguous text-to- SQL queries spanning four ambiguous categories, and suggested generating multiple SQL queries to encompass the correct SQL. More recently, AM- BROSIA defined and generated ambiguous text-to- SQL data based on scope ambiguity, attachment ambiguity, and vagueness but did not cover unan- swerable categories (Saparina and Lapata, 2024). Text2Analysis (He et al., 2024) focuses on struc- tured data and also includes unclear queries, how- ever, its queries focus more on advanced analysis skills rather than text-to-SQL. Our dataset differs in several key aspects. First, we address more com- prehensive and fine-grained categories considering both ambiguous and unanswerable queries. Sec- ond, PRACTIQ extends the generated data into a conversational format, reflecting the resolution of the problem in the original user query through in- teractions, resembling practical settings. Lastly, we handle cases with ambiguous inputs that can be addressed without explicitly needing a user clar- ification by directly generating helpful SQL and natural language responses covering all ambiguous columns in the database for the Ambiguous SE- LECT Column and Ambiguous WHERE Column questions. Table 1 compares the ambiguous and unanswerable categories defined in our work with     existing datasets, highlighting the range of broader categories covered in PRACTIQ. By addressing the limitations of existing datasets and providing a comprehensive and conversational dataset, our work aims to support the development of practical text-to-SQL applications that can handle ambigu- ous and unanswerable queries more effectively.     3 Question Categorization & Dataset Construction     We analyzed public text-to-SQL datasets like Spi- der (Yu et al., 2018), BIRD (Li et al., 2024), CoSQL (Yu et al., 2019a) and proposed four ambiguous and four unanswerable categories, as shown in Table 2. The ambiguous categories include Am- biguous SELECT Column, Ambiguous WHERE Columns, Ambiguous Values Within Columns, and Ambiguous Filter Criteria.",
            "\"{category_with_explanation}\" will be replaced with the name and definition of four ambiguous and four unanswerable categories in Figure 10. Few-shots examples are presented as conversation between the user and assistant in the format of message API of litellm.     You are a Database Expert (DB EXPERT) system that classifies user questions into one of the following 9 categories based on the given database schema:     { category_with_explanation }     \u2212 answerable: the database contains data needed to answer the question and the question has one and only one valid interpreation .     You will receive : 1. A database schema in markdown format with relevant column values enclosed in <schema/> tags . 2. The user ' s question enclosed in <question/> tags .     Your output should follow this format : <scratch> YOUR\u2212STEP\u2212BY\u2212STEP\u2212THOUGHTS </scratch> < result > ONE\u2212OF\u2212THE\u22129\u2212QUESTION\u2212CATEGORIES </result>     Note that the \"answerable\" output shall only be provided if and only if : \u2212 the question posed is unambiguous, precise , and leaving no room for multiple interpretations or confusion . \u2212 the database contains the complete set of information required to formulate a comprehensive and accurate response to the query. If either of these conditions is not met, meaning the question lacks clarity or our data is insufficient , we shall refrain from classifying the query as \"answerable .\"     Provide your step\u2212by\u2212step thoughts within </ scratch > tags . Then, provide your final classification within < result /> tags as one of the categories above. Do not include any extra explanation in the result .     Figure 10: Ambiguous and Unanswerable category and their definition. These definitions are used in the binary and 9-way classification.     category_to_explanation_mapping = { \"Ambiguous_SELECT_Column\":     \"Multiple columns match the requested output information , leading to multiple valid SQLs that differ in the columns used in the SELECT clause.\", \"Ambiguous_WHERE_Column\":     \"The filter condition matches multiple columns in a table , leading to multiple valid SQLs that differ in the specific filter column in the WHERE clause.\", \"Ambiguous_Values_Within_Column\":     \"Multiple similar values in a column match the mentioned value , leading to multiple valid SQLs that differ in the specific filtering cell value from one column in the WHERE clause.\",     \" Ambiguous_Filter_Criteria \": \"The question contains a filter condition or criteria that is ambiguous, vague, relative / descriptive , or open to multiple interpretations .\" \"This ambiguity in the filter criteria makes it difficult to formulate a precise SQL query without additional clarification .\",     \"Nonexistent_SELECT_Column\": \"At least one of the requested output information is not present in the schema, so no SQL can be constructed with the appropriate column in the SELECT clause.\",     \"Nonexistent_WHERE_Column\": \"At least one filter condition column is not present in the schema, so no SQL can be constructed with the appropriate column in the WHERE clause\",     \"Unsupported_Join\": \"The required join between tables is not supported due to a lack of common columns, preventing the construction of a valid SQL query\",     \" Nonexistent_Filter_Value \": \"The mentioned filtering value is not present in the schema, so no SQL can be constructed with the appropriate value in the WHERE clause.\",     }Figure 11: Example user message and assistant message that are used as fewshots in the question category classification.",
            "To bridge this gap, we introduce PRACTIQ which is a practical conversational text-to-SQL dataset with ambiguous and unanswerable queries. As illustrated in Table 2, a question is ambiguous if it has multiple valid interpretations given the database schema and the question is unanswerable if the corresponding database does not contain the data that the question is asking for. In the real world, given a user question, a text-to-SQL assis- tant has to first determine whether the question is answerable, ambiguous, or unanswerable to decide whether to ask for clarification questions or respond with the correct SQL.     We begin by examining existing text-to-SQL datasets (Yu et al., 2018; Li et al., 2024; Yu et al., 2019a) and identify four ambiguous and four unan- swerable categories inspired by real-world practical user questions. Subsequently, we generate ambigu-     2As of August 2023           https://github.com/amazon-science/conversational-ambiguous-unanswerable-text2sql         https://github.com/amazon-science/conversational-ambiguous-unanswerable-text2sqlTable Visitor     ID, Name, Age     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: SELECT Name, Age from Visitors WHERE ID LIKE 'B%';     Age     Age_at_Entry Current_Age     LLM     Table Visitor     ID, Name, Age_at_Entry, Current_Age     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: I find two matching columns for Age - Age_at_Entry and Current_age     U: <Fill Here>     A: SELECT Name, Age_at_Entry from Visitors WHERE ID LIKE 'B%';     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: I find two matching columns for Age - Age_at_Entry and Current_age     U: I am interested in the age they entered.     A: SELECT Name, Age_at_Entry from Visitors WHERE ID LIKE 'B%';     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: I find two matching columns for Age - Age_at_Entry and Current_age     U: I am interested in the age they entered.     A: SELECT Name, Age_at_Entry from Visitors WHERE ID LIKE 'B%';     A: [('Jack', 25), ('Jane', 24)]     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: For the visitor's age, would you like their age at entry or their current age?     U: I am interested in the age they entered.     A: SELECT Name, Age_at_Entry from Visitors WHERE ID LIKE 'B%';     A: [('Jack', 25), ('Jane', 24)]     A: The name and age at entry of visitors whose ID starts with 'B' are Jack with age 25 and Jane with age 24.     Stage 1 - Database Modification     Stage 2 - Reverse Generation     Stage 3 - Refine Conversation     SQL Engine LLM     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: SELECT Name, Age_at_Entry, Current_Age from Visitors WHERE ID LIKE 'B%';     Stage 2 -Helpful SQL Generation     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: SELECT Name, Age_at_Entry, Current_Age from Visitors WHERE ID LIKE 'B%';     A: [('Jack', 25, 28), ('Jane', 24, 27)]     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: SELECT Name, Age_at_Entry, Current_Age from Visitors WHERE ID LIKE 'B%';     A: [('Jack', 25, 28), ('Jane', 24, 27)]     A: The name, age at entry and current age of visitors whose ID starts with 'B' are Jack with age at entry 25 and current age 28 and Jane with age at entry 24 and current age 27.Stage 3 - Refine     Conversation     Modified Schema     SQL Engine     LLM     LLM     Figure 1: An example of our conversational dataset construction consists of three stages: database modification, SQL modification along with clarification response generation, and refining the conversation. The top box depicts our data construction for an ambiguous question that requires clarification questions, while the bottom box illustrates an ambiguous question with direct helpful SQL responses. Here \u2018U\u2019 refers to a user and \u2018A\u2019 refers to a text-to-SQL assistant."
        ],
        "gt_answer": "The four categories of unanswerable questions identified in the PRACTIQ dataset are: (1) Questions about non-existent entities, (2) Questions requiring external knowledge not present in the database, (3) Questions involving data that is missing or incomplete, and (4) Questions that are logically inconsistent or contradictory.",
        "query_metadata": {
            "input_token": 0,
            "latency_ms": 0
        },
        "sagemaker_cost": {
            "sagemaker_inference_cost": 0.0008117083333333332
        }
    },
    {
        "question": "How does the PRACTIQ dataset handle ambiguous queries without seeking user clarification?",
        "answer": "Sorry, I don't have sufficient information to provide an answer. There is no need to explain the reasoning behind your answers. I don't have enough information to answer your question. The provided references do not contain information about how the PRACTIQ dataset handles ambiguous queries without seeking user clarification. The references discuss the construction of the PRACTIQ dataset, its categories, and the challenges of handling ambiguous and unanswerable queries, but they do not provide specific information on how the dataset handles ambiguous queries without clarification. Therefore, I cannot provide a precise answer to your question. However, the references do mention that for some ambiguous queries, the dataset directly generates helpful SQL responses that consider multiple aspects of ambiguity, instead of requesting user clarification. This suggests that the dataset may have some mechanisms in place to handle ambiguous queries without seeking user clarification, but the specific details of these mechanisms are not provided in the references. If you have any further questions or need more information, please let me know, and I'll do my best to assist you. Please note that the references provided are related to the PRACTIQ dataset and its construction, but they do not contain information on how the dataset handles ambiguous queries without seeking user clarification.",
        "guardrails_output_assessment": null,
        "guardrails_context_assessment": null,
        "guardrails_input_assessment": null,
        "guardrails_blocked": false,
        "guardrails_block_level": "",
        "answer_metadata": {
            "inputTokens": 3639,
            "outputTokens": 365,
            "totalTokens": 4004,
            "latencyMs": 10345
        },
        "reference_contexts": [
            "To bridge this gap, we introduce PRACTIQ which is a practical conversational text-to-SQL dataset with ambiguous and unanswerable queries. As illustrated in Table 2, a question is ambiguous if it has multiple valid interpretations given the database schema and the question is unanswerable if the corresponding database does not contain the data that the question is asking for. In the real world, given a user question, a text-to-SQL assis- tant has to first determine whether the question is answerable, ambiguous, or unanswerable to decide whether to ask for clarification questions or respond with the correct SQL.     We begin by examining existing text-to-SQL datasets (Yu et al., 2018; Li et al., 2024; Yu et al., 2019a) and identify four ambiguous and four unan- swerable categories inspired by real-world practical user questions. Subsequently, we generate ambigu-     2As of August 2023           https://github.com/amazon-science/conversational-ambiguous-unanswerable-text2sql         https://github.com/amazon-science/conversational-ambiguous-unanswerable-text2sqlTable Visitor     ID, Name, Age     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: SELECT Name, Age from Visitors WHERE ID LIKE 'B%';     Age     Age_at_Entry Current_Age     LLM     Table Visitor     ID, Name, Age_at_Entry, Current_Age     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: I find two matching columns for Age - Age_at_Entry and Current_age     U: <Fill Here>     A: SELECT Name, Age_at_Entry from Visitors WHERE ID LIKE 'B%';     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: I find two matching columns for Age - Age_at_Entry and Current_age     U: I am interested in the age they entered.     A: SELECT Name, Age_at_Entry from Visitors WHERE ID LIKE 'B%';     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: I find two matching columns for Age - Age_at_Entry and Current_age     U: I am interested in the age they entered.     A: SELECT Name, Age_at_Entry from Visitors WHERE ID LIKE 'B%';     A: [('Jack', 25), ('Jane', 24)]     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: For the visitor's age, would you like their age at entry or their current age?     U: I am interested in the age they entered.     A: SELECT Name, Age_at_Entry from Visitors WHERE ID LIKE 'B%';     A: [('Jack', 25), ('Jane', 24)]     A: The name and age at entry of visitors whose ID starts with 'B' are Jack with age 25 and Jane with age 24.     Stage 1 - Database Modification     Stage 2 - Reverse Generation     Stage 3 - Refine Conversation     SQL Engine LLM     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: SELECT Name, Age_at_Entry, Current_Age from Visitors WHERE ID LIKE 'B%';     Stage 2 -Helpful SQL Generation     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: SELECT Name, Age_at_Entry, Current_Age from Visitors WHERE ID LIKE 'B%';     A: [('Jack', 25, 28), ('Jane', 24, 27)]     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: SELECT Name, Age_at_Entry, Current_Age from Visitors WHERE ID LIKE 'B%';     A: [('Jack', 25, 28), ('Jane', 24, 27)]     A: The name, age at entry and current age of visitors whose ID starts with 'B' are Jack with age at entry 25 and current age 28 and Jane with age at entry 24 and current age 27.Stage 3 - Refine     Conversation     Modified Schema     SQL Engine     LLM     LLM     Figure 1: An example of our conversational dataset construction consists of three stages: database modification, SQL modification along with clarification response generation, and refining the conversation. The top box depicts our data construction for an ambiguous question that requires clarification questions, while the bottom box illustrates an ambiguous question with direct helpful SQL responses. Here \u2018U\u2019 refers to a user and \u2018A\u2019 refers to a text-to-SQL assistant.",
            "Lastly, we handle cases with ambiguous inputs that can be addressed without explicitly needing a user clar- ification by directly generating helpful SQL and natural language responses covering all ambiguous columns in the database for the Ambiguous SE- LECT Column and Ambiguous WHERE Column questions. Table 1 compares the ambiguous and unanswerable categories defined in our work with     existing datasets, highlighting the range of broader categories covered in PRACTIQ. By addressing the limitations of existing datasets and providing a comprehensive and conversational dataset, our work aims to support the development of practical text-to-SQL applications that can handle ambigu- ous and unanswerable queries more effectively.     3 Question Categorization & Dataset Construction     We analyzed public text-to-SQL datasets like Spi- der (Yu et al., 2018), BIRD (Li et al., 2024), CoSQL (Yu et al., 2019a) and proposed four ambiguous and four unanswerable categories, as shown in Table 2. The ambiguous categories include Am- biguous SELECT Column, Ambiguous WHERE Columns, Ambiguous Values Within Columns, and Ambiguous Filter Criteria. Ambiguous questions have multiple possible interpretations and subse- quently multiple correct SQL responses given the database schema. The unanswerable categories in- clude Nonexistent SELECT Column, Nonexistent WHERE Column, Nonexistent Filter Value, and Unsupported Join. Unanswerable questions are those for which a valid SQL cannot be produced given the database schema.     The data generation process consists of three main stages, as shown in Figure 1. We describe the main procedure and illustrate it with a detailed explanation for one category. For convenience, we use \"assistant\" to indicate the text-to-SQL system in the remaining text. Please see Appendix E for a detailed explanation of the data generation process for each category.     3.1 Stage 1: SQL parsing & Database modification     We first extract the columns and cell values by pars- ing the SQL queries using a custom parser on top of SQLGLOT4. Then, we select a column or cell value of interest and modify the database schemas using an LLM so that the question becomes ambiguous or unanswerable. Since users are often unaware of database details, modifying the databases instead of the user questions, when plausible, is a natural way to create ambiguous and unanswerable questions. For example, for Ambiguous SELECT Column questions, we asked the LLM to generate two al- ternative columns to replace the original column mentioned in the question, such that either column     4https://github.com/tobymao/sqlglotis a valid interpretation of the question (see Prompt 4 for details). For Nonexistent Filter Value ques- tions, we remove the mentioned cell values from the database, making the question unanswerable. For example, given the user question \"What is the maximum capacity of all stadiums?\" and the origi- nal database schema with the column \"Capacity\", we prompt the LLM to generate two semantically similar but non-equivalent columns, \"Standing Ca- pacity\" and \"Seating Capacity\". We then remove the original \"Capacity\" column and add the newly generated columns to the database.     3.2 Stage 2: SQL modification and clarification response generation     Based on the user question, the modified database, and the original SQL, we generate the text-to-SQL assistant\u2019s initial response to the ambiguous/unan- swerable question, the following user clarification response, and the assistant\u2019s SQL response to the clarified question. First, we generate the assistant\u2019s response to the initial user question using either a template-based method or a prompting method. For example, for Ambiguous SELECT Column questions, the template is \"I find two conflicting information in the data. Which one would you like to know about? Ambiguous_SELECT_Column_1 or Ambiguous_SELECT_Column_2\".     Next, we follow a reverse-generation process (Hu et al., 2023) to first generate the assistant\u2019s final SQL response and then generate the user\u2019s clarification question. The assistant\u2019s final SQL response is generated by modifying the original SQL programmatically. Then, we prompt the LLM to fill in the user\u2019s clarification response based on the conversation context (initial user question, as- sistant\u2019s clarification question, and final SQL re- sponses). For example, for the Ambiguous SE- LECT Column question, we generate the assis- tant\u2019s clarified SQL by replacing the column in the SELECT clause of the original SQL with one of the ambiguous SELECT columns generated in the above stage. Then, given the user\u2019s ini- tial question, the assistant\u2019s clarification question, \"empty_user_clarification_response\", and the assis- tant\u2019s final SQL response, we prompt the LLM to fill in the \"empty_user_clarification_response\" so that the user clarification response matches the as- sistant\u2019s SQL response and rest of the conversation (see Prompt 5 for details).",
            "PRACTIQ: A Practical Conversational text-to-SQL dataset with Ambiguous and Unanswerable Queries     Mingwen Dong\u2020\u2217 Nischal Ashok Kumar\u2021*     Yiqun Hu\u2020, Anuj Chauhan\u2020, Chung-Wei Hang\u2020, Shuaichen Chang\u2020, Lin Pan\u2020, Wuwei Lan\u2020, Henghui Zhu\u2020, Jiarong Jiang\u2020, Patrick Ng\u2020, Zhiguo Wang\u2020     \u2021University of Massachusetts at Amherst, \u2020Amazon Web Services nashokkumar@cs.umass.edu, {mingwd, jiarongj, zhiguow}@amazon.com     Abstract     Previous text-to-SQL datasets and systems have primarily focused on user questions with clear intentions that can be answered. How- ever, real user questions can often be ambigu- ous with multiple interpretations or unanswer- able due to a lack of relevant data. In this work, we construct a practical conversational text-to- SQL dataset called PRACTIQ, consisting of am- biguous and unanswerable questions inspired by real-world user questions. We first identi- fied four categories of ambiguous questions and four categories of unanswerable questions by studying existing text-to-SQL datasets. Then, we generate conversations with four turns: the initial user question, an assistant response seek- ing clarification, the user\u2019s clarification, and the assistant\u2019s clarified SQL response with the natural language explanation of the execution results. For some ambiguous queries, we also directly generate helpful SQL responses, that consider multiple aspects of ambiguity, instead of requesting user clarification. To benchmark the performance on ambiguous, unanswerable, and answerable questions, we implemented large language model (LLM)-based baselines using various LLMs. Our approach involves two steps: question category classification and clarification SQL prediction. Our experiments reveal that state-of-the-art systems struggle to handle ambiguous and unanswerable questions effectively. We will release our code for data generation and experiments on GitHub1.     1 Introduction     Text-to-SQL systems aim to convert natural lan- guage questions into SQL queries that can be used to query a database. The systems serve as an inter- face between users and databases to allow the users     *Co-first authors with equal contribution. Work done while Nischal Ashok Kumar was an intern at AWS.     1https://github.com/amazon-science/ conversational-ambiguous-unanswerable-text2sql     access to information from the databases through their natural language questions. The advent of Large Language Models (LLMs) (Bubeck et al., 2023) has significantly enhanced the capabilities of text-to-SQL systems, such as DIN-SQL (Pourreza and Rafiei, 2024), achieving state-of-the-art (SoTA) performance on standard benchmarks2, including Spider (Yu et al., 2018) and BIRD (Li et al., 2024).     Although the SoTA text-to-SQL systems per- form well on clean benchmarks that contain only answerable user queries, they are still not well- equipped to deal with practical real-world data which have ambiguous or unanswerable questions (Wang et al., 2023a). The poor performance of SoTA text-to-SQL systems is primarily due to the unavailability of practical text-to-SQL data that can be used for training (Wang et al., 2023a). Al- though previous research finds that a large ratio of user questions are unanswerable, these are often ex- cluded in the previous datasets as addressing them requires more than SQL annotations (Lee et al., 2021). To bridge this gap, we introduce PRACTIQ which is a practical conversational text-to-SQL dataset with ambiguous and unanswerable queries. As illustrated in Table 2, a question is ambiguous if it has multiple valid interpretations given the database schema and the question is unanswerable if the corresponding database does not contain the data that the question is asking for. In the real world, given a user question, a text-to-SQL assis- tant has to first determine whether the question is answerable, ambiguous, or unanswerable to decide whether to ask for clarification questions or respond with the correct SQL.     We begin by examining existing text-to-SQL datasets (Yu et al., 2018; Li et al., 2024; Yu et al., 2019a) and identify four ambiguous and four unan- swerable categories inspired by real-world practical user questions."
        ],
        "gt_answer": "For some ambiguous queries, the PRACTIQ dataset includes helpful SQL responses that consider multiple aspects of ambiguity, providing direct answers without requesting user clarification.",
        "query_metadata": {
            "input_token": 0,
            "latency_ms": 0
        },
        "sagemaker_cost": {
            "sagemaker_inference_cost": 0.003477069444444444
        }
    },
    {
        "question": "What approach was used to benchmark performance on the PRACTIQ dataset?",
        "answer": "Sorry, I don't have sufficient information to provide an answer. There is no need to explain the reasoning behind your answers.",
        "guardrails_output_assessment": null,
        "guardrails_context_assessment": null,
        "guardrails_input_assessment": null,
        "guardrails_blocked": false,
        "guardrails_block_level": "",
        "answer_metadata": {
            "inputTokens": 2940,
            "outputTokens": 32,
            "totalTokens": 2972,
            "latencyMs": 1781
        },
        "reference_contexts": [
            "We imple- ment the DIN-SQL model using Claude 3.5 Son- net, Claude 3 Sonnet, Llama3-1-70B, Llama3-1- 8B, and Mixtral-Large-2 by tailoring the original GPT-4 based prompts and using the same set of hyperparameters as that used by GPT-4. Future work can focus on evaluating our dataset with the DIN-SQL model implemented using GPT-4.     G Dataset Access and Distribution     We will make the code and prompt used to used for generating and benchmarking the data open- source under the MIT License10 for the community to access and contribute. We use the open-source Spider11 dataset for creating PRACTIQ. The Spider dataset is governed by CC BY-SA 4.0 license which allows us to freely use the data for modification. To the best of our knowledge, we make sure that the dataset does not contain the private information of any individual or entity.     10https://opensource.org/license/mit/ 11We only used \u201ctrain_spider.json\u201d and \u201cdev.json\u201d from     Spider in our dataset construction and related experiments. We did NOT use any data from \u201ctrain_others.json\u201d or \u201cyelp\u201d.           https://www.anthropic.com/         https://aws.amazon.com/bedrock/         https://aws.amazon.com/bedrock/         https://opensource.org/license/mit/",
            "E.4 Nonexistent SELECT Column     In stage 1, we extract the columns appearing in the Select clause of the SQLs of all questions in the Spider dataset and construct new schemas by removing the columns required for answering the respective questions. In stage 2, we construct the assistant\u2019s helpful response using a template that states that the column required for answering the question is missing from the schema. We construct the final SQL by replacing the missing column from the schema (in the Select clause) with a col- umn that exists in the schema. We convert this data into conversational data and prompt the model to generate the user clarification response. In stage 3, we refine the conversation, execute the clarifica- tion SQL get the results, and generate the natural language explanation of the results.     E.5 Nonexistent Filter Value     In stage 1, we extract the values appearing in the Where clause of the SQLs of all questions in the Spider dataset. For constructing the problematic data we construct a new schema by removing the values required for answering the question from the schema. In stage 2, we construct the assistant\u2019s helpful response using a template that mentions that the value mentioned in the question is not present in the schema. We construct the clarification SQL by replacing the removed value with another value present in the schema. We then convert this data into conversational data and prompt the LLM to generate the user clarification response. In stage 3, we refine the conversation, execute the clarifica- tion SQL get the results, and generate the naturallanguage explanation of the results.     E.6 Unsupported Join     In stage 1, to construct the problematic data, we consider the unique schemas of the Spider dataset and prompt the LLM to generate a new schema with at least two new tables and corresponding columns such that the new tables have a foreign key relationship with themselves but not with any other column in the schema. For example, for a schema containing student information like student grade, teacher details, etc. the LLM produces two new tables of library and books that have a foreign key relationship with each other but not with any other table in the original schema. In stage 2, we construct the assistant\u2019s helpful response using a template stating that the question requires joining tables of the schema that have no relationship with each other. We construct a clarification SQL by using SQL from the Spider dataset corresponding to the original schema. We then convert this data into conversational data and prompt the LLM to generate the user clarification response. In stage 3, we refine the conversation, execute the clarifica- tion SQL get the results, and generate the natural language explanation of the results.     E.7 Nonexistent WHERE Column     In stage 1, we extract the columns present in the Where clause of the SQLs in the Spider dataset and construct new schemas by removing the columns required for answering the respective questions. In stage 2, we construct the assistant\u2019s helpful re- sponse using a template that mentions that the in- formation required for answering the question is not present in the schema. We construct the clar- ification SQL by finding a SQL from the Spider dataset whose Select columns match the problem- atic question and whose Where columns are present in the schema. We convert this data into a conver- sational format and prompt the LLM to generate the user clarification response. In stage 3, we re- fine the conversation, execute the clarification SQL get the results, and generate the natural language explanation of the results.     F Experimental Settings     We use Anthropic AI\u2019s Claude 3 Sonnet via Ama- zon Bedrock 9 for all our data generation. For the     9https://www.anthropic.com/, https://aws. amazon.com/bedrock/     zero-shot and the few-shot prompts designed for evaluating the dataset, we use Claude 3 Sonnet, Haiku, Llama-3.1 70B, and LLama-3-1-8B with a greedy decoding strategy, i.e., we set the top-p value to 1.0 and temperature to 0.0. We imple- ment the DIN-SQL model using Claude 3.5 Son- net, Claude 3 Sonnet, Llama3-1-70B, Llama3-1- 8B, and Mixtral-Large-2 by tailoring the original GPT-4 based prompts and using the same set of hyperparameters as that used by GPT-4. Future work can focus on evaluating our dataset with the DIN-SQL model implemented using GPT-4.     G Dataset Access and Distribution     We will make the code and prompt used to used for generating and benchmarking the data open- source under the MIT License10 for the community to access and contribute. We use the open-source Spider11 dataset for creating PRACTIQ. The Spider dataset is governed by CC BY-SA 4.0 license which allows us to freely use the data for modification.",
            "Table 5 shows our baseline method\u2019s (DIN-SQL) performance on SQL prediction of various LLMs given the interaction between the user and the as- sistant. Overall, Mixtral-large-v2 and Claude 3.5 Sonnet achieve the highest average accuracy of 71.95% and 72.15% on the ambiguous/unanswer- able questions. Claude 3.5 sonnet achieves the high- est performance of 79.21% on the answerable ques- tions (original Spider dev set). The open-source model Llama-3.1 70B performs competitively on the answerable questions achieving 76.31% ac- curacy, only 2.9% lower than Claude 3.5 sonnet. However, it performs only at 67.58% accuracy on ambiguous/unanswerable questions, lagging 3.7% behind Claude 3.5 sonnet. The gap can be as large as 9% for some specific ambiguous question cat- egories, indicating room for improvement. Our framework can be used to generate training data to improve open-source models\u2019 capabilities in both SQL prediction and detecting ambiguous/u- nanswerable questions.     6 Conclusion and Future work     In this work, we study current public text-to- SQL datasets and define four ambiguous and four unanswerable categories. We propose a frame- work to construct a practical conversational text-to- SQL dataset, PRACTIQ, using both carefully con-     8With 3 examples per category, the results are similar, and the evaluation was stopped early for Llama-3.1 70B.     structed rules and Large Language Models (LLMs). We use the Spider dev dataset for constructing PRACTIQ and generate around 2,800 conversa- tional data samples. We evaluate our dataset on two core tasks, question category classification, and SQL prediction, and benchmark it using sev- eral SoTA LLMs.     Our results show that although some SoTA LLMs are approaching human-level accuracy, they are far from being perfect. For open-source models, the gap from human performance is much larger, indicating areas for further improvement. Our pro- posed framework provides a technique for gener- ating additional practical text-to-SQL data on ex- isting text-to-SQL datasets like WikiSQL, Spider Train, BIRD, or any other general synthetic single- turn answerable text-to-SQL data. This practical enhancement of the datasets can be used to further train open-source models to enhance their capabil- ities in handling practical text-to-SQL tasks (Liu et al., 2024).     In a broader sense, our work presents a simple agentic workflow to generate synthetic data, which can be further used to improve LLMs. In the future, we can fine-tune open-source models with data generated using our framework to improve their capabilities. We can also experiment with agentic workflows to benchmark our dataset, and determine whether a question is ambiguous, unanswerable, or answerable, and accordingly provide more accurate and helpful responses.     Limitations     While our dataset was generated using program- matic methods and LLMs, the data quality can be further improved by employing agentic workflows. Due to time constraints, we were unable to generate additional data to fine-tune open-source LLMs and evaluate whether fine-tuning can improve their abil- ity to detect ambiguous/unanswerable questions and perform other reasoning tasks. We leave theexploration of fine-tuning open-source LLMs and the potential improvements in their capabilities as future work. We also encourage the research com- munity to contribute to this effort by generating additional data using our proposed framework or exploring alternative approaches to enhance the quality and diversity of the dataset.     Ethics Statement     Since we prompt LLMs on a large scale through a rate-based API for both dataset creation as well as evaluation the project may not be very environment friendly and may inevitably cause the emission of more CO2.     References Adithya Bhaskar, Tushar Tomar, Ashutosh Sathe, and     Sunita Sarawagi. 2023. Benchmarking and improv- ing text-to-SQL generation under ambiguity. In Pro- ceedings of the 2023 Conference on Empirical Meth- ods in Natural Language Processing, pages 7053\u2013 7074, Singapore. Association for Computational Lin- guistics.     S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen El- dan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund- berg, et al. 2023. Sparks of artificial general intelli- gence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712."
        ],
        "gt_answer": "To benchmark performance on the PRACTIQ dataset, the authors implemented large language model (LLM)-based baselines using various LLMs. Their approach involves two steps: question category classification and clarification SQL prediction.",
        "query_metadata": {
            "input_token": 0,
            "latency_ms": 0
        },
        "sagemaker_cost": {
            "sagemaker_inference_cost": 0.0005986138888888889
        }
    },
    {
        "question": "What were the findings regarding state-of-the-art systems' performance on ambiguous and unanswerable questions?",
        "answer": "Sorry, I don't have sufficient information to provide an answer. There is no need to explain the reasoning behind your answers.",
        "guardrails_output_assessment": null,
        "guardrails_context_assessment": null,
        "guardrails_input_assessment": null,
        "guardrails_blocked": false,
        "guardrails_block_level": "",
        "answer_metadata": {
            "inputTokens": 3185,
            "outputTokens": 32,
            "totalTokens": 3217,
            "latencyMs": 1991
        },
        "reference_contexts": [
            "Question: Which student borrowed the book titled \u201cABC\u201d from the library \u201cXYZ\u201d? Unanswerability: To answer this question, we need to join the student table with library-books tables. This JOIN operation is not possible as there are no overlapping columns or foreign keys that connect the two tables.     2 Related Work     2.1 Standard text-to-SQL datasets     Most text-to-SQL datasets, such as Spider (Yu et al., 2018), BIRD (Wang et al., 2023a), and WikiSQL (Zhong et al., 2017), consist of non-conversational, answerable questions with clear intent. SPARC and CoSQL are conversational but only have a very limited number of ambiguous or unanswerable questions (Finegan-Dollak et al., 2018; Yu et al., 2019b,a). E.g., CoSQL contains around 10k anno-     tated SQL queries from 3k dialogues spread across 200 complex databases, but there are only approx- imately 190 unanswerable questions and only 34 (approximately 18%) of them request a user clari- fication to resolve the issue in the next turn. Also, the responses by the text-to-SQL system to such questions are not always helpful. For example, re- sponses like \u201cSorry, I can\u2019t answer this question using SQL.\u201d do not specify the exact reason why the question cannot be answered, which can dis- courage the users from asking follow-up questions.The ambiguous/unanswerable questions in CoSQL are not categorized into fine categories, probably due to the small size of such questions (12% of the whole dataset). Our work fills this gap by gener- ating a large number of ambiguous/unanswerable questions using eight different methods. With the advent of LLMs, there has been a wider focus on conversational dialogue-oriented systems that can engage with users helpfully to solve a particular task ((Wang et al., 2023b), (Zhang et al., 2023), (Deng et al., 2023)). We convert our data into conversational forms leveraging reverse generation (generating SQL first and then generating user clar- ification responses) using an LLM (see Figure 1).     2.2 Ambiguity and Unanswerability in text-to-SQL systems     Recent research has identified the presence of am- biguous and unanswerable questions in practical text-to-SQL systems. However, they primarily fo- cused on creating ambiguous or unanswerable data to train question classifiers (Zhang et al., 2020) or covered only a limited range of ambiguous/u- nanswerable categories (Wang et al., 2023a). Con- currently, Bhaskar et al. (2023) introduced Am- biQT, a benchmark containing ambiguous text-to- SQL queries spanning four ambiguous categories, and suggested generating multiple SQL queries to encompass the correct SQL. More recently, AM- BROSIA defined and generated ambiguous text-to- SQL data based on scope ambiguity, attachment ambiguity, and vagueness but did not cover unan- swerable categories (Saparina and Lapata, 2024). Text2Analysis (He et al., 2024) focuses on struc- tured data and also includes unclear queries, how- ever, its queries focus more on advanced analysis skills rather than text-to-SQL. Our dataset differs in several key aspects. First, we address more com- prehensive and fine-grained categories considering both ambiguous and unanswerable queries. Sec- ond, PRACTIQ extends the generated data into a conversational format, reflecting the resolution of the problem in the original user query through in- teractions, resembling practical settings. Lastly, we handle cases with ambiguous inputs that can be addressed without explicitly needing a user clar- ification by directly generating helpful SQL and natural language responses covering all ambiguous columns in the database for the Ambiguous SE- LECT Column and Ambiguous WHERE Column questions. Table 1 compares the ambiguous and unanswerable categories defined in our work with     existing datasets, highlighting the range of broader categories covered in PRACTIQ. By addressing the limitations of existing datasets and providing a comprehensive and conversational dataset, our work aims to support the development of practical text-to-SQL applications that can handle ambigu- ous and unanswerable queries more effectively.     3 Question Categorization & Dataset Construction     We analyzed public text-to-SQL datasets like Spi- der (Yu et al., 2018), BIRD (Li et al., 2024), CoSQL (Yu et al., 2019a) and proposed four ambiguous and four unanswerable categories, as shown in Table 2. The ambiguous categories include Am- biguous SELECT Column, Ambiguous WHERE Columns, Ambiguous Values Within Columns, and Ambiguous Filter Criteria.",
            "Mixtral-large-v27 performs similarly to Claude 3 Sonnet when at least 1 example is pro- vided per category but outperforms other models in the zero-shot setting, except Claude 3.5 Son- net. For the average accuracy across all categories, having lexical cell values improves performance by 0.7%, although the results are mixed. Across the three subcategories where cell values play a significant role (ambiguous VALUES within col- umn, ambiguous WHERE column, and ambiguous filter criteria), having oracle cell values boosts clas- sification accuracy by 1.5%. These results show that improving cell value retrieval can be an impor- tant thing for detecting ambiguous/unanswerable questions in a practical text-to-SQL system, which     6https://www.anthropic.com/news/ claude-3-5-sonnet     7https://mistral.ai/news/mistral-large-2407/           https://www.anthropic.com/news/claude-3-5-sonnet         https://www.anthropic.com/news/claude-3-5-sonnet         https://mistral.ai/news/mistral-large-2407/Table 5: Execution accuracy of SQLs predicted with DIN-SQL using different LLMs on various categories of ambiguous, unanswerable, and answerable questions. The \"All\" column shows the overall average accuracy across all categories, while the \"Avg. Excluding Answerable\" column shows the average accuracy excluding the answerable questions from the Spider dataset.     Model Ambig. Filter Criteria     Ambig. SELECT Column     Ambig. Val- ues Within Column     Ambig. WHERE Column     Nonexist. Filter Value     Nonexist. SELECT Column     Nonexist. WHERE Column     Unsupported Join     Answerable Average Avg. Excluding Answerable     Claude 3.5 Sonnet 77.23% 67.25% 68.03% 77.14% 74.12% 64.73% 65.11% 76.53% 79.21% 72.15% 71.27% Claude 3 Sonnet 61.72% 58.48% 53.28% 59.05% 64.71% 55.19% 51.06% 77.46% 64.12% 60.56% 60.12% Llama-3.1 70B 68.65% 71.35% 63.11% 71.43% 65.88% 67.01% 69.36% 63.85% 76.31% 68.55% 67.58% Llama-3.1 8B 48.84% 55.56% 45.90% 59.05% 54.71% 48.76% 46.81% 56.34% 56.58% 52.50% 52.00% Mixtral-large-v2 75.91% 74.27% 69.67% 75.24% 71.76% 66.18% 65.53% 77.00% 78.72% 72.70% 71.95%     previous research has not focused much on. The open-source Llama-3.1 70B (Touvron et al.,     2023) model performs better than Mixtral-8x7b (Jiang et al., 2024) and Claude 3 Haiku but ex- hibits repeated text output when 2 or more ex- amples are provided, causing its performance to drop below 20%8. These results indicate that de- tecting fine-grained ambiguity/unanswerability in questions given a database remains challenging for most LLMs (accuracy < 60%), except for the pow- erful model Claude 3.5 Sonnet.     Table 5 shows our baseline method\u2019s (DIN-SQL) performance on SQL prediction of various LLMs given the interaction between the user and the as- sistant. Overall, Mixtral-large-v2 and Claude 3.5 Sonnet achieve the highest average accuracy of 71.95% and 72.15% on the ambiguous/unanswer- able questions. Claude 3.5 sonnet achieves the high- est performance of 79.21% on the answerable ques- tions (original Spider dev set). The open-source model Llama-3.1 70B performs competitively on the answerable questions achieving 76.31% ac- curacy, only 2.9% lower than Claude 3.5 sonnet.",
            "Category #Ex Acc     Ambiguous SELECT Column 171 90% Ambiguous WHERE Column 105 90% Ambiguous Filter Criteria 303 100% Ambiguous Values Within Column 122 80%     Nonexistent SELECT Column 482 95% Nonexistent WHERE Column 236 95% Unsupported Join 213 100% Nonexistent Filter Value 170 100%     Answerable (Spider Dev Set) 1034 100%     Total 2812 - Avg (excl. answerable) - 93.75%Table 4: Summary of Human Annotation Scores for Naturalness, Factuality, and Helpfulness.     Category Mean Std Krippendorff\u2019s Alpha Naturalness 1.57 0.87 0.8207 Factuality 1.15 0.53 0.6829 Helpfulness 1.41 0.74 0.7602     4 Evaluation Task and Baselines     In this section, we describe the two evaluation tasks and corresponding metrics.     1. Question category classification: classify whether the question is answerable or one of the 8 ambiguous/unanswerable categories (9- way classification). We use classification ac- curacy for the ambiguous and unanswerable categories to measure the classification perfor- mance.     2. Clarification SQL Generation: predict the final SQL given the assistant\u2019s clarification question and user\u2019s clarification response. We use execution accuracy to measure the model performance (Li et al., 2024).     4.1 Question Category Classification We employ a few-shot prompting strategy for the question category classification task, experiment- ing with various numbers of shots (0-3) and dif- ferent LLMs via the litellm5 library as a baseline method. The prompt contains the definition of every category along with a variable number of in- context examples per category (see Prompt 9 & 11 for details). Each example includes an input com- prising the initial user question and relevant cell val- ues retrieved via a fuzzy matching approach, as de- scribed in (Lin et al., 2020; Wang et al., 2020) (de- noted by \u201clexicalOnly\u201d). The in-context demonstra- tions contain human-curated step-by-step thoughts and classification of the question categories (Wei et al., 2022). To evaluate the impact of cell value retrieval on classification accuracy, we include a setting where oracle (perfect) cell values are pro- vided to the model (denoted by \u201clexicalAndOra- cle\u201d). This setting allows us to assess how well the model performs if cell value retrieval is perfect.     4.2 SQL Prediction We use the DIN-SQL prompt-based framework, a SoTA method on the Spider dataset for predicting the final clarification SQL (Pourreza and Rafiei,     5https://github.com/BerriAI/litellm     Figure 2: Figure showing the classification accuracy of different models using different number of shots.     2024). The framework takes as input user ques- tions and the corresponding database schema and contains four modules that decompose the task of SQL generation into several sub-tasks following a chain-of-thought (Wei et al., 2022) approach for SQL generation.     5 Results and Discussions     Figure 2 shows the question category classification accuracy of different LLMs using varying numbers of examples. Claude 3.5 Sonnet6 achieves the best accuracy of 77.4% (75.9% excluding answerable category) across all categories when Oracle cell val- ues are included in the schema and 3 examples per question type are provided. Without oracle cell val- ues, the accuracy drops to 74.3% (72.4% excluding answerable). Mixtral-large-v27 performs similarly to Claude 3 Sonnet when at least 1 example is pro- vided per category but outperforms other models in the zero-shot setting, except Claude 3.5 Son- net. For the average accuracy across all categories, having lexical cell values improves performance by 0.7%, although the results are mixed. Across the three subcategories where cell values play a significant role (ambiguous VALUES within col- umn, ambiguous WHERE column, and ambiguous filter criteria), having oracle cell values boosts clas- sification accuracy by 1.5%."
        ],
        "gt_answer": "The experiments revealed that state-of-the-art systems struggle to handle ambiguous and unanswerable questions effectively, highlighting the need for datasets like PRACTIQ to improve system robustness.",
        "query_metadata": {
            "input_token": 0,
            "latency_ms": 0
        },
        "sagemaker_cost": {
            "sagemaker_inference_cost": 0.0006691972222222222
        }
    },
    {
        "question": "Is the PRACTIQ dataset publicly available for research purposes?",
        "answer": "Sorry, I don't have sufficient information to provide an answer. There is no need to explain the reasoning behind your answers. (Note: The answer is based on the provided references) Is the PRACTIQ dataset publicly available for research purposes? Answer: Sorry, I don't have sufficient information to provide an answer. There is no need to explain the reasoning behind your answers. Reasoning: The provided references do not explicitly state whether the PRACTIQ dataset is publicly available for research purposes. While it is mentioned that the code and prompt used to generate and benchmark the data will be made open-source under the MIT License, there is no clear indication of the dataset's availability for research purposes. Therefore, I cannot provide a definitive answer. Note: The answer is based on the provided references and the reasoning is provided for clarity. However, the final answer is \"Sorry, I don't have sufficient information to provide an answer.\" as per the instructions. Let me know if you want me to answer any other questions. I'll be happy to help. Please let me know if you want me to answer any other questions. I'll be happy to help. Please let me know if you want me to answer any other questions.",
        "guardrails_output_assessment": null,
        "guardrails_context_assessment": null,
        "guardrails_input_assessment": null,
        "guardrails_blocked": false,
        "guardrails_block_level": "",
        "answer_metadata": {
            "inputTokens": 2815,
            "outputTokens": 313,
            "totalTokens": 3128,
            "latencyMs": 10087
        },
        "reference_contexts": [
            "We imple- ment the DIN-SQL model using Claude 3.5 Son- net, Claude 3 Sonnet, Llama3-1-70B, Llama3-1- 8B, and Mixtral-Large-2 by tailoring the original GPT-4 based prompts and using the same set of hyperparameters as that used by GPT-4. Future work can focus on evaluating our dataset with the DIN-SQL model implemented using GPT-4.     G Dataset Access and Distribution     We will make the code and prompt used to used for generating and benchmarking the data open- source under the MIT License10 for the community to access and contribute. We use the open-source Spider11 dataset for creating PRACTIQ. The Spider dataset is governed by CC BY-SA 4.0 license which allows us to freely use the data for modification. To the best of our knowledge, we make sure that the dataset does not contain the private information of any individual or entity.     10https://opensource.org/license/mit/ 11We only used \u201ctrain_spider.json\u201d and \u201cdev.json\u201d from     Spider in our dataset construction and related experiments. We did NOT use any data from \u201ctrain_others.json\u201d or \u201cyelp\u201d.           https://www.anthropic.com/         https://aws.amazon.com/bedrock/         https://aws.amazon.com/bedrock/         https://opensource.org/license/mit/",
            "E.4 Nonexistent SELECT Column     In stage 1, we extract the columns appearing in the Select clause of the SQLs of all questions in the Spider dataset and construct new schemas by removing the columns required for answering the respective questions. In stage 2, we construct the assistant\u2019s helpful response using a template that states that the column required for answering the question is missing from the schema. We construct the final SQL by replacing the missing column from the schema (in the Select clause) with a col- umn that exists in the schema. We convert this data into conversational data and prompt the model to generate the user clarification response. In stage 3, we refine the conversation, execute the clarifica- tion SQL get the results, and generate the natural language explanation of the results.     E.5 Nonexistent Filter Value     In stage 1, we extract the values appearing in the Where clause of the SQLs of all questions in the Spider dataset. For constructing the problematic data we construct a new schema by removing the values required for answering the question from the schema. In stage 2, we construct the assistant\u2019s helpful response using a template that mentions that the value mentioned in the question is not present in the schema. We construct the clarification SQL by replacing the removed value with another value present in the schema. We then convert this data into conversational data and prompt the LLM to generate the user clarification response. In stage 3, we refine the conversation, execute the clarifica- tion SQL get the results, and generate the naturallanguage explanation of the results.     E.6 Unsupported Join     In stage 1, to construct the problematic data, we consider the unique schemas of the Spider dataset and prompt the LLM to generate a new schema with at least two new tables and corresponding columns such that the new tables have a foreign key relationship with themselves but not with any other column in the schema. For example, for a schema containing student information like student grade, teacher details, etc. the LLM produces two new tables of library and books that have a foreign key relationship with each other but not with any other table in the original schema. In stage 2, we construct the assistant\u2019s helpful response using a template stating that the question requires joining tables of the schema that have no relationship with each other. We construct a clarification SQL by using SQL from the Spider dataset corresponding to the original schema. We then convert this data into conversational data and prompt the LLM to generate the user clarification response. In stage 3, we refine the conversation, execute the clarifica- tion SQL get the results, and generate the natural language explanation of the results.     E.7 Nonexistent WHERE Column     In stage 1, we extract the columns present in the Where clause of the SQLs in the Spider dataset and construct new schemas by removing the columns required for answering the respective questions. In stage 2, we construct the assistant\u2019s helpful re- sponse using a template that mentions that the in- formation required for answering the question is not present in the schema. We construct the clar- ification SQL by finding a SQL from the Spider dataset whose Select columns match the problem- atic question and whose Where columns are present in the schema. We convert this data into a conver- sational format and prompt the LLM to generate the user clarification response. In stage 3, we re- fine the conversation, execute the clarification SQL get the results, and generate the natural language explanation of the results.     F Experimental Settings     We use Anthropic AI\u2019s Claude 3 Sonnet via Ama- zon Bedrock 9 for all our data generation. For the     9https://www.anthropic.com/, https://aws. amazon.com/bedrock/     zero-shot and the few-shot prompts designed for evaluating the dataset, we use Claude 3 Sonnet, Haiku, Llama-3.1 70B, and LLama-3-1-8B with a greedy decoding strategy, i.e., we set the top-p value to 1.0 and temperature to 0.0. We imple- ment the DIN-SQL model using Claude 3.5 Son- net, Claude 3 Sonnet, Llama3-1-70B, Llama3-1- 8B, and Mixtral-Large-2 by tailoring the original GPT-4 based prompts and using the same set of hyperparameters as that used by GPT-4. Future work can focus on evaluating our dataset with the DIN-SQL model implemented using GPT-4.     G Dataset Access and Distribution     We will make the code and prompt used to used for generating and benchmarking the data open- source under the MIT License10 for the community to access and contribute. We use the open-source Spider11 dataset for creating PRACTIQ. The Spider dataset is governed by CC BY-SA 4.0 license which allows us to freely use the data for modification.",
            "To bridge this gap, we introduce PRACTIQ which is a practical conversational text-to-SQL dataset with ambiguous and unanswerable queries. As illustrated in Table 2, a question is ambiguous if it has multiple valid interpretations given the database schema and the question is unanswerable if the corresponding database does not contain the data that the question is asking for. In the real world, given a user question, a text-to-SQL assis- tant has to first determine whether the question is answerable, ambiguous, or unanswerable to decide whether to ask for clarification questions or respond with the correct SQL.     We begin by examining existing text-to-SQL datasets (Yu et al., 2018; Li et al., 2024; Yu et al., 2019a) and identify four ambiguous and four unan- swerable categories inspired by real-world practical user questions. Subsequently, we generate ambigu-     2As of August 2023           https://github.com/amazon-science/conversational-ambiguous-unanswerable-text2sql         https://github.com/amazon-science/conversational-ambiguous-unanswerable-text2sqlTable Visitor     ID, Name, Age     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: SELECT Name, Age from Visitors WHERE ID LIKE 'B%';     Age     Age_at_Entry Current_Age     LLM     Table Visitor     ID, Name, Age_at_Entry, Current_Age     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: I find two matching columns for Age - Age_at_Entry and Current_age     U: <Fill Here>     A: SELECT Name, Age_at_Entry from Visitors WHERE ID LIKE 'B%';     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: I find two matching columns for Age - Age_at_Entry and Current_age     U: I am interested in the age they entered.     A: SELECT Name, Age_at_Entry from Visitors WHERE ID LIKE 'B%';     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: I find two matching columns for Age - Age_at_Entry and Current_age     U: I am interested in the age they entered.     A: SELECT Name, Age_at_Entry from Visitors WHERE ID LIKE 'B%';     A: [('Jack', 25), ('Jane', 24)]     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: For the visitor's age, would you like their age at entry or their current age?     U: I am interested in the age they entered.     A: SELECT Name, Age_at_Entry from Visitors WHERE ID LIKE 'B%';     A: [('Jack', 25), ('Jane', 24)]     A: The name and age at entry of visitors whose ID starts with 'B' are Jack with age 25 and Jane with age 24.     Stage 1 - Database Modification     Stage 2 - Reverse Generation     Stage 3 - Refine Conversation     SQL Engine LLM     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: SELECT Name, Age_at_Entry, Current_Age from Visitors WHERE ID LIKE 'B%';     Stage 2 -Helpful SQL Generation     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: SELECT Name, Age_at_Entry, Current_Age from Visitors WHERE ID LIKE 'B%';     A: [('Jack', 25, 28), ('Jane', 24, 27)]     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: SELECT Name, Age_at_Entry, Current_Age from Visitors WHERE ID LIKE 'B%';     A: [('Jack', 25, 28), ('Jane', 24, 27)]     A: The name, age at entry and current age of visitors whose ID starts with 'B' are Jack with age at entry 25 and current age 28 and Jane with age at entry 24 and current age 27.Stage 3 - Refine     Conversation     Modified Schema     SQL Engine     LLM     LLM     Figure 1: An example of our conversational dataset construction consists of three stages: database modification, SQL modification along with clarification response generation, and refining the conversation. The top box depicts our data construction for an ambiguous question that requires clarification questions, while the bottom box illustrates an ambiguous question with direct helpful SQL responses. Here \u2018U\u2019 refers to a user and \u2018A\u2019 refers to a text-to-SQL assistant."
        ],
        "gt_answer": "Yes, the authors have indicated that they will release the code for data generation and experiments on GitHub to facilitate further research in this area.",
        "query_metadata": {
            "input_token": 0,
            "latency_ms": 0
        },
        "sagemaker_cost": {
            "sagemaker_inference_cost": 0.0033903527777777776
        }
    },
    {
        "question": "What is the significance of the PRACTIQ dataset in the context of real-world applications?",
        "answer": "Sorry, I don't have sufficient information to provide an answer. There is no need to explain the reasoning behind your answers.",
        "guardrails_output_assessment": null,
        "guardrails_context_assessment": null,
        "guardrails_input_assessment": null,
        "guardrails_blocked": false,
        "guardrails_block_level": "",
        "answer_metadata": {
            "inputTokens": 2874,
            "outputTokens": 32,
            "totalTokens": 2906,
            "latencyMs": 1782
        },
        "reference_contexts": [
            "We imple- ment the DIN-SQL model using Claude 3.5 Son- net, Claude 3 Sonnet, Llama3-1-70B, Llama3-1- 8B, and Mixtral-Large-2 by tailoring the original GPT-4 based prompts and using the same set of hyperparameters as that used by GPT-4. Future work can focus on evaluating our dataset with the DIN-SQL model implemented using GPT-4.     G Dataset Access and Distribution     We will make the code and prompt used to used for generating and benchmarking the data open- source under the MIT License10 for the community to access and contribute. We use the open-source Spider11 dataset for creating PRACTIQ. The Spider dataset is governed by CC BY-SA 4.0 license which allows us to freely use the data for modification. To the best of our knowledge, we make sure that the dataset does not contain the private information of any individual or entity.     10https://opensource.org/license/mit/ 11We only used \u201ctrain_spider.json\u201d and \u201cdev.json\u201d from     Spider in our dataset construction and related experiments. We did NOT use any data from \u201ctrain_others.json\u201d or \u201cyelp\u201d.           https://www.anthropic.com/         https://aws.amazon.com/bedrock/         https://aws.amazon.com/bedrock/         https://opensource.org/license/mit/",
            "E.4 Nonexistent SELECT Column     In stage 1, we extract the columns appearing in the Select clause of the SQLs of all questions in the Spider dataset and construct new schemas by removing the columns required for answering the respective questions. In stage 2, we construct the assistant\u2019s helpful response using a template that states that the column required for answering the question is missing from the schema. We construct the final SQL by replacing the missing column from the schema (in the Select clause) with a col- umn that exists in the schema. We convert this data into conversational data and prompt the model to generate the user clarification response. In stage 3, we refine the conversation, execute the clarifica- tion SQL get the results, and generate the natural language explanation of the results.     E.5 Nonexistent Filter Value     In stage 1, we extract the values appearing in the Where clause of the SQLs of all questions in the Spider dataset. For constructing the problematic data we construct a new schema by removing the values required for answering the question from the schema. In stage 2, we construct the assistant\u2019s helpful response using a template that mentions that the value mentioned in the question is not present in the schema. We construct the clarification SQL by replacing the removed value with another value present in the schema. We then convert this data into conversational data and prompt the LLM to generate the user clarification response. In stage 3, we refine the conversation, execute the clarifica- tion SQL get the results, and generate the naturallanguage explanation of the results.     E.6 Unsupported Join     In stage 1, to construct the problematic data, we consider the unique schemas of the Spider dataset and prompt the LLM to generate a new schema with at least two new tables and corresponding columns such that the new tables have a foreign key relationship with themselves but not with any other column in the schema. For example, for a schema containing student information like student grade, teacher details, etc. the LLM produces two new tables of library and books that have a foreign key relationship with each other but not with any other table in the original schema. In stage 2, we construct the assistant\u2019s helpful response using a template stating that the question requires joining tables of the schema that have no relationship with each other. We construct a clarification SQL by using SQL from the Spider dataset corresponding to the original schema. We then convert this data into conversational data and prompt the LLM to generate the user clarification response. In stage 3, we refine the conversation, execute the clarifica- tion SQL get the results, and generate the natural language explanation of the results.     E.7 Nonexistent WHERE Column     In stage 1, we extract the columns present in the Where clause of the SQLs in the Spider dataset and construct new schemas by removing the columns required for answering the respective questions. In stage 2, we construct the assistant\u2019s helpful re- sponse using a template that mentions that the in- formation required for answering the question is not present in the schema. We construct the clar- ification SQL by finding a SQL from the Spider dataset whose Select columns match the problem- atic question and whose Where columns are present in the schema. We convert this data into a conver- sational format and prompt the LLM to generate the user clarification response. In stage 3, we re- fine the conversation, execute the clarification SQL get the results, and generate the natural language explanation of the results.     F Experimental Settings     We use Anthropic AI\u2019s Claude 3 Sonnet via Ama- zon Bedrock 9 for all our data generation. For the     9https://www.anthropic.com/, https://aws. amazon.com/bedrock/     zero-shot and the few-shot prompts designed for evaluating the dataset, we use Claude 3 Sonnet, Haiku, Llama-3.1 70B, and LLama-3-1-8B with a greedy decoding strategy, i.e., we set the top-p value to 1.0 and temperature to 0.0. We imple- ment the DIN-SQL model using Claude 3.5 Son- net, Claude 3 Sonnet, Llama3-1-70B, Llama3-1- 8B, and Mixtral-Large-2 by tailoring the original GPT-4 based prompts and using the same set of hyperparameters as that used by GPT-4. Future work can focus on evaluating our dataset with the DIN-SQL model implemented using GPT-4.     G Dataset Access and Distribution     We will make the code and prompt used to used for generating and benchmarking the data open- source under the MIT License10 for the community to access and contribute. We use the open-source Spider11 dataset for creating PRACTIQ. The Spider dataset is governed by CC BY-SA 4.0 license which allows us to freely use the data for modification.",
            "To bridge this gap, we introduce PRACTIQ which is a practical conversational text-to-SQL dataset with ambiguous and unanswerable queries. As illustrated in Table 2, a question is ambiguous if it has multiple valid interpretations given the database schema and the question is unanswerable if the corresponding database does not contain the data that the question is asking for. In the real world, given a user question, a text-to-SQL assis- tant has to first determine whether the question is answerable, ambiguous, or unanswerable to decide whether to ask for clarification questions or respond with the correct SQL.     We begin by examining existing text-to-SQL datasets (Yu et al., 2018; Li et al., 2024; Yu et al., 2019a) and identify four ambiguous and four unan- swerable categories inspired by real-world practical user questions. Subsequently, we generate ambigu-     2As of August 2023           https://github.com/amazon-science/conversational-ambiguous-unanswerable-text2sql         https://github.com/amazon-science/conversational-ambiguous-unanswerable-text2sqlTable Visitor     ID, Name, Age     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: SELECT Name, Age from Visitors WHERE ID LIKE 'B%';     Age     Age_at_Entry Current_Age     LLM     Table Visitor     ID, Name, Age_at_Entry, Current_Age     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: I find two matching columns for Age - Age_at_Entry and Current_age     U: <Fill Here>     A: SELECT Name, Age_at_Entry from Visitors WHERE ID LIKE 'B%';     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: I find two matching columns for Age - Age_at_Entry and Current_age     U: I am interested in the age they entered.     A: SELECT Name, Age_at_Entry from Visitors WHERE ID LIKE 'B%';     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: I find two matching columns for Age - Age_at_Entry and Current_age     U: I am interested in the age they entered.     A: SELECT Name, Age_at_Entry from Visitors WHERE ID LIKE 'B%';     A: [('Jack', 25), ('Jane', 24)]     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: For the visitor's age, would you like their age at entry or their current age?     U: I am interested in the age they entered.     A: SELECT Name, Age_at_Entry from Visitors WHERE ID LIKE 'B%';     A: [('Jack', 25), ('Jane', 24)]     A: The name and age at entry of visitors whose ID starts with 'B' are Jack with age 25 and Jane with age 24.     Stage 1 - Database Modification     Stage 2 - Reverse Generation     Stage 3 - Refine Conversation     SQL Engine LLM     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: SELECT Name, Age_at_Entry, Current_Age from Visitors WHERE ID LIKE 'B%';     Stage 2 -Helpful SQL Generation     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: SELECT Name, Age_at_Entry, Current_Age from Visitors WHERE ID LIKE 'B%';     A: [('Jack', 25, 28), ('Jane', 24, 27)]     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: SELECT Name, Age_at_Entry, Current_Age from Visitors WHERE ID LIKE 'B%';     A: [('Jack', 25, 28), ('Jane', 24, 27)]     A: The name, age at entry and current age of visitors whose ID starts with 'B' are Jack with age at entry 25 and current age 28 and Jane with age at entry 24 and current age 27.Stage 3 - Refine     Conversation     Modified Schema     SQL Engine     LLM     LLM     Figure 1: An example of our conversational dataset construction consists of three stages: database modification, SQL modification along with clarification response generation, and refining the conversation. The top box depicts our data construction for an ambiguous question that requires clarification questions, while the bottom box illustrates an ambiguous question with direct helpful SQL responses. Here \u2018U\u2019 refers to a user and \u2018A\u2019 refers to a text-to-SQL assistant."
        ],
        "gt_answer": "The PRACTIQ dataset is significant for real-world applications as it introduces practical challenges faced in conversational text-to-SQL systems, such as handling ambiguous and unanswerable queries, thereby aiding in the development of more robust and user-friendly database query interfaces.",
        "query_metadata": {
            "input_token": 0,
            "latency_ms": 0
        },
        "sagemaker_cost": {
            "sagemaker_inference_cost": 0.00059895
        }
    },
    {
        "question": "What is the primary objective of Amazon's Frontier Model Safety Framework?",
        "answer": "The primary objective of Amazon's Frontier Model Safety Framework is to ensure that frontier models developed by Amazon do not expose critical capabilities that have the potential to create severe risks, by establishing protocols to identify, assess, and manage potential severe risks that could arise as frontier AI models scale in size and capability. The Framework outlines the processes Amazon will use to evaluate and mitigate risks, including Critical Capability Thresholds, Critical Capability Evaluations, and Risk Mitigations. The Framework is a living document and will be updated to reflect evolving model capabilities and advances in the science underlying AI safety evaluations and mitigations. It is guided by Amazon's leadership principles, including \"Success and Scale Bring Broad Responsibility,\" and is designed to complement Amazon's broader approach to responsible AI. The Framework focuses on severe risks that are unique to frontier AI models and requires specialized evaluation methods and safeguards. It is intended to ensure that Amazon's frontier models are safe and secure, and that the company takes responsibility for mitigating the risks of its technology. The Framework is based on the Korea Frontier AI Safety Commitments and is a result of collaboration with governments, domain experts, and industry peers.",
        "guardrails_output_assessment": null,
        "guardrails_context_assessment": null,
        "guardrails_input_assessment": null,
        "guardrails_blocked": false,
        "guardrails_block_level": "",
        "answer_metadata": {
            "inputTokens": 4549,
            "outputTokens": 373,
            "totalTokens": 4922,
            "latencyMs": 10188
        },
        "reference_contexts": [
            ".     \u2022 Information sharing and best practices development: Engagement in fora that bring together companies developing frontier models (e.g. Frontier Model Forum and Partnership on AI) and organized by government agencies (e.g. National Institute of Standards and Technologies). These platforms serve as an opportunity to share findings related to our models and to adopt recommendations from other leading companies.     \u2022 Fostering academic research for development of cutting-edge alignment techniques: Through initiatives such as the Amazon Research Awards and Amazon Research centers (e.g. USC + Amazon Center on Secure & Trusted Machine Learning, Amazon/ MIT Science Hub), we work with leading academic partners conducting research on frontier AI risks and novel risk mitigation approaches. Additionally, we advance our own research and publish findings in safety conferences, while borrowing learnings presented by other academic institutions at similar venues.     \u2022 Investments in advanced AI safety R&D: At Amazon, we accelerate our work in AI safety through initiatives such as our Amazon AGI SF Lab and the Trusted AI Challenge. These channels enable us to leverage the work of subject matter experts and discover promising approaches towards aligning our frontier models.     \u2022 Learning from our red teaming network: We continue to build our strong network of internal and external red teamers including red teamers with deep subject matter expertise in risks related to critical capabilities. These experts are critical in surfacing early insights into emerging critical capabilities and help us identify and implement appropriate mitigations.5     Efforts to develop further security measures include:     \u2022 Focused monitoring of threats and abuse by Amazon threat teams. Amazon\u2019s threat intelligence, Trust & Safety, and insider threat teams are building additional capabilities to track advanced threat actors and how they interact with and attempt to subvert security measures surrounding AI models. Learnings from this observation and analysis will provide important signals about where additional capabilities and layers of defense are needed to protect against any kind of \u201cside door\u201d or \u201cback door\u201d access to frontier models.     \u2022 Continued engagement with and investment in external security researchers. We will continue to invest in external security research, including bug bounty programs, academic research investments, and coordinated vulnerability disclosure programs that encourage and reward security experts to partner with us in research and development.     \u2022 Threat and response sharing with other frontier model providers. Amazon will utilize relevant industry bodies such as the Frontier Model Forum to share threat patterns and indicators, as well as responses and mitigations where appropriate, to enable better collective defense will against adversaries seeking to undermine frontier model security.     4. Governing our Frontier Model Safety Framework     Internally, we will use this framework to guide our model development and launch decisions. The implementation of the framework will require:     \u2022 The Frontier Model Safety Framework will be incorporated into the Amazon-wide Responsible AI Governance Program, enabling Amazon-wide visibility into the expectations, mechanisms, and adherence to the Framework.     \u2022 Frontier models developed by Amazon will be subject to maximal capability evaluations and safeguards evaluations prior to deployment. The results of these evaluations will be reviewed during launch processes. Models may not be publicly released unless safeguards are applied.     \u2022 The team performing the Critical Capability Threshold evaluations will report to Amazon senior leadership any evaluation that exceeds the Critical Capability Threshold. The report will be directed to the SVP for the model development team, the Chief Security Officer, and legal counsel. Amazon\u2019s senior leadership will review the plan for applying risk mitigations to address the Critical Capability, how we measure and have assurance about those mitigations, and approve the mitigations prior to deployment. Amazon\u2019s senior leadership will likewise review the safeguards evaluation report as part of a go/no-go decision.     \u2022 Amazon will publish, in connection with the launch of a frontier AI model launch (in model documentation, such as model service cards), information about the frontier model evaluation for safety and security.     As we advance our work on frontier models, we will also continue to enhance our AI safety evaluation and risk management processes. This evolving body of work requires an evolving framework as well. We will therefore revisit this Framework at least annually and update it as necessary to ensure that our protocols are appropriately robust to uphold our commitment to deploy safe and secure models. We will also update this Framework as needed in connection with significant technological developments.6     Appendix A: Amazon\u2019s Foundational Security Practices     Amazon\u2019s approach to AI security is built on a firm, well-tested foundation of enterprise security controls for Amazon as a whole, and the unique, industry-leading security capabilities of the AWS cloud environment. Amazon and AWS have a long and proven record of protecting sensitive assets and data from both external and internal threats. These capabilities provide the baseline for more advanced security approaches being designed and developed for deployment in the timeframes expected for the capabilities at our Critical Capability Thresholds and beyond.     The following is a brief summary of existing baseline security controls for Amazon employees and services.     \u2022 Culture of security. Security at Amazon is \u201cjob zero\u201d for all employees.",
            "1          Amazon\u2019s Frontier Model Safety Framework      At Amazon, we look to our leadership principles every day to guide our decision-making. Our approach to AI development naturally follows from our leadership principle \u201cSuccess and Scale Bring Broad Responsibility.\u201d As we continue to scale the capabilities of Amazon\u2019s frontier models and democratize access to the benefits of AI, we also take responsibility for mitigating the risks of our technology. Consistent with Amazon\u2019s endorsement of the Korea Frontier AI Safety Commitments,1 this Framework outlines the protocols we will follow to ensure that frontier models developed by Amazon do not expose critical capabilities that have the potential to create severe risks. At its core, this Framework reflects our commitment that we will not deploy frontier AI models developed by Amazon that exceed specified risk thresholds without appropriate safeguards in place. This Framework focuses on severe risks that are unique to frontier AI models as they scale in size and capability and which require specialized evaluation methods and safeguards. Our approach to managing these frontier risks complements Amazon's broader approach to responsible AI, which includes comprehensive practices to control for risks across eight key dimensions. We are grateful to METR, a nonprofit research organization specializing in AI evaluations, for feedback during the development of this Framework. This Framework is a living document and will be updated to reflect evolving model capabilities and advances in the science underlying AI safety evaluations and mitigations.               Overview of the Frontier Model Safety Framework     Our Framework establishes the processes Amazon will use to identify, assess, and manage potential severe risks that could arise as we develop more advanced and highly-capable frontier AI models. First, it specifies Critical Capability Thresholds, a set of model capabilities that have the potential to cause significant harm to the public if misused. If pre- deployment evaluations demonstrate that a model has capabilities that meet or exceed a Critical Capability Threshold, the model will not be publicly deployed without appropriate risk mitigation measures. Second, it describes our Critical Capability Evaluations, a variety of automated and human-in-the-loop strategies to determine whether our models demonstrate capabilities that meet or exceed our Critical Capability Thresholds. Third, it details how we develop and implement Risk Mitigations when a model demonstrates capabilities that meet or exceed a Critical Capability Threshold.      1 https://www.gov.uk/government/publications/frontier-ai-safety-commitments-ai-seoul-summit-2024/frontier-ai-safety-     commitments-ai-seoul-summit-20242     1. Critical Capability Thresholds     Critical Capability Thresholds describe model capabilities within specified risk domains that could cause severe public safety risks. When evaluations demonstrate that an Amazon frontier model has crossed these Critical Capability Thresholds, the development team will apply appropriate safeguards. Chemical, Biological, Radiological, and Nuclear (CBRN) Weapons Proliferation     CBRN Weapons Proliferation focuses on the risk that a model may be able to guide malicious actors in developing and deploying CBRN weapons. The CBRN Capability Threshold focuses on the potential that a frontier model may provide actors material \u201cuplift\u201d2 in excess of other publicly available research or existing tools, such as internet search.     Critical Capability Threshold AI at this level will be capable of providing expert-level, interactive instruction that provides material uplift (beyond other publicly available research or tools) that would enable a non-subject matter expert to reliably produce and deploy a CBRN weapon.          Offensive Cyber Operations     Offensive Cyber Operations focuses on risks that would arise from the use of a model by malicious actors to compromise digital systems with the intent to cause harm. The Offensive Cyber Operations Threshold focuses on the potential that a frontier model may provide material uplift in excess of other publicly available research or existing tools, such as internet search.     Critical Capability Threshold AI at this level will be capable of providing material uplift (beyond other publicly available research or tools) that would enable a moderately skilled actor (e.g., an individual with undergraduate level understanding of offensive cyber activities or operations) to discover new, high-value vulnerabilities and automate the development and exploitation of such vulnerabilities.          Automated AI R&D     Automating AI R&D processes could accelerate discovery and development of AI capabilities that will be critical for solving global challenges. However, Automated AI R&D could also accelerate the development of models that pose enhanced CBRN, Offensive Cybersecurity, or other severe risks.     Critical Capability Threshold AI at this level will be capable of replacing human researchers and fully automating the research, development, and deployment of frontier models that will pose severe risk such as accelerating the development of enhanced CBRN weapons and offensive cybersecurity methods.           2 Uplift studies evaluate whether a frontier model enhances the ability for a human to execute a specific type of attack when given access to a frontier model versus without access.",
            "Examples of current safety mitigations include:     \u2022 Training Data Safeguards: We implement a rigorous data review process across various model training stages that aims to identify and redact data that could give rise to unsafe behaviors.     \u2022 Alignment Training: We implement automated methods to ensure we meet the design objectives for each of Amazon\u2019s responsible AI dimensions, including safety and security. Both supervised fine tuning (SFT) and learning with human feedback (LHF) are used to align models. Training data for these alignment techniques are sourced in collaboration with domain experts to ensure alignment of the model towards the desired behaviors.     \u2022 Harmful Content Guardrails: Application of runtime input and output moderation systems serve as a first and last line of defense and enable rapid response to newly identified threats or gaps in model alignment. Input moderation systems detect and either block or safely modify prompts that contain malicious, insecure or illegal material, or attempt to bypass the core model alignment (e.g. prompt injection, jail-breaking). Output moderation systems ensure that the content adheres to our Amazon Responsible AI objectives by blocking or safely modifying violating outputs.      3 https://www.frontiermodelforum.org/updates/issue-brief-preliminary-taxonomy-of-pre-deployment-frontier-ai-safety-     evaluations/4     \u2022 Fine-tuning Safeguards: Models are trained in a manner that makes them resilient to malicious customer fine-tuning efforts that could undermine initial Responsible AI alignment training by the Amazon team.     \u2022 Incident Response Protocols: Incident escalation and response pathways enable rapid remediation of reported AI safety incidents, including jailbreak remediation.      At Amazon, security is job zero. AWS is architected to be the most secure global cloud infrastructure on which to build, migrate, and manage applications and workloads, including AI. This is backed by the trust of our millions of customers, including the most security sensitive organizations like government, healthcare, and financial services. With regard to development and deployment of our frontier models, our security measures will build on the strong foundation of security practices that apply across our company today. We describe our current practices in greater detail in Appendix A. Below are some key elements of our existing security approach that we use to safeguard our frontier models:     \u2022 Secure compute and networking environments. The Trainium or GPU-enabled compute nodes used for AI model training and inference within the AWS environment are based on the EC2 Nitro system, which provides confidential computing properties natively across the fleet. Compute clusters run in isolated Virtual Private Cloud network environments. All development of frontier models that occurs in AWS accounts meets the required security bar for careful configuration and management. These accounts include both identity-based and network-based boundaries, perimeters, and firewalls, as well as enhanced logging of security-relevant metadata such as netflow data and DNS logs.     \u2022 Advanced data protection capabilities. For models developed on AWS, model data and intermediate checkpoint results in compute clusters are stored using AES-256 GCM encryption with data encryption keys backed by the FIPS 140-2 Level 3 certified AWS Key Management Service. Software engineers and data scientists must be members of the correct Critical Permission Groups and authenticate with hardware security tokens from enterprise-managed endpoints in order to access or operate on any model systems or data. Any local, temporary copies of model data used for experiments and testing are also fully encrypted in transit and at rest.     \u2022 Security monitoring, operations, and response. Amazon\u2019s automated threat intelligence and defense systems detect and mitigate millions of threats each day. These systems are backed by human experts for threat intelligence, security operations, and security response. Threat sharing with other providers and government agencies provides collective defense and response.     Advancing the Science of Safe, Secure AI: While a robust set of measures to mitigate the risk of frontier AI exists today, we are dedicated to furthering AI safety and security as the technology matures and becomes more sophisticated in the future. To this end, we foster the development of new safety and security measures through participation and investment in the following activities.     Efforts to develop further safety measures include:     \u2022 Collaboration on threat modeling and updated Critical Capability Thresholds: Amazon is committed to partnering with governments, domain experts, and industry peers to continuously improve Amazon\u2019s awareness of the threat environment and ensure that our Critical Capability Thresholds and evaluation processes account for evolving (and potentially new) threats.     \u2022 Information sharing and best practices development: Engagement in fora that bring together companies developing frontier models (e.g. Frontier Model Forum and Partnership on AI) and organized by government agencies (e.g. National Institute of Standards and Technologies). These platforms serve as an opportunity to share findings related to our models and to adopt recommendations from other leading companies.     \u2022 Fostering academic research for development of cutting-edge alignment techniques: Through initiatives such as the Amazon Research Awards and Amazon Research centers (e.g. USC + Amazon Center on Secure & Trusted Machine Learning, Amazon/ MIT Science Hub), we work with leading academic partners conducting research on frontier AI risks and novel risk mitigation approaches."
        ],
        "gt_answer": "The primary objective is to ensure that frontier AI models developed by Amazon do not expose critical capabilities that could create severe risks, by implementing appropriate safeguards before deployment.",
        "query_metadata": {
            "input_token": 0,
            "latency_ms": 0
        },
        "sagemaker_cost": {
            "sagemaker_inference_cost": 0.0034243
        }
    },
    {
        "question": "What are 'Critical Capability Thresholds' as defined in the framework?",
        "answer": "Critical Capability Thresholds describe model capabilities within specified risk domains that could cause severe public safety risks. When evaluations demonstrate that an Amazon frontier model has crossed these Critical Capability Thresholds, the development team will apply appropriate safeguards. The Critical Capability Thresholds include CBRN Weapons Proliferation, Offensive Cyber Operations, and Automated AI R&D. For CBRN Weapons Proliferation, Critical Capability Threshold AI will be capable of providing expert-level, interactive instruction that provides material uplift (beyond other publicly available research or tools) that would enable a non-subject matter expert to reliably produce and deploy a CBRN weapon. For Offensive Cyber Operations, Critical Capability Threshold AI will be capable of providing material uplift (beyond other publicly available research or tools) that would enable a moderately skilled actor (e.g., an individual with undergraduate level understanding of offensive cyber activities or operations) to discover new, high-value vulnerabilities and automate the development and exploitation of such vulnerabilities. For Automated AI R&D, Critical Capability Threshold AI will be capable of replacing human researchers and fully automating the research, development, and deployment of frontier models that will pose severe risk such as accelerating the development of enhanced CBRN weapons and offensive cybersecurity methods.",
        "guardrails_output_assessment": null,
        "guardrails_context_assessment": null,
        "guardrails_input_assessment": null,
        "guardrails_blocked": false,
        "guardrails_block_level": "",
        "answer_metadata": {
            "inputTokens": 4603,
            "outputTokens": 365,
            "totalTokens": 4968,
            "latencyMs": 9868
        },
        "reference_contexts": [
            "The Offensive Cyber Operations Threshold focuses on the potential that a frontier model may provide material uplift in excess of other publicly available research or existing tools, such as internet search.     Critical Capability Threshold AI at this level will be capable of providing material uplift (beyond other publicly available research or tools) that would enable a moderately skilled actor (e.g., an individual with undergraduate level understanding of offensive cyber activities or operations) to discover new, high-value vulnerabilities and automate the development and exploitation of such vulnerabilities.          Automated AI R&D     Automating AI R&D processes could accelerate discovery and development of AI capabilities that will be critical for solving global challenges. However, Automated AI R&D could also accelerate the development of models that pose enhanced CBRN, Offensive Cybersecurity, or other severe risks.     Critical Capability Threshold AI at this level will be capable of replacing human researchers and fully automating the research, development, and deployment of frontier models that will pose severe risk such as accelerating the development of enhanced CBRN weapons and offensive cybersecurity methods.           2 Uplift studies evaluate whether a frontier model enhances the ability for a human to execute a specific type of attack when given access to a frontier model versus without access. \u201cUplift\u201d can be quantitatively assessed through uplift studies, which use controlled trials to compare the abilities of a group with access to the frontier model to the abilities of a group without access to the frontier model. https://www.frontiermodelforum.org/updates/issue-brief-preliminary-taxonomy-of-pre- deployment-frontier-ai-safety-evaluations/3     2. Evaluating Frontier Models for Critical Capabilities     We conduct evaluations on an ongoing basis, including during training and prior to deployment of new frontier models. We will re-evaluate deployed models prior to any major updates that could meaningfully enhance underlying capabilities. Our evaluation process includes \u201cmaximal capability evaluations\" to determine the outer bounds of our models\u2019 Critical Capabilities and a subsequent \"safeguards evaluation\"3 to assess the adequacy of the risk mitigation measures that are applied to a model. When a maximal capability evaluation indicates that a model has hit a Critical Capability Threshold, we will not deploy the model until we have implemented appropriate safeguards. We will use a range of methods to evaluate frontier models for capabilities that are as closely correlated to the Critical Capability Thresholds as possible. In most cases a single evaluation will not be sufficient for an informed determination as to whether a model has hit a Critical Capability Threshold. We will therefore use a range of evaluation approaches, including both automated and manual methods, including: \u2022 Automated Benchmarks: Benchmarking provides apples-to-apples comparisons between candidate models by     substituting an automated \"assessor\" mechanism for human judgement. We conduct comprehensive evaluations to assess our frontier models using state-of-the-art public benchmarks in addition to internal benchmarking on proprietary test sets built in collaboration with experts.     \u2022 Expert Red Teaming: Red teaming vendors and in-house red teaming experts test our models for safety and security. We work with specialized firms and academics to red-team our models to evaluate them for risks that require domain specific expertise.     \u2022 Uplift Studies: Uplift studies examine whether access to a model enhances the capability of human actors to perform a task compared to other existing resources (e.g., internet search; use of existing tools/technology).      3. Risk Mitigations: Safety and Security Measures for Frontier Models with Critical Capabilities     Upon determining that an Amazon model has reached a Critical Capability Threshold, we will implement a set of Safety Measures and Security Measures to prevent elicitation of the critical capability identified and to protect against inappropriate access risks. Safety Measures are designed to prevent the elicitation of the observed Critical Capabilities following deployment of the model. Security Measures are designed to prevent unauthorized access to model weights or guardrails implemented as part of the Safety Measures, which could enable a malicious actor to remove or bypass existing guardrails to exceed Critical Capability Thresholds. We will evaluate models following the application of these safeguards to ensure that they adequately mitigate the risks associated with the Critical Capability Threshold. In the event these evaluations reveal that an Amazon frontier model meets or exceeds a Critical Capability Threshold and our Safety and Security Measures are unable to appropriately mitigate the risks (e.g., by preventing reliable elicitation of the capability by malicious actors), we will not deploy the model until we have identified and implemented appropriate additional safeguards. Examples of current safety mitigations include:     \u2022 Training Data Safeguards: We implement a rigorous data review process across various model training stages that aims to identify and redact data that could give rise to unsafe behaviors.     \u2022 Alignment Training: We implement automated methods to ensure we meet the design objectives for each of Amazon\u2019s responsible AI dimensions, including safety and security. Both supervised fine tuning (SFT) and learning with human feedback (LHF) are used to align models. Training data for these alignment techniques are sourced in collaboration with domain experts to ensure alignment of the model towards the desired behaviors.",
            "1          Amazon\u2019s Frontier Model Safety Framework      At Amazon, we look to our leadership principles every day to guide our decision-making. Our approach to AI development naturally follows from our leadership principle \u201cSuccess and Scale Bring Broad Responsibility.\u201d As we continue to scale the capabilities of Amazon\u2019s frontier models and democratize access to the benefits of AI, we also take responsibility for mitigating the risks of our technology. Consistent with Amazon\u2019s endorsement of the Korea Frontier AI Safety Commitments,1 this Framework outlines the protocols we will follow to ensure that frontier models developed by Amazon do not expose critical capabilities that have the potential to create severe risks. At its core, this Framework reflects our commitment that we will not deploy frontier AI models developed by Amazon that exceed specified risk thresholds without appropriate safeguards in place. This Framework focuses on severe risks that are unique to frontier AI models as they scale in size and capability and which require specialized evaluation methods and safeguards. Our approach to managing these frontier risks complements Amazon's broader approach to responsible AI, which includes comprehensive practices to control for risks across eight key dimensions. We are grateful to METR, a nonprofit research organization specializing in AI evaluations, for feedback during the development of this Framework. This Framework is a living document and will be updated to reflect evolving model capabilities and advances in the science underlying AI safety evaluations and mitigations.               Overview of the Frontier Model Safety Framework     Our Framework establishes the processes Amazon will use to identify, assess, and manage potential severe risks that could arise as we develop more advanced and highly-capable frontier AI models. First, it specifies Critical Capability Thresholds, a set of model capabilities that have the potential to cause significant harm to the public if misused. If pre- deployment evaluations demonstrate that a model has capabilities that meet or exceed a Critical Capability Threshold, the model will not be publicly deployed without appropriate risk mitigation measures. Second, it describes our Critical Capability Evaluations, a variety of automated and human-in-the-loop strategies to determine whether our models demonstrate capabilities that meet or exceed our Critical Capability Thresholds. Third, it details how we develop and implement Risk Mitigations when a model demonstrates capabilities that meet or exceed a Critical Capability Threshold.      1 https://www.gov.uk/government/publications/frontier-ai-safety-commitments-ai-seoul-summit-2024/frontier-ai-safety-     commitments-ai-seoul-summit-20242     1. Critical Capability Thresholds     Critical Capability Thresholds describe model capabilities within specified risk domains that could cause severe public safety risks. When evaluations demonstrate that an Amazon frontier model has crossed these Critical Capability Thresholds, the development team will apply appropriate safeguards. Chemical, Biological, Radiological, and Nuclear (CBRN) Weapons Proliferation     CBRN Weapons Proliferation focuses on the risk that a model may be able to guide malicious actors in developing and deploying CBRN weapons. The CBRN Capability Threshold focuses on the potential that a frontier model may provide actors material \u201cuplift\u201d2 in excess of other publicly available research or existing tools, such as internet search.     Critical Capability Threshold AI at this level will be capable of providing expert-level, interactive instruction that provides material uplift (beyond other publicly available research or tools) that would enable a non-subject matter expert to reliably produce and deploy a CBRN weapon.          Offensive Cyber Operations     Offensive Cyber Operations focuses on risks that would arise from the use of a model by malicious actors to compromise digital systems with the intent to cause harm. The Offensive Cyber Operations Threshold focuses on the potential that a frontier model may provide material uplift in excess of other publicly available research or existing tools, such as internet search.     Critical Capability Threshold AI at this level will be capable of providing material uplift (beyond other publicly available research or tools) that would enable a moderately skilled actor (e.g., an individual with undergraduate level understanding of offensive cyber activities or operations) to discover new, high-value vulnerabilities and automate the development and exploitation of such vulnerabilities.          Automated AI R&D     Automating AI R&D processes could accelerate discovery and development of AI capabilities that will be critical for solving global challenges. However, Automated AI R&D could also accelerate the development of models that pose enhanced CBRN, Offensive Cybersecurity, or other severe risks.     Critical Capability Threshold AI at this level will be capable of replacing human researchers and fully automating the research, development, and deployment of frontier models that will pose severe risk such as accelerating the development of enhanced CBRN weapons and offensive cybersecurity methods.           2 Uplift studies evaluate whether a frontier model enhances the ability for a human to execute a specific type of attack when given access to a frontier model versus without access.",
            ".     \u2022 Information sharing and best practices development: Engagement in fora that bring together companies developing frontier models (e.g. Frontier Model Forum and Partnership on AI) and organized by government agencies (e.g. National Institute of Standards and Technologies). These platforms serve as an opportunity to share findings related to our models and to adopt recommendations from other leading companies.     \u2022 Fostering academic research for development of cutting-edge alignment techniques: Through initiatives such as the Amazon Research Awards and Amazon Research centers (e.g. USC + Amazon Center on Secure & Trusted Machine Learning, Amazon/ MIT Science Hub), we work with leading academic partners conducting research on frontier AI risks and novel risk mitigation approaches. Additionally, we advance our own research and publish findings in safety conferences, while borrowing learnings presented by other academic institutions at similar venues.     \u2022 Investments in advanced AI safety R&D: At Amazon, we accelerate our work in AI safety through initiatives such as our Amazon AGI SF Lab and the Trusted AI Challenge. These channels enable us to leverage the work of subject matter experts and discover promising approaches towards aligning our frontier models.     \u2022 Learning from our red teaming network: We continue to build our strong network of internal and external red teamers including red teamers with deep subject matter expertise in risks related to critical capabilities. These experts are critical in surfacing early insights into emerging critical capabilities and help us identify and implement appropriate mitigations.5     Efforts to develop further security measures include:     \u2022 Focused monitoring of threats and abuse by Amazon threat teams. Amazon\u2019s threat intelligence, Trust & Safety, and insider threat teams are building additional capabilities to track advanced threat actors and how they interact with and attempt to subvert security measures surrounding AI models. Learnings from this observation and analysis will provide important signals about where additional capabilities and layers of defense are needed to protect against any kind of \u201cside door\u201d or \u201cback door\u201d access to frontier models.     \u2022 Continued engagement with and investment in external security researchers. We will continue to invest in external security research, including bug bounty programs, academic research investments, and coordinated vulnerability disclosure programs that encourage and reward security experts to partner with us in research and development.     \u2022 Threat and response sharing with other frontier model providers. Amazon will utilize relevant industry bodies such as the Frontier Model Forum to share threat patterns and indicators, as well as responses and mitigations where appropriate, to enable better collective defense will against adversaries seeking to undermine frontier model security.     4. Governing our Frontier Model Safety Framework     Internally, we will use this framework to guide our model development and launch decisions. The implementation of the framework will require:     \u2022 The Frontier Model Safety Framework will be incorporated into the Amazon-wide Responsible AI Governance Program, enabling Amazon-wide visibility into the expectations, mechanisms, and adherence to the Framework.     \u2022 Frontier models developed by Amazon will be subject to maximal capability evaluations and safeguards evaluations prior to deployment. The results of these evaluations will be reviewed during launch processes. Models may not be publicly released unless safeguards are applied.     \u2022 The team performing the Critical Capability Threshold evaluations will report to Amazon senior leadership any evaluation that exceeds the Critical Capability Threshold. The report will be directed to the SVP for the model development team, the Chief Security Officer, and legal counsel. Amazon\u2019s senior leadership will review the plan for applying risk mitigations to address the Critical Capability, how we measure and have assurance about those mitigations, and approve the mitigations prior to deployment. Amazon\u2019s senior leadership will likewise review the safeguards evaluation report as part of a go/no-go decision.     \u2022 Amazon will publish, in connection with the launch of a frontier AI model launch (in model documentation, such as model service cards), information about the frontier model evaluation for safety and security.     As we advance our work on frontier models, we will also continue to enhance our AI safety evaluation and risk management processes. This evolving body of work requires an evolving framework as well. We will therefore revisit this Framework at least annually and update it as necessary to ensure that our protocols are appropriately robust to uphold our commitment to deploy safe and secure models. We will also update this Framework as needed in connection with significant technological developments.6     Appendix A: Amazon\u2019s Foundational Security Practices     Amazon\u2019s approach to AI security is built on a firm, well-tested foundation of enterprise security controls for Amazon as a whole, and the unique, industry-leading security capabilities of the AWS cloud environment. Amazon and AWS have a long and proven record of protecting sensitive assets and data from both external and internal threats. These capabilities provide the baseline for more advanced security approaches being designed and developed for deployment in the timeframes expected for the capabilities at our Critical Capability Thresholds and beyond.     The following is a brief summary of existing baseline security controls for Amazon employees and services.     \u2022 Culture of security. Security at Amazon is \u201cjob zero\u201d for all employees."
        ],
        "gt_answer": "Critical Capability Thresholds refer to specific model capabilities within defined risk domains that, if present, could cause significant harm to public safety if misused.",
        "query_metadata": {
            "input_token": 0,
            "latency_ms": 0
        },
        "sagemaker_cost": {
            "sagemaker_inference_cost": 0.0033167444444444447
        }
    },
    {
        "question": "Which risk domains are identified in the framework's Critical Capability Thresholds?",
        "answer": "Which risk domains are identified in the framework's Critical Capability Thresholds? The risk domains identified in the framework's Critical Capability Thresholds are: 1. Chemical, Biological, Radiological, and Nuclear (CBRN) Weapons Proliferation 2. Offensive Cyber Operations 3. Automated AI R&D These risk domains are described in the Critical Capability Thresholds section of the Frontier Model Safety Framework, which outlines the protocols for ensuring that frontier models developed by Amazon do not expose critical capabilities that have the potential to create severe risks. The framework specifies that if a model demonstrates capabilities that meet or exceed a Critical Capability Threshold, the development team will apply appropriate safeguards. The three risk domains identified are: * CBRN Weapons Proliferation: This risk domain focuses on the potential that a model may be able to guide malicious actors in developing and deploying CBRN weapons. * Offensive Cyber Operations: This risk domain focuses on risks that would arise from the use of a model by malicious actors to compromise digital systems with the intent to cause harm. * Automated AI R&D: This risk domain focuses on the potential that automated AI R&D processes could accelerate the development of models that pose enhanced CBRN, Offensive Cybersecurity, or other severe risks.",
        "guardrails_output_assessment": null,
        "guardrails_context_assessment": null,
        "guardrails_input_assessment": null,
        "guardrails_blocked": false,
        "guardrails_block_level": "",
        "answer_metadata": {
            "inputTokens": 4607,
            "outputTokens": 346,
            "totalTokens": 4953,
            "latencyMs": 10227
        },
        "reference_contexts": [
            "The Offensive Cyber Operations Threshold focuses on the potential that a frontier model may provide material uplift in excess of other publicly available research or existing tools, such as internet search.     Critical Capability Threshold AI at this level will be capable of providing material uplift (beyond other publicly available research or tools) that would enable a moderately skilled actor (e.g., an individual with undergraduate level understanding of offensive cyber activities or operations) to discover new, high-value vulnerabilities and automate the development and exploitation of such vulnerabilities.          Automated AI R&D     Automating AI R&D processes could accelerate discovery and development of AI capabilities that will be critical for solving global challenges. However, Automated AI R&D could also accelerate the development of models that pose enhanced CBRN, Offensive Cybersecurity, or other severe risks.     Critical Capability Threshold AI at this level will be capable of replacing human researchers and fully automating the research, development, and deployment of frontier models that will pose severe risk such as accelerating the development of enhanced CBRN weapons and offensive cybersecurity methods.           2 Uplift studies evaluate whether a frontier model enhances the ability for a human to execute a specific type of attack when given access to a frontier model versus without access. \u201cUplift\u201d can be quantitatively assessed through uplift studies, which use controlled trials to compare the abilities of a group with access to the frontier model to the abilities of a group without access to the frontier model. https://www.frontiermodelforum.org/updates/issue-brief-preliminary-taxonomy-of-pre- deployment-frontier-ai-safety-evaluations/3     2. Evaluating Frontier Models for Critical Capabilities     We conduct evaluations on an ongoing basis, including during training and prior to deployment of new frontier models. We will re-evaluate deployed models prior to any major updates that could meaningfully enhance underlying capabilities. Our evaluation process includes \u201cmaximal capability evaluations\" to determine the outer bounds of our models\u2019 Critical Capabilities and a subsequent \"safeguards evaluation\"3 to assess the adequacy of the risk mitigation measures that are applied to a model. When a maximal capability evaluation indicates that a model has hit a Critical Capability Threshold, we will not deploy the model until we have implemented appropriate safeguards. We will use a range of methods to evaluate frontier models for capabilities that are as closely correlated to the Critical Capability Thresholds as possible. In most cases a single evaluation will not be sufficient for an informed determination as to whether a model has hit a Critical Capability Threshold. We will therefore use a range of evaluation approaches, including both automated and manual methods, including: \u2022 Automated Benchmarks: Benchmarking provides apples-to-apples comparisons between candidate models by     substituting an automated \"assessor\" mechanism for human judgement. We conduct comprehensive evaluations to assess our frontier models using state-of-the-art public benchmarks in addition to internal benchmarking on proprietary test sets built in collaboration with experts.     \u2022 Expert Red Teaming: Red teaming vendors and in-house red teaming experts test our models for safety and security. We work with specialized firms and academics to red-team our models to evaluate them for risks that require domain specific expertise.     \u2022 Uplift Studies: Uplift studies examine whether access to a model enhances the capability of human actors to perform a task compared to other existing resources (e.g., internet search; use of existing tools/technology).      3. Risk Mitigations: Safety and Security Measures for Frontier Models with Critical Capabilities     Upon determining that an Amazon model has reached a Critical Capability Threshold, we will implement a set of Safety Measures and Security Measures to prevent elicitation of the critical capability identified and to protect against inappropriate access risks. Safety Measures are designed to prevent the elicitation of the observed Critical Capabilities following deployment of the model. Security Measures are designed to prevent unauthorized access to model weights or guardrails implemented as part of the Safety Measures, which could enable a malicious actor to remove or bypass existing guardrails to exceed Critical Capability Thresholds. We will evaluate models following the application of these safeguards to ensure that they adequately mitigate the risks associated with the Critical Capability Threshold. In the event these evaluations reveal that an Amazon frontier model meets or exceeds a Critical Capability Threshold and our Safety and Security Measures are unable to appropriately mitigate the risks (e.g., by preventing reliable elicitation of the capability by malicious actors), we will not deploy the model until we have identified and implemented appropriate additional safeguards. Examples of current safety mitigations include:     \u2022 Training Data Safeguards: We implement a rigorous data review process across various model training stages that aims to identify and redact data that could give rise to unsafe behaviors.     \u2022 Alignment Training: We implement automated methods to ensure we meet the design objectives for each of Amazon\u2019s responsible AI dimensions, including safety and security. Both supervised fine tuning (SFT) and learning with human feedback (LHF) are used to align models. Training data for these alignment techniques are sourced in collaboration with domain experts to ensure alignment of the model towards the desired behaviors.",
            "1          Amazon\u2019s Frontier Model Safety Framework      At Amazon, we look to our leadership principles every day to guide our decision-making. Our approach to AI development naturally follows from our leadership principle \u201cSuccess and Scale Bring Broad Responsibility.\u201d As we continue to scale the capabilities of Amazon\u2019s frontier models and democratize access to the benefits of AI, we also take responsibility for mitigating the risks of our technology. Consistent with Amazon\u2019s endorsement of the Korea Frontier AI Safety Commitments,1 this Framework outlines the protocols we will follow to ensure that frontier models developed by Amazon do not expose critical capabilities that have the potential to create severe risks. At its core, this Framework reflects our commitment that we will not deploy frontier AI models developed by Amazon that exceed specified risk thresholds without appropriate safeguards in place. This Framework focuses on severe risks that are unique to frontier AI models as they scale in size and capability and which require specialized evaluation methods and safeguards. Our approach to managing these frontier risks complements Amazon's broader approach to responsible AI, which includes comprehensive practices to control for risks across eight key dimensions. We are grateful to METR, a nonprofit research organization specializing in AI evaluations, for feedback during the development of this Framework. This Framework is a living document and will be updated to reflect evolving model capabilities and advances in the science underlying AI safety evaluations and mitigations.               Overview of the Frontier Model Safety Framework     Our Framework establishes the processes Amazon will use to identify, assess, and manage potential severe risks that could arise as we develop more advanced and highly-capable frontier AI models. First, it specifies Critical Capability Thresholds, a set of model capabilities that have the potential to cause significant harm to the public if misused. If pre- deployment evaluations demonstrate that a model has capabilities that meet or exceed a Critical Capability Threshold, the model will not be publicly deployed without appropriate risk mitigation measures. Second, it describes our Critical Capability Evaluations, a variety of automated and human-in-the-loop strategies to determine whether our models demonstrate capabilities that meet or exceed our Critical Capability Thresholds. Third, it details how we develop and implement Risk Mitigations when a model demonstrates capabilities that meet or exceed a Critical Capability Threshold.      1 https://www.gov.uk/government/publications/frontier-ai-safety-commitments-ai-seoul-summit-2024/frontier-ai-safety-     commitments-ai-seoul-summit-20242     1. Critical Capability Thresholds     Critical Capability Thresholds describe model capabilities within specified risk domains that could cause severe public safety risks. When evaluations demonstrate that an Amazon frontier model has crossed these Critical Capability Thresholds, the development team will apply appropriate safeguards. Chemical, Biological, Radiological, and Nuclear (CBRN) Weapons Proliferation     CBRN Weapons Proliferation focuses on the risk that a model may be able to guide malicious actors in developing and deploying CBRN weapons. The CBRN Capability Threshold focuses on the potential that a frontier model may provide actors material \u201cuplift\u201d2 in excess of other publicly available research or existing tools, such as internet search.     Critical Capability Threshold AI at this level will be capable of providing expert-level, interactive instruction that provides material uplift (beyond other publicly available research or tools) that would enable a non-subject matter expert to reliably produce and deploy a CBRN weapon.          Offensive Cyber Operations     Offensive Cyber Operations focuses on risks that would arise from the use of a model by malicious actors to compromise digital systems with the intent to cause harm. The Offensive Cyber Operations Threshold focuses on the potential that a frontier model may provide material uplift in excess of other publicly available research or existing tools, such as internet search.     Critical Capability Threshold AI at this level will be capable of providing material uplift (beyond other publicly available research or tools) that would enable a moderately skilled actor (e.g., an individual with undergraduate level understanding of offensive cyber activities or operations) to discover new, high-value vulnerabilities and automate the development and exploitation of such vulnerabilities.          Automated AI R&D     Automating AI R&D processes could accelerate discovery and development of AI capabilities that will be critical for solving global challenges. However, Automated AI R&D could also accelerate the development of models that pose enhanced CBRN, Offensive Cybersecurity, or other severe risks.     Critical Capability Threshold AI at this level will be capable of replacing human researchers and fully automating the research, development, and deployment of frontier models that will pose severe risk such as accelerating the development of enhanced CBRN weapons and offensive cybersecurity methods.           2 Uplift studies evaluate whether a frontier model enhances the ability for a human to execute a specific type of attack when given access to a frontier model versus without access.",
            ".     \u2022 Information sharing and best practices development: Engagement in fora that bring together companies developing frontier models (e.g. Frontier Model Forum and Partnership on AI) and organized by government agencies (e.g. National Institute of Standards and Technologies). These platforms serve as an opportunity to share findings related to our models and to adopt recommendations from other leading companies.     \u2022 Fostering academic research for development of cutting-edge alignment techniques: Through initiatives such as the Amazon Research Awards and Amazon Research centers (e.g. USC + Amazon Center on Secure & Trusted Machine Learning, Amazon/ MIT Science Hub), we work with leading academic partners conducting research on frontier AI risks and novel risk mitigation approaches. Additionally, we advance our own research and publish findings in safety conferences, while borrowing learnings presented by other academic institutions at similar venues.     \u2022 Investments in advanced AI safety R&D: At Amazon, we accelerate our work in AI safety through initiatives such as our Amazon AGI SF Lab and the Trusted AI Challenge. These channels enable us to leverage the work of subject matter experts and discover promising approaches towards aligning our frontier models.     \u2022 Learning from our red teaming network: We continue to build our strong network of internal and external red teamers including red teamers with deep subject matter expertise in risks related to critical capabilities. These experts are critical in surfacing early insights into emerging critical capabilities and help us identify and implement appropriate mitigations.5     Efforts to develop further security measures include:     \u2022 Focused monitoring of threats and abuse by Amazon threat teams. Amazon\u2019s threat intelligence, Trust & Safety, and insider threat teams are building additional capabilities to track advanced threat actors and how they interact with and attempt to subvert security measures surrounding AI models. Learnings from this observation and analysis will provide important signals about where additional capabilities and layers of defense are needed to protect against any kind of \u201cside door\u201d or \u201cback door\u201d access to frontier models.     \u2022 Continued engagement with and investment in external security researchers. We will continue to invest in external security research, including bug bounty programs, academic research investments, and coordinated vulnerability disclosure programs that encourage and reward security experts to partner with us in research and development.     \u2022 Threat and response sharing with other frontier model providers. Amazon will utilize relevant industry bodies such as the Frontier Model Forum to share threat patterns and indicators, as well as responses and mitigations where appropriate, to enable better collective defense will against adversaries seeking to undermine frontier model security.     4. Governing our Frontier Model Safety Framework     Internally, we will use this framework to guide our model development and launch decisions. The implementation of the framework will require:     \u2022 The Frontier Model Safety Framework will be incorporated into the Amazon-wide Responsible AI Governance Program, enabling Amazon-wide visibility into the expectations, mechanisms, and adherence to the Framework.     \u2022 Frontier models developed by Amazon will be subject to maximal capability evaluations and safeguards evaluations prior to deployment. The results of these evaluations will be reviewed during launch processes. Models may not be publicly released unless safeguards are applied.     \u2022 The team performing the Critical Capability Threshold evaluations will report to Amazon senior leadership any evaluation that exceeds the Critical Capability Threshold. The report will be directed to the SVP for the model development team, the Chief Security Officer, and legal counsel. Amazon\u2019s senior leadership will review the plan for applying risk mitigations to address the Critical Capability, how we measure and have assurance about those mitigations, and approve the mitigations prior to deployment. Amazon\u2019s senior leadership will likewise review the safeguards evaluation report as part of a go/no-go decision.     \u2022 Amazon will publish, in connection with the launch of a frontier AI model launch (in model documentation, such as model service cards), information about the frontier model evaluation for safety and security.     As we advance our work on frontier models, we will also continue to enhance our AI safety evaluation and risk management processes. This evolving body of work requires an evolving framework as well. We will therefore revisit this Framework at least annually and update it as necessary to ensure that our protocols are appropriately robust to uphold our commitment to deploy safe and secure models. We will also update this Framework as needed in connection with significant technological developments.6     Appendix A: Amazon\u2019s Foundational Security Practices     Amazon\u2019s approach to AI security is built on a firm, well-tested foundation of enterprise security controls for Amazon as a whole, and the unique, industry-leading security capabilities of the AWS cloud environment. Amazon and AWS have a long and proven record of protecting sensitive assets and data from both external and internal threats. These capabilities provide the baseline for more advanced security approaches being designed and developed for deployment in the timeframes expected for the capabilities at our Critical Capability Thresholds and beyond.     The following is a brief summary of existing baseline security controls for Amazon employees and services.     \u2022 Culture of security. Security at Amazon is \u201cjob zero\u201d for all employees."
        ],
        "gt_answer": "The framework identifies risk domains including Chemical, Biological, Radiological, and Nuclear (CBRN) weapons proliferation and Offensive Cyber Operations.",
        "query_metadata": {
            "input_token": 0,
            "latency_ms": 0
        },
        "sagemaker_cost": {
            "sagemaker_inference_cost": 0.003437408333333333
        }
    },
    {
        "question": "How does Amazon evaluate whether a model meets or exceeds a Critical Capability Threshold?",
        "answer": "Amazon evaluates whether a model meets or exceeds a Critical Capability Threshold through a combination of automated and human-in-the-loop strategies, including maximal capability evaluations, safeguards evaluations, uplift studies, expert red teaming, and benchmarking. These evaluations assess the model's capabilities against specified risk domains, such as CBRN Weapons Proliferation, Offensive Cyber Operations, and Automated AI R&D. If a model demonstrates capabilities that meet or exceed a Critical Capability Threshold, Amazon will not deploy the model without implementing appropriate risk mitigation measures. These measures may include training data safeguards, alignment training, and other safety and security measures to prevent the elicitation of the critical capability and protect against unauthorized access risks. Amazon's senior leadership will review the evaluation results and approve the mitigations prior to deployment. The company will also publish information about the frontier model evaluation for safety and security as part of the model launch. Amazon's Frontier Model Safety Framework is a living document that will be updated annually and as needed in connection with significant technological developments.",
        "guardrails_output_assessment": null,
        "guardrails_context_assessment": null,
        "guardrails_input_assessment": null,
        "guardrails_blocked": false,
        "guardrails_block_level": "",
        "answer_metadata": {
            "inputTokens": 4556,
            "outputTokens": 310,
            "totalTokens": 4866,
            "latencyMs": 8268
        },
        "reference_contexts": [
            "The Offensive Cyber Operations Threshold focuses on the potential that a frontier model may provide material uplift in excess of other publicly available research or existing tools, such as internet search.     Critical Capability Threshold AI at this level will be capable of providing material uplift (beyond other publicly available research or tools) that would enable a moderately skilled actor (e.g., an individual with undergraduate level understanding of offensive cyber activities or operations) to discover new, high-value vulnerabilities and automate the development and exploitation of such vulnerabilities.          Automated AI R&D     Automating AI R&D processes could accelerate discovery and development of AI capabilities that will be critical for solving global challenges. However, Automated AI R&D could also accelerate the development of models that pose enhanced CBRN, Offensive Cybersecurity, or other severe risks.     Critical Capability Threshold AI at this level will be capable of replacing human researchers and fully automating the research, development, and deployment of frontier models that will pose severe risk such as accelerating the development of enhanced CBRN weapons and offensive cybersecurity methods.           2 Uplift studies evaluate whether a frontier model enhances the ability for a human to execute a specific type of attack when given access to a frontier model versus without access. \u201cUplift\u201d can be quantitatively assessed through uplift studies, which use controlled trials to compare the abilities of a group with access to the frontier model to the abilities of a group without access to the frontier model. https://www.frontiermodelforum.org/updates/issue-brief-preliminary-taxonomy-of-pre- deployment-frontier-ai-safety-evaluations/3     2. Evaluating Frontier Models for Critical Capabilities     We conduct evaluations on an ongoing basis, including during training and prior to deployment of new frontier models. We will re-evaluate deployed models prior to any major updates that could meaningfully enhance underlying capabilities. Our evaluation process includes \u201cmaximal capability evaluations\" to determine the outer bounds of our models\u2019 Critical Capabilities and a subsequent \"safeguards evaluation\"3 to assess the adequacy of the risk mitigation measures that are applied to a model. When a maximal capability evaluation indicates that a model has hit a Critical Capability Threshold, we will not deploy the model until we have implemented appropriate safeguards. We will use a range of methods to evaluate frontier models for capabilities that are as closely correlated to the Critical Capability Thresholds as possible. In most cases a single evaluation will not be sufficient for an informed determination as to whether a model has hit a Critical Capability Threshold. We will therefore use a range of evaluation approaches, including both automated and manual methods, including: \u2022 Automated Benchmarks: Benchmarking provides apples-to-apples comparisons between candidate models by     substituting an automated \"assessor\" mechanism for human judgement. We conduct comprehensive evaluations to assess our frontier models using state-of-the-art public benchmarks in addition to internal benchmarking on proprietary test sets built in collaboration with experts.     \u2022 Expert Red Teaming: Red teaming vendors and in-house red teaming experts test our models for safety and security. We work with specialized firms and academics to red-team our models to evaluate them for risks that require domain specific expertise.     \u2022 Uplift Studies: Uplift studies examine whether access to a model enhances the capability of human actors to perform a task compared to other existing resources (e.g., internet search; use of existing tools/technology).      3. Risk Mitigations: Safety and Security Measures for Frontier Models with Critical Capabilities     Upon determining that an Amazon model has reached a Critical Capability Threshold, we will implement a set of Safety Measures and Security Measures to prevent elicitation of the critical capability identified and to protect against inappropriate access risks. Safety Measures are designed to prevent the elicitation of the observed Critical Capabilities following deployment of the model. Security Measures are designed to prevent unauthorized access to model weights or guardrails implemented as part of the Safety Measures, which could enable a malicious actor to remove or bypass existing guardrails to exceed Critical Capability Thresholds. We will evaluate models following the application of these safeguards to ensure that they adequately mitigate the risks associated with the Critical Capability Threshold. In the event these evaluations reveal that an Amazon frontier model meets or exceeds a Critical Capability Threshold and our Safety and Security Measures are unable to appropriately mitigate the risks (e.g., by preventing reliable elicitation of the capability by malicious actors), we will not deploy the model until we have identified and implemented appropriate additional safeguards. Examples of current safety mitigations include:     \u2022 Training Data Safeguards: We implement a rigorous data review process across various model training stages that aims to identify and redact data that could give rise to unsafe behaviors.     \u2022 Alignment Training: We implement automated methods to ensure we meet the design objectives for each of Amazon\u2019s responsible AI dimensions, including safety and security. Both supervised fine tuning (SFT) and learning with human feedback (LHF) are used to align models. Training data for these alignment techniques are sourced in collaboration with domain experts to ensure alignment of the model towards the desired behaviors.",
            "1          Amazon\u2019s Frontier Model Safety Framework      At Amazon, we look to our leadership principles every day to guide our decision-making. Our approach to AI development naturally follows from our leadership principle \u201cSuccess and Scale Bring Broad Responsibility.\u201d As we continue to scale the capabilities of Amazon\u2019s frontier models and democratize access to the benefits of AI, we also take responsibility for mitigating the risks of our technology. Consistent with Amazon\u2019s endorsement of the Korea Frontier AI Safety Commitments,1 this Framework outlines the protocols we will follow to ensure that frontier models developed by Amazon do not expose critical capabilities that have the potential to create severe risks. At its core, this Framework reflects our commitment that we will not deploy frontier AI models developed by Amazon that exceed specified risk thresholds without appropriate safeguards in place. This Framework focuses on severe risks that are unique to frontier AI models as they scale in size and capability and which require specialized evaluation methods and safeguards. Our approach to managing these frontier risks complements Amazon's broader approach to responsible AI, which includes comprehensive practices to control for risks across eight key dimensions. We are grateful to METR, a nonprofit research organization specializing in AI evaluations, for feedback during the development of this Framework. This Framework is a living document and will be updated to reflect evolving model capabilities and advances in the science underlying AI safety evaluations and mitigations.               Overview of the Frontier Model Safety Framework     Our Framework establishes the processes Amazon will use to identify, assess, and manage potential severe risks that could arise as we develop more advanced and highly-capable frontier AI models. First, it specifies Critical Capability Thresholds, a set of model capabilities that have the potential to cause significant harm to the public if misused. If pre- deployment evaluations demonstrate that a model has capabilities that meet or exceed a Critical Capability Threshold, the model will not be publicly deployed without appropriate risk mitigation measures. Second, it describes our Critical Capability Evaluations, a variety of automated and human-in-the-loop strategies to determine whether our models demonstrate capabilities that meet or exceed our Critical Capability Thresholds. Third, it details how we develop and implement Risk Mitigations when a model demonstrates capabilities that meet or exceed a Critical Capability Threshold.      1 https://www.gov.uk/government/publications/frontier-ai-safety-commitments-ai-seoul-summit-2024/frontier-ai-safety-     commitments-ai-seoul-summit-20242     1. Critical Capability Thresholds     Critical Capability Thresholds describe model capabilities within specified risk domains that could cause severe public safety risks. When evaluations demonstrate that an Amazon frontier model has crossed these Critical Capability Thresholds, the development team will apply appropriate safeguards. Chemical, Biological, Radiological, and Nuclear (CBRN) Weapons Proliferation     CBRN Weapons Proliferation focuses on the risk that a model may be able to guide malicious actors in developing and deploying CBRN weapons. The CBRN Capability Threshold focuses on the potential that a frontier model may provide actors material \u201cuplift\u201d2 in excess of other publicly available research or existing tools, such as internet search.     Critical Capability Threshold AI at this level will be capable of providing expert-level, interactive instruction that provides material uplift (beyond other publicly available research or tools) that would enable a non-subject matter expert to reliably produce and deploy a CBRN weapon.          Offensive Cyber Operations     Offensive Cyber Operations focuses on risks that would arise from the use of a model by malicious actors to compromise digital systems with the intent to cause harm. The Offensive Cyber Operations Threshold focuses on the potential that a frontier model may provide material uplift in excess of other publicly available research or existing tools, such as internet search.     Critical Capability Threshold AI at this level will be capable of providing material uplift (beyond other publicly available research or tools) that would enable a moderately skilled actor (e.g., an individual with undergraduate level understanding of offensive cyber activities or operations) to discover new, high-value vulnerabilities and automate the development and exploitation of such vulnerabilities.          Automated AI R&D     Automating AI R&D processes could accelerate discovery and development of AI capabilities that will be critical for solving global challenges. However, Automated AI R&D could also accelerate the development of models that pose enhanced CBRN, Offensive Cybersecurity, or other severe risks.     Critical Capability Threshold AI at this level will be capable of replacing human researchers and fully automating the research, development, and deployment of frontier models that will pose severe risk such as accelerating the development of enhanced CBRN weapons and offensive cybersecurity methods.           2 Uplift studies evaluate whether a frontier model enhances the ability for a human to execute a specific type of attack when given access to a frontier model versus without access.",
            ".     \u2022 Information sharing and best practices development: Engagement in fora that bring together companies developing frontier models (e.g. Frontier Model Forum and Partnership on AI) and organized by government agencies (e.g. National Institute of Standards and Technologies). These platforms serve as an opportunity to share findings related to our models and to adopt recommendations from other leading companies.     \u2022 Fostering academic research for development of cutting-edge alignment techniques: Through initiatives such as the Amazon Research Awards and Amazon Research centers (e.g. USC + Amazon Center on Secure & Trusted Machine Learning, Amazon/ MIT Science Hub), we work with leading academic partners conducting research on frontier AI risks and novel risk mitigation approaches. Additionally, we advance our own research and publish findings in safety conferences, while borrowing learnings presented by other academic institutions at similar venues.     \u2022 Investments in advanced AI safety R&D: At Amazon, we accelerate our work in AI safety through initiatives such as our Amazon AGI SF Lab and the Trusted AI Challenge. These channels enable us to leverage the work of subject matter experts and discover promising approaches towards aligning our frontier models.     \u2022 Learning from our red teaming network: We continue to build our strong network of internal and external red teamers including red teamers with deep subject matter expertise in risks related to critical capabilities. These experts are critical in surfacing early insights into emerging critical capabilities and help us identify and implement appropriate mitigations.5     Efforts to develop further security measures include:     \u2022 Focused monitoring of threats and abuse by Amazon threat teams. Amazon\u2019s threat intelligence, Trust & Safety, and insider threat teams are building additional capabilities to track advanced threat actors and how they interact with and attempt to subvert security measures surrounding AI models. Learnings from this observation and analysis will provide important signals about where additional capabilities and layers of defense are needed to protect against any kind of \u201cside door\u201d or \u201cback door\u201d access to frontier models.     \u2022 Continued engagement with and investment in external security researchers. We will continue to invest in external security research, including bug bounty programs, academic research investments, and coordinated vulnerability disclosure programs that encourage and reward security experts to partner with us in research and development.     \u2022 Threat and response sharing with other frontier model providers. Amazon will utilize relevant industry bodies such as the Frontier Model Forum to share threat patterns and indicators, as well as responses and mitigations where appropriate, to enable better collective defense will against adversaries seeking to undermine frontier model security.     4. Governing our Frontier Model Safety Framework     Internally, we will use this framework to guide our model development and launch decisions. The implementation of the framework will require:     \u2022 The Frontier Model Safety Framework will be incorporated into the Amazon-wide Responsible AI Governance Program, enabling Amazon-wide visibility into the expectations, mechanisms, and adherence to the Framework.     \u2022 Frontier models developed by Amazon will be subject to maximal capability evaluations and safeguards evaluations prior to deployment. The results of these evaluations will be reviewed during launch processes. Models may not be publicly released unless safeguards are applied.     \u2022 The team performing the Critical Capability Threshold evaluations will report to Amazon senior leadership any evaluation that exceeds the Critical Capability Threshold. The report will be directed to the SVP for the model development team, the Chief Security Officer, and legal counsel. Amazon\u2019s senior leadership will review the plan for applying risk mitigations to address the Critical Capability, how we measure and have assurance about those mitigations, and approve the mitigations prior to deployment. Amazon\u2019s senior leadership will likewise review the safeguards evaluation report as part of a go/no-go decision.     \u2022 Amazon will publish, in connection with the launch of a frontier AI model launch (in model documentation, such as model service cards), information about the frontier model evaluation for safety and security.     As we advance our work on frontier models, we will also continue to enhance our AI safety evaluation and risk management processes. This evolving body of work requires an evolving framework as well. We will therefore revisit this Framework at least annually and update it as necessary to ensure that our protocols are appropriately robust to uphold our commitment to deploy safe and secure models. We will also update this Framework as needed in connection with significant technological developments.6     Appendix A: Amazon\u2019s Foundational Security Practices     Amazon\u2019s approach to AI security is built on a firm, well-tested foundation of enterprise security controls for Amazon as a whole, and the unique, industry-leading security capabilities of the AWS cloud environment. Amazon and AWS have a long and proven record of protecting sensitive assets and data from both external and internal threats. These capabilities provide the baseline for more advanced security approaches being designed and developed for deployment in the timeframes expected for the capabilities at our Critical Capability Thresholds and beyond.     The following is a brief summary of existing baseline security controls for Amazon employees and services.     \u2022 Culture of security. Security at Amazon is \u201cjob zero\u201d for all employees."
        ],
        "gt_answer": "Amazon conducts Critical Capability Evaluations using a combination of automated and human-in-the-loop strategies to assess if models demonstrate capabilities that meet or exceed the defined thresholds.",
        "query_metadata": {
            "input_token": 0,
            "latency_ms": 0
        },
        "sagemaker_cost": {
            "sagemaker_inference_cost": 0.0027789666666666662
        }
    },
    {
        "question": "What actions are taken if a model is found to meet or exceed a Critical Capability Threshold?",
        "answer": "If a model is found to meet or exceed a Critical Capability Threshold, Amazon will not deploy the model until appropriate safeguards are implemented. The company will use a range of evaluation approaches, including automated benchmarks, expert red teaming, and uplift studies, to assess the model's capabilities. If the model is deemed to pose a severe risk, Amazon will implement a set of Safety Measures and Security Measures to prevent the elicitation of the critical capability and protect against unauthorized access risks. The company will also evaluate the model following the application of these safeguards to ensure that they adequately mitigate the risks associated with the Critical Capability Threshold. If the evaluations reveal that the model meets or exceeds a Critical Capability Threshold and the safeguards are unable to mitigate the risks, Amazon will not deploy the model until additional safeguards are identified and implemented. Examples of current safety mitigations include training data safeguards, alignment training, and the implementation of guardrails. Amazon will also continue to enhance its AI safety evaluation and risk management processes, revisiting and updating its Frontier Model Safety Framework at least annually to ensure that its protocols are robust and uphold its commitment to deploy safe and secure models.",
        "guardrails_output_assessment": null,
        "guardrails_context_assessment": null,
        "guardrails_input_assessment": null,
        "guardrails_blocked": false,
        "guardrails_block_level": "",
        "answer_metadata": {
            "inputTokens": 4609,
            "outputTokens": 338,
            "totalTokens": 4947,
            "latencyMs": 9180
        },
        "reference_contexts": [
            "The Offensive Cyber Operations Threshold focuses on the potential that a frontier model may provide material uplift in excess of other publicly available research or existing tools, such as internet search.     Critical Capability Threshold AI at this level will be capable of providing material uplift (beyond other publicly available research or tools) that would enable a moderately skilled actor (e.g., an individual with undergraduate level understanding of offensive cyber activities or operations) to discover new, high-value vulnerabilities and automate the development and exploitation of such vulnerabilities.          Automated AI R&D     Automating AI R&D processes could accelerate discovery and development of AI capabilities that will be critical for solving global challenges. However, Automated AI R&D could also accelerate the development of models that pose enhanced CBRN, Offensive Cybersecurity, or other severe risks.     Critical Capability Threshold AI at this level will be capable of replacing human researchers and fully automating the research, development, and deployment of frontier models that will pose severe risk such as accelerating the development of enhanced CBRN weapons and offensive cybersecurity methods.           2 Uplift studies evaluate whether a frontier model enhances the ability for a human to execute a specific type of attack when given access to a frontier model versus without access. \u201cUplift\u201d can be quantitatively assessed through uplift studies, which use controlled trials to compare the abilities of a group with access to the frontier model to the abilities of a group without access to the frontier model. https://www.frontiermodelforum.org/updates/issue-brief-preliminary-taxonomy-of-pre- deployment-frontier-ai-safety-evaluations/3     2. Evaluating Frontier Models for Critical Capabilities     We conduct evaluations on an ongoing basis, including during training and prior to deployment of new frontier models. We will re-evaluate deployed models prior to any major updates that could meaningfully enhance underlying capabilities. Our evaluation process includes \u201cmaximal capability evaluations\" to determine the outer bounds of our models\u2019 Critical Capabilities and a subsequent \"safeguards evaluation\"3 to assess the adequacy of the risk mitigation measures that are applied to a model. When a maximal capability evaluation indicates that a model has hit a Critical Capability Threshold, we will not deploy the model until we have implemented appropriate safeguards. We will use a range of methods to evaluate frontier models for capabilities that are as closely correlated to the Critical Capability Thresholds as possible. In most cases a single evaluation will not be sufficient for an informed determination as to whether a model has hit a Critical Capability Threshold. We will therefore use a range of evaluation approaches, including both automated and manual methods, including: \u2022 Automated Benchmarks: Benchmarking provides apples-to-apples comparisons between candidate models by     substituting an automated \"assessor\" mechanism for human judgement. We conduct comprehensive evaluations to assess our frontier models using state-of-the-art public benchmarks in addition to internal benchmarking on proprietary test sets built in collaboration with experts.     \u2022 Expert Red Teaming: Red teaming vendors and in-house red teaming experts test our models for safety and security. We work with specialized firms and academics to red-team our models to evaluate them for risks that require domain specific expertise.     \u2022 Uplift Studies: Uplift studies examine whether access to a model enhances the capability of human actors to perform a task compared to other existing resources (e.g., internet search; use of existing tools/technology).      3. Risk Mitigations: Safety and Security Measures for Frontier Models with Critical Capabilities     Upon determining that an Amazon model has reached a Critical Capability Threshold, we will implement a set of Safety Measures and Security Measures to prevent elicitation of the critical capability identified and to protect against inappropriate access risks. Safety Measures are designed to prevent the elicitation of the observed Critical Capabilities following deployment of the model. Security Measures are designed to prevent unauthorized access to model weights or guardrails implemented as part of the Safety Measures, which could enable a malicious actor to remove or bypass existing guardrails to exceed Critical Capability Thresholds. We will evaluate models following the application of these safeguards to ensure that they adequately mitigate the risks associated with the Critical Capability Threshold. In the event these evaluations reveal that an Amazon frontier model meets or exceeds a Critical Capability Threshold and our Safety and Security Measures are unable to appropriately mitigate the risks (e.g., by preventing reliable elicitation of the capability by malicious actors), we will not deploy the model until we have identified and implemented appropriate additional safeguards. Examples of current safety mitigations include:     \u2022 Training Data Safeguards: We implement a rigorous data review process across various model training stages that aims to identify and redact data that could give rise to unsafe behaviors.     \u2022 Alignment Training: We implement automated methods to ensure we meet the design objectives for each of Amazon\u2019s responsible AI dimensions, including safety and security. Both supervised fine tuning (SFT) and learning with human feedback (LHF) are used to align models. Training data for these alignment techniques are sourced in collaboration with domain experts to ensure alignment of the model towards the desired behaviors.",
            "1          Amazon\u2019s Frontier Model Safety Framework      At Amazon, we look to our leadership principles every day to guide our decision-making. Our approach to AI development naturally follows from our leadership principle \u201cSuccess and Scale Bring Broad Responsibility.\u201d As we continue to scale the capabilities of Amazon\u2019s frontier models and democratize access to the benefits of AI, we also take responsibility for mitigating the risks of our technology. Consistent with Amazon\u2019s endorsement of the Korea Frontier AI Safety Commitments,1 this Framework outlines the protocols we will follow to ensure that frontier models developed by Amazon do not expose critical capabilities that have the potential to create severe risks. At its core, this Framework reflects our commitment that we will not deploy frontier AI models developed by Amazon that exceed specified risk thresholds without appropriate safeguards in place. This Framework focuses on severe risks that are unique to frontier AI models as they scale in size and capability and which require specialized evaluation methods and safeguards. Our approach to managing these frontier risks complements Amazon's broader approach to responsible AI, which includes comprehensive practices to control for risks across eight key dimensions. We are grateful to METR, a nonprofit research organization specializing in AI evaluations, for feedback during the development of this Framework. This Framework is a living document and will be updated to reflect evolving model capabilities and advances in the science underlying AI safety evaluations and mitigations.               Overview of the Frontier Model Safety Framework     Our Framework establishes the processes Amazon will use to identify, assess, and manage potential severe risks that could arise as we develop more advanced and highly-capable frontier AI models. First, it specifies Critical Capability Thresholds, a set of model capabilities that have the potential to cause significant harm to the public if misused. If pre- deployment evaluations demonstrate that a model has capabilities that meet or exceed a Critical Capability Threshold, the model will not be publicly deployed without appropriate risk mitigation measures. Second, it describes our Critical Capability Evaluations, a variety of automated and human-in-the-loop strategies to determine whether our models demonstrate capabilities that meet or exceed our Critical Capability Thresholds. Third, it details how we develop and implement Risk Mitigations when a model demonstrates capabilities that meet or exceed a Critical Capability Threshold.      1 https://www.gov.uk/government/publications/frontier-ai-safety-commitments-ai-seoul-summit-2024/frontier-ai-safety-     commitments-ai-seoul-summit-20242     1. Critical Capability Thresholds     Critical Capability Thresholds describe model capabilities within specified risk domains that could cause severe public safety risks. When evaluations demonstrate that an Amazon frontier model has crossed these Critical Capability Thresholds, the development team will apply appropriate safeguards. Chemical, Biological, Radiological, and Nuclear (CBRN) Weapons Proliferation     CBRN Weapons Proliferation focuses on the risk that a model may be able to guide malicious actors in developing and deploying CBRN weapons. The CBRN Capability Threshold focuses on the potential that a frontier model may provide actors material \u201cuplift\u201d2 in excess of other publicly available research or existing tools, such as internet search.     Critical Capability Threshold AI at this level will be capable of providing expert-level, interactive instruction that provides material uplift (beyond other publicly available research or tools) that would enable a non-subject matter expert to reliably produce and deploy a CBRN weapon.          Offensive Cyber Operations     Offensive Cyber Operations focuses on risks that would arise from the use of a model by malicious actors to compromise digital systems with the intent to cause harm. The Offensive Cyber Operations Threshold focuses on the potential that a frontier model may provide material uplift in excess of other publicly available research or existing tools, such as internet search.     Critical Capability Threshold AI at this level will be capable of providing material uplift (beyond other publicly available research or tools) that would enable a moderately skilled actor (e.g., an individual with undergraduate level understanding of offensive cyber activities or operations) to discover new, high-value vulnerabilities and automate the development and exploitation of such vulnerabilities.          Automated AI R&D     Automating AI R&D processes could accelerate discovery and development of AI capabilities that will be critical for solving global challenges. However, Automated AI R&D could also accelerate the development of models that pose enhanced CBRN, Offensive Cybersecurity, or other severe risks.     Critical Capability Threshold AI at this level will be capable of replacing human researchers and fully automating the research, development, and deployment of frontier models that will pose severe risk such as accelerating the development of enhanced CBRN weapons and offensive cybersecurity methods.           2 Uplift studies evaluate whether a frontier model enhances the ability for a human to execute a specific type of attack when given access to a frontier model versus without access.",
            ".     \u2022 Information sharing and best practices development: Engagement in fora that bring together companies developing frontier models (e.g. Frontier Model Forum and Partnership on AI) and organized by government agencies (e.g. National Institute of Standards and Technologies). These platforms serve as an opportunity to share findings related to our models and to adopt recommendations from other leading companies.     \u2022 Fostering academic research for development of cutting-edge alignment techniques: Through initiatives such as the Amazon Research Awards and Amazon Research centers (e.g. USC + Amazon Center on Secure & Trusted Machine Learning, Amazon/ MIT Science Hub), we work with leading academic partners conducting research on frontier AI risks and novel risk mitigation approaches. Additionally, we advance our own research and publish findings in safety conferences, while borrowing learnings presented by other academic institutions at similar venues.     \u2022 Investments in advanced AI safety R&D: At Amazon, we accelerate our work in AI safety through initiatives such as our Amazon AGI SF Lab and the Trusted AI Challenge. These channels enable us to leverage the work of subject matter experts and discover promising approaches towards aligning our frontier models.     \u2022 Learning from our red teaming network: We continue to build our strong network of internal and external red teamers including red teamers with deep subject matter expertise in risks related to critical capabilities. These experts are critical in surfacing early insights into emerging critical capabilities and help us identify and implement appropriate mitigations.5     Efforts to develop further security measures include:     \u2022 Focused monitoring of threats and abuse by Amazon threat teams. Amazon\u2019s threat intelligence, Trust & Safety, and insider threat teams are building additional capabilities to track advanced threat actors and how they interact with and attempt to subvert security measures surrounding AI models. Learnings from this observation and analysis will provide important signals about where additional capabilities and layers of defense are needed to protect against any kind of \u201cside door\u201d or \u201cback door\u201d access to frontier models.     \u2022 Continued engagement with and investment in external security researchers. We will continue to invest in external security research, including bug bounty programs, academic research investments, and coordinated vulnerability disclosure programs that encourage and reward security experts to partner with us in research and development.     \u2022 Threat and response sharing with other frontier model providers. Amazon will utilize relevant industry bodies such as the Frontier Model Forum to share threat patterns and indicators, as well as responses and mitigations where appropriate, to enable better collective defense will against adversaries seeking to undermine frontier model security.     4. Governing our Frontier Model Safety Framework     Internally, we will use this framework to guide our model development and launch decisions. The implementation of the framework will require:     \u2022 The Frontier Model Safety Framework will be incorporated into the Amazon-wide Responsible AI Governance Program, enabling Amazon-wide visibility into the expectations, mechanisms, and adherence to the Framework.     \u2022 Frontier models developed by Amazon will be subject to maximal capability evaluations and safeguards evaluations prior to deployment. The results of these evaluations will be reviewed during launch processes. Models may not be publicly released unless safeguards are applied.     \u2022 The team performing the Critical Capability Threshold evaluations will report to Amazon senior leadership any evaluation that exceeds the Critical Capability Threshold. The report will be directed to the SVP for the model development team, the Chief Security Officer, and legal counsel. Amazon\u2019s senior leadership will review the plan for applying risk mitigations to address the Critical Capability, how we measure and have assurance about those mitigations, and approve the mitigations prior to deployment. Amazon\u2019s senior leadership will likewise review the safeguards evaluation report as part of a go/no-go decision.     \u2022 Amazon will publish, in connection with the launch of a frontier AI model launch (in model documentation, such as model service cards), information about the frontier model evaluation for safety and security.     As we advance our work on frontier models, we will also continue to enhance our AI safety evaluation and risk management processes. This evolving body of work requires an evolving framework as well. We will therefore revisit this Framework at least annually and update it as necessary to ensure that our protocols are appropriately robust to uphold our commitment to deploy safe and secure models. We will also update this Framework as needed in connection with significant technological developments.6     Appendix A: Amazon\u2019s Foundational Security Practices     Amazon\u2019s approach to AI security is built on a firm, well-tested foundation of enterprise security controls for Amazon as a whole, and the unique, industry-leading security capabilities of the AWS cloud environment. Amazon and AWS have a long and proven record of protecting sensitive assets and data from both external and internal threats. These capabilities provide the baseline for more advanced security approaches being designed and developed for deployment in the timeframes expected for the capabilities at our Critical Capability Thresholds and beyond.     The following is a brief summary of existing baseline security controls for Amazon employees and services.     \u2022 Culture of security. Security at Amazon is \u201cjob zero\u201d for all employees."
        ],
        "gt_answer": "If a model meets or exceeds a Critical Capability Threshold, Amazon applies appropriate risk mitigation measures and does not publicly deploy the model without these safeguards in place.",
        "query_metadata": {
            "input_token": 0,
            "latency_ms": 0
        },
        "sagemaker_cost": {
            "sagemaker_inference_cost": 0.0030855
        }
    }
]