{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a6fdaec-a86e-47f7-9ed9-3d0242e5a9b3",
   "metadata": {},
   "source": [
    "# üìä Evaluation in Flotorch\n",
    "\n",
    "[Flotorch](https://www.flotorch.ai/) provides a comprehensive evaluation framework for Retrieval-Augmented Generation (RAG) systems. It helps assess and compare Large Language Models (LLMs) based on relevance, quality, cost, and performance to support enterprise-grade deployments.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Key Evaluation Features\n",
    "\n",
    "- **Automated LLM Evaluation**  \n",
    "  Flotorch automates evaluation across:\n",
    "  - Relevance\n",
    "  - Fluency\n",
    "  - Robustness\n",
    "  - Cost\n",
    "  - Execution Speed\n",
    "\n",
    "- **Performance Metrics**  \n",
    "  It generates quantitative scores for evaluating how well a model performs across different criteria.\n",
    "\n",
    "- **Cost and Time Insights**  \n",
    "  Offers pricing and latency breakdowns for different LLM setups, enabling cost-effective choices.\n",
    "\n",
    "- **Data-Driven Decision-Making**  \n",
    "  Helps teams align LLM usage with specific application goals, budget, and performance needs.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Evaluation Workflow\n",
    "\n",
    "1. **Experiment Configuration**  \n",
    "   Define models, parameters, and goals for evaluation.\n",
    "\n",
    "2. **Automated Execution**  \n",
    "   Run evaluation pipelines to generate performance data.\n",
    "\n",
    "3. **Results Analysis**  \n",
    "   View dashboards or reports that summarize evaluation results.\n",
    "\n",
    "4. **Expert Evaluation (Optional)**  \n",
    "   Combine automatic evaluation with human review for more nuanced feedback.\n",
    "\n",
    "---\n",
    "\n",
    "This evaluation framework enables continuous monitoring, benchmarking, and optimization of RAG systems using LLMs, helping organizations deploy more reliable and efficient AI solutions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce74afb-8660-4576-b446-2b73f4b38191",
   "metadata": {},
   "source": [
    "## Load experiment config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7bb7517-ea81-426b-8d2f-2e246970b996",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_config_data = {\n",
    "            \"temp_retrieval_llm\": \"0.1\",\n",
    "            \"retrival_service\": \"sagemaker\",\n",
    "            \"eval_retrieval_model\": \"bedrock/cohere.command-r-v1:0\",\n",
    "            \"eval_prompt\": prompt\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54042c51-bdab-4524-922c-d65651d09004",
   "metadata": {},
   "source": [
    "## Load inference metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "982fb77b-f992-4cf9-ba5f-6e8e63941a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(f\"./results/{exp_config_data['retrival_service']}_inference_metrics.json\", \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3f7e0c2-87b7-43ed-8c1c-5803fb6abd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_file_path = './data/eval_prompt.json'\n",
    "with open(prompt_file_path, 'r') as f:\n",
    "    prompt = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59153913-5f0a-4a5f-8fd6-a7af3f5ae19b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a0c847c-dec5-4bb0-a29f-1f74f2ee5d91",
   "metadata": {},
   "source": [
    "## Load Evaluator Class\n",
    "\n",
    "### üß† Evaluation with `CustomEvaluator`\n",
    "\n",
    "```python\n",
    "processor = CustomEvaluator(evaluator_llm=exp_config_data['eval_retrieval_model'])\n",
    "results = processor.evaluate(data)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üîπ Step-by-Step Breakdown\n",
    "\n",
    "| Line | Description |\n",
    "|------|-------------|\n",
    "| `processor = CustomEvaluator(...)` | Instantiates a `CustomEvaluator` using a language model specified in the config (`exp_config_data['eval_retrieval_model']`). |\n",
    "| `results = processor.evaluate(data)` | Runs the evaluation on the `data` using the evaluator, returning performance metrics or scoring output. |\n",
    "\n",
    "---\n",
    "\n",
    "#### üß© Key Components\n",
    "\n",
    "- **`CustomEvaluator`**: A custom class designed to handle evaluation logic, potentially wrapping RAGAS or similar frameworks.\n",
    "- **`evaluator_llm`**: The evaluation language model (e.g. GPT, Claude, etc.) used for scoring responses.\n",
    "- **`data`**: A list of evaluation items (e.g. questions, answers, reference contexts).\n",
    "- **`results`**: The output from the evaluation ‚Äî typically a dictionary or structured result with metric scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "913229fb-86b2-4da2-9321-a461618a5fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluator import CustomEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d81ee62-cff0-4581-9274-ba047fe199b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = CustomEvaluator(evaluator_llm_info = exp_config_data)\n",
    "results = processor.evaluate(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59655f87-ad1c-4c84-8bba-e7a4d8037d0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What are the three main sub-tasks in Knowledge Base Question Answering (KBQA) as identified in the paper?',\n",
       " 'answer': 'The three main sub-tasks in Knowledge Base Question Answering (KBQA) are topic entity detection, entity linking, and relation detection.',\n",
       " 'guardrails_output_assessment': None,\n",
       " 'guardrails_context_assessment': None,\n",
       " 'guardrails_input_assessment': None,\n",
       " 'guardrails_blocked': False,\n",
       " 'guardrails_block_level': '',\n",
       " 'answer_metadata': {'inputTokens': 3701,\n",
       "  'outputTokens': 34,\n",
       "  'totalTokens': 3735,\n",
       "  'latencyMs': 3019},\n",
       " 'reference_contexts': ['[   {     \"question\": \"What are the three main sub-tasks in Knowledge Base Question Answering (KBQA) as identified in the paper?\",     \"answer\": \"The three main sub-tasks in KBQA are topic entity detection, entity linking, and relation detection.\"   },   {     \"question\": \"How does the proposed method handle large-scale knowledge bases efficiently?\",     \"answer\": \"The method uses an IR-based retrieval approach to collect high-quality candidates efficiently, enabling adaptation to large-scale KBs.\"   },   {     \"question\": \"What is the role of BERT in the proposed KBQA framework?\",     \"answer\": \"BERT is used to improve accuracy across all three sub-tasks by serving as a shared encoder in the multi-task learning framework.\"   },   {     \"question\": \"How does multi-task learning benefit the proposed KBQA model?\",     \"answer\": \"Multi-task learning allows the unified model to achieve further improvements with only one-third of the original parameters.\"   },   {     \"question\": \"On which datasets did the proposed model achieve competitive or superior performance?\",     \"answer\": \"The model achieved competitive results on the SimpleQuestions dataset and superior performance on the FreebaseQA dataset.\"   },   {     \"question\": \"What is the primary motivation behind creating the PRACTIQ dataset?\",     \"answer\": \"The PRACTIQ dataset was created to address the limitations of existing text-to-SQL datasets, which primarily focus on clear, answerable user queries. Real-world user questions are often ambiguous or unanswerable due to various factors, and PRACTIQ aims to provide a practical dataset that includes such complexities to better train and evaluate conversational text-to-SQL systems.\"   },   {     \"question\": \"How are ambiguous and unanswerable questions defined in the PRACTIQ dataset?\",     \"answer\": \"In the PRACTIQ dataset, a question is considered ambiguous if it has multiple valid interpretations given the database schema. A question is deemed unanswerable if the corresponding database does not contain the data that the question is asking for.\"   },   {     \"question\": \"What methodology was used to construct conversations in the PRACTIQ dataset?\",     \"answer\": \"Conversations in the PRACTIQ dataset are constructed with four turns: the initial user question, an assistant response seeking clarification, the user\\'s clarification, and the assistant\\'s clarified SQL response along with a natural language explanation of the execution results.\"   },   {     \"question\": \"What are the four categories of ambiguous questions identified in the PRACTIQ dataset?\",     \"answer\": \"The four categories of ambiguous questions identified in the PRACTIQ dataset are: (1) Ambiguity due to multiple columns with similar names, (2) Ambiguity due to multiple tables containing similar information, (3) Ambiguity arising from unspecified aggregation operations, and (4) Ambiguity caused by vague temporal references.\"   },   {     \"question\": \"What are the four categories of unanswerable questions identified in the PRACTIQ dataset?\",     \"answer\": \"The four categories of unanswerable questions identified in the PRACTIQ dataset are: (1) Questions about non-existent entities, (2) Questions requiring external knowledge not present in the database, (3) Questions involving data that is missing or incomplete, and (4) Questions that are logically inconsistent or contradictory.\"   },   {     \"question\": \"How does the PRACTIQ dataset handle ambiguous queries without seeking user clarification?\",     \"answer\": \"For some ambiguous queries, the PRACTIQ dataset includes helpful SQL responses that consider multiple aspects of ambiguity, providing direct answers without requesting user clarification.\"   },   {     \"question\": \"What approach was used to benchmark performance on the PRACTIQ dataset?\",     \"answer\": \"To benchmark performance on the PRACTIQ dataset, the authors implemented large language model (LLM)-based baselines using various LLMs. Their approach involves two steps: question category classification and clarification SQL prediction.\"   },   {     \"question\": \"What were the findings regarding state-of-the-art systems\\' performance on ambiguous and unanswerable questions?\",     \"answer\": \"The experiments revealed that state-of-the-art systems struggle to handle ambiguous and unanswerable questions effectively, highlighting the need for datasets like PRACTIQ to improve system robustness.\"   },   {     \"question\": \"Is the PRACTIQ dataset publicly available for research purposes?\",     \"answer\": \"Yes, the authors have indicated that they will release the code for data generation and experiments on GitHub to facilitate further research in this area.\"   },   {     \"question\": \"What is the significance of the PRACTIQ dataset in the context of real-world applications?\"',\n",
       "  'We acknowledge that this method does not consider the already covered semantics in the first retrieval step, when we do the second step retrieval. Since the main goal of the retrieval step is to collect a list of high-quality candidates, we will perform better semantic matching in the re-ranking step with more powerful neural networks. If multi-hop relation- chains are needed, we can iterate this process until reaching the maximum steps. Usually, the number of max-hop is pre-computed on the target question sets. Another way is to utilize a model to decide when to stop (Chen et al., 2019b), however we will leave this option in the future work.     After collecting a list of relation-chains, we leverage another BERT model to compute the similarity between a question Q and each relation- chain r. Each pair of Q and r will be represented as a sequence of tokens with the format ‚Äú[CLS] question [SEP] topic-entity name [SEP] relation chain [SEP] answer name [SEP] answer types [SEP]‚Äù, where topic-entity name is the name for the linked entity node, relation chain is the word sequence of a candidate relation-chain3, answer name is the name of the trailing node in the relation-chain, and answer types are all types of the trailing node. The hidden vector for the [CLS] token will be fed into a linear layer (with one output neuron) to predict the similarity between Q and r.     5 Multi-Task Learning for KBQA     5.1 Training Objectives     For the topic entity detection model, we define the objective function as the cross-entropy loss be- tween true distributions and predicted distributions. We sum up the cross-entropy losses for both start and end models, and average over all N training instances:     L(Œ∏t) = ‚àí 1     N     N‚àë i=1     log(P i s) + log(P i     e) (3)     where Œ∏t is the trainable parameter for topic entity detection model.     3A relation-chain is split into a word sequence based on delimiters such as periods, hyphens and underscores.     Both entity linking and relation detection tasks are ranking tasks, therefore we leverage a hinge loss function for both tasks:     L(Œ∏) = ‚àí 1     N     N‚àë i=1     max(0, l+ s(Q, c‚àí)‚àí s(Q, c+)     (4) Where Œ∏ is the trainable parameter, l is a margin, s(Q, c) can be the model of Pl or Pr, c+ is a correct candidate, and c‚àí is an incorrect candidate. We set l = 1.0 in this work.     5.2 Multi-Task Learning A naive approach would be to use three different BERT encoders for the topic entity detection, entity linking and relation detection sub-tasks individu- ally. Since BERT model is a very large model, it is expensive to host three BERT models in real ap- plications. To address this, we propose to share a BERT encoder across all three sub-tasks, and define lean layers for each individual sub-task on top of the shared layer. This unified model is then trained under the multi-task learning framework proposed by Liu et al. (2019). First, training instances for each sub-tasks are packed into mini-batches sep- arately. At the beginning of each training epoch, mini-batches from all three sub-tasks are mixed together and randomly shuffled. During training, a mini-batch is selected, and the model is updated according to the task-specific objective for the se- lected mini-batch.     6 Experiments     We evaluate the effectiveness of our model on stan- dard benchmarks in this section. We first conduct experiments on each sub-task with a separate BERT model in Section 6.2, 6.3 and 6.4, then evaluate the influence of sharing a BERT encoder for all three models in Section 6.5. Finally, we benchmark our method on full Freebase in Section 6.6.     6.1 Datasets and Basic Settings We evaluate our proposed model on two large-scale benchmarks: SimpleQuestions and FreebaseQA. Other existing datasets, such as WebQuestions (Be- rant et al., 2013), Free917 (Cai and Yates, 2013) and WebQSP (Yih et al., 2016), are not considered, because they only contain few thousands of ques- tions which is even less than the number of relation types in Freebase.     SimpleQuestions: The SimpleQuestions dataset (Bordes et al., 2015) is so far the largestKBQA dataset. It consists of 108,442 English questions written by human annotators, and all questions can be answered by 1-hop relation chains in Freebase. Each question is annotated with a gold-standard subject-relation-object triple from Freebase.',\n",
       "  'Previous methods usually worked on a subset of KB in order to fit KB into memory. For entity linking, some sophisticated heuristics were com- monly used to collect entity candidates. For rela- tion detection, previous work usually enumerated all possible 1-hop and 2-hop relation-chains (start- ing from linked entity nodes) as candidates. AllWho wrote the book Beau Geste ?     MID name type     m.04wxy8 Beau Geste book     m.0dl_h4 Beau Geste film     m.051vvdc Beau Geste music     ‚Ä¶ ‚Ä¶ ‚Ä¶     (2) Entity linking     predicate object MID     book.written_work.author m.05f834     book.written_work.subjects m.0322m     book.book.genre m.05hgj     ‚Ä¶ ‚Ä¶     (3) Relation detection     (1) Topic entity detection     Node candidates for entity linking. Relation-chain candidates for relation detection.     Figure 1: A typical workflow for KBQA. Given a question ‚ÄúWho wrote the book Beau Geste?‚Äù, the topic entity detection model first identifies a topic entity ‚ÄúBeau Geste‚Äù from the question. Then, the entity linking model links the topic entity into an entity node (m.04wxy8) in the KB. Finally, the relation book.written work.author is selected as the relation-chain leading to the final answer node (m.05f834).     these workarounds may prevent their methods from generalizing well to other datasets, and scaling up to bigger KBs.     To tackle these issues, we leverage a retrieve- and-rerank strategy to access KBs. In the retrieval step, we ingest KBs into two inverted indices: one that stores all entity nodes for entity linking, and the other one that stores all subject-predicate-object triples for relation detection. Then, we use TF-IDF algorithm to retrieve candidates for both entity link- ing and relation detection sub-tasks. This method naturally overcomes the memory overhead when dealing with large-scale KBs, therefore makes our method easily scale up to large-scale tasks. In the re-ranking step, we leverage the advanced BERT model to re-rank all candidates by fine-grained se- mantic matching. For the topic entity detection sub-task, we utilize another BERT model to predict the start and end positions of a topic entity within a question. Since assigning a different BERT model for each sub-task may incur prohibitive costs, we therefore propose to share a BERT encoder across sub-tasks and define task-specific layers for each individual sub-task on top of the shared layer. This unified BERT model is then trained under the multi- task learning framework. Experiments on two stan- dard benchmarks show that: (1) Our IR-based re- trieval method is able to collect high-quality candi- dates efficiently; (2) the BERT model improves the accuracy across all three sub-tasks; and (3) bene- fiting from multi-task learning, the unified model obtains further improvements with only 1/3 of the original parameters. Our final model achieves com- petitive results on the SimpleQuestions dataset and superior performance on the FreebaseQA dataset.     2 Task Definition     Knowledge-base question answering (KBQA) aims to find answers for natural language questions from structural knowledge bases (KB). We assume a KB K is a collection of subject-predicate-object triples „Äàe1, p, e2„Äâ, where e1, e2 ‚àà E are entities, and p ‚àà P is a relation type between two entities, E is the set of all entities, and P is the set of all relation types. Given a question Q, the goal of KBQA is to find an entity node a ‚àà E from the KB as the final answer, thus can be formulated as     aÃÇ = argmax a‚ààE     Pr(a|Q,K) (1)     where Pr(a|Q,K) is the probability of a to be the answer for Q. A general purpose KB usually contains millions of entities in E and billions of relations in K (Bollacker et al., 2008), therefore directly modeling Pr(a|Q,K) is challenging. Pre- vious studies usually factorize this model in dif- ferent ways. One line of research forms KBQA as a semantic parsing task Pr(q|Q,K) to parse a question Q directly into a logical form query q, and execute the query q over KB to derive the final answer. Another line of studies views KBQA as a semantic matching task, and finds a relation-chain within KB that is similar to the question in a com- mon semantic space. Then the trailing entity of the relation-chain is taken as the final answer.'],\n",
       " 'gt_answer': 'The three main sub-tasks in KBQA are topic entity detection, entity linking, and relation detection.',\n",
       " 'query_metadata': {'input_token': 0, 'latency_ms': 0},\n",
       " 'sagemaker_cost': {'sagemaker_inference_cost': 1.0147194444444445},\n",
       " 'response': {'message': 'The prediction matches the ground truth.',\n",
       "  'score': 1}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15768f7b-e1bb-43f4-a4e4-a736f074e372",
   "metadata": {},
   "source": [
    "## Save results to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "feb01294-86c6-4f9d-9744-57c3ab68dd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "csv_file = './results/evaluation_output.csv'\n",
    "\n",
    "# Check if 'sagemaker_cost' exists in any item\n",
    "include_sagemaker_cost = any('sagemaker_cost' in item for item in results)\n",
    "include_inference_cost = any('inference_cost' in item for item in results)\n",
    "\n",
    "fieldnames=['question', 'answer', 'inputTokens', 'outputTokens', 'totalTokens', 'latencyMs', 'ground answer','message','score']\n",
    "\n",
    "if include_sagemaker_cost:\n",
    "    fieldnames.insert(fieldnames.index('message'), 'sagemaker_cost')  # Insert before 'ground answer'\n",
    "\n",
    "if include_inference_cost:\n",
    "    fieldnames.insert(fieldnames.index('message'), 'bedrock_input_cost')  # Insert before 'ground answer'\n",
    "    fieldnames.insert(fieldnames.index('message'), 'bedrock_output_cost')  # Insert before 'ground answer'\n",
    "    \n",
    "\n",
    "with open(csv_file, mode='w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for _id, item in enumerate(results):\n",
    "        answer_metadata = item.get('answer_metadata', {})\n",
    "        response = item.get('response', {})\n",
    "\n",
    "        row = {\n",
    "            'question': item.get('question', ''),\n",
    "            'answer': item.get('answer', ''),\n",
    "            'inputTokens': answer_metadata.get('inputTokens', ''),\n",
    "            'outputTokens': answer_metadata.get('outputTokens', ''),\n",
    "            'totalTokens': answer_metadata.get('totalTokens', ''),\n",
    "            'latencyMs': answer_metadata.get('latencyMs', ''),\n",
    "            'ground answer': item.get('gt_answer', ''),\n",
    "            'message': response.get('message', ''),\n",
    "            'score': response.get('score', ''),\n",
    "        }\n",
    "\n",
    "        if include_sagemaker_cost:\n",
    "            sagemaker_cost = item.get('sagemaker_cost', {})\n",
    "            row['sagemaker_cost'] = sagemaker_cost.get('sagemaker_cost', '')\n",
    "        if include_inference_cost:\n",
    "            inference_cost = item.get('inference_cost', {})\n",
    "            row['bedrock_input_cost'] = inference_cost.get('inference_input_cost', '')\n",
    "            row['bedrock_output_cost'] = inference_cost.get('inference_output_cost', '')\n",
    "\n",
    "        writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18b928b-c5b3-41b5-a1ad-f6ce6f0a2dea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
