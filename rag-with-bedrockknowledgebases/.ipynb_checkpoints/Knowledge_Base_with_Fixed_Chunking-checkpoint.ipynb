{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aa74695-63c7-48a0-91b3-d3317035dd19",
   "metadata": {},
   "source": [
    "## Create a Knowledge Base with fixed chunking strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04e9f76",
   "metadata": {},
   "source": [
    "Chunking data is essential. If you are adding large documents with hundreds of pages to your knowledge base then you need to split them up and return only the relevant sections to use as context for your inference. If you are returning too much context it will increase costs (models charge based on input token count) and latency. It may also harm output quality. Shorter chunks will provide a better match but may lack the context necessary to answer a question.\n",
    "\n",
    "Bedrock Knowledge bases has a few different chunking strategies to choose from. They handle everything from splitting at semantic boundaries like paragraphs and hierarchical structures. However some document types can benefit from custom chunking. For example, any form of mark up can be used by a custom chunking approach.\n",
    "\n",
    "You can also create your own custom chunking approach using a Lambda function. If you want to add any custom metadata then you will need to add a Lambda function. You can either handle the chunking yourself, edit an existing chunk or just add metadata. Metadata can then be used for filtering.\n",
    "\n",
    "It is important to tune your chunking to the type of documents being ingested. Getting the wrong chunk size will affect the accuracy and response times. It will also increase the costs in both the vector storage and inference steps. The defaults supplied in Bedrock are pretty good but they may need tailored to your specific circumstances. Longer and more technical documents may need larger chunk sizes to make sure they include more context. Speech (like a chat transcript) can benefit from shorter chunks.\n",
    "\n",
    "![Chunking Strategies](./chunking-strategies.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e9c5e0",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this notebook, we will implement a knowledge base using a fixed chunking strategy. Here are the key steps we'll perform:\n",
    "\n",
    "1. **Create a Knowledge Base**: Set up an Amazon Bedrock Knowledge Base with fixed-size chunking configuration that will store and retrieve our vector embeddings.\n",
    "\n",
    "2. **Create a Data Source**: Connect our Knowledge Base to the documents we uploaded to S3 in the previous notebook.\n",
    "\n",
    "3. **Start Ingestion Job**: Begin the process of transforming our documents into chunks, creating embeddings, and storing them in our vector database.\n",
    "\n",
    "4. **Retrieve and Generate**: Test our Knowledge Base by retrieving relevant information based on a sample query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce312491-c1c5-4754-89bd-d28da5558005",
   "metadata": {},
   "source": [
    "#### Concept\n",
    "\n",
    "**Fixed Chunking**: Involves dividing your documents into fixed-size chunks, regardless of the content within them. Each chunk contains a predefined number of tokens or characters, and this method allows for more uniform data organization. \n",
    "\n",
    "Fixed chunking is useful when you want to ensure that your chunks are of a consistent size, making them easier to process and retrieve in a predictable manner. The document is split into sections of equal length, and each section becomes a separate chunk. This method works well when the content is relatively homogeneous, and the chunk boundaries are not as crucial to understanding the underlying context.\n",
    "\n",
    "#### Benefits\n",
    "\n",
    "- **Uniformity**: Each chunk has the same size, making the system more predictable. This helps with processing efficiency since you know that each chunk is of a consistent size, making batch operations and parallel processing easier.\n",
    "- **Simplified Retrieval**: Since the chunk sizes are uniform, searching through the data becomes straightforward. You can quickly determine the length of chunks, which can be useful for performance optimization and scalability in large datasets.\n",
    "- **Performance Optimization**: Fixed chunks are ideal when you want to control the computational cost of document retrieval and chunking. Having equal-sized chunks reduces the chance of computational bottlenecks in scenarios requiring large-scale document processing.\n",
    "\n",
    "> **Note:** While fixed chunking can be efficient for certain use cases, it may not preserve the natural semantic boundaries of the content, such as paragraphs or sections. This may lead to chunks that start or end at arbitrary places, potentially cutting off context in the middle of a sentence or idea.\n",
    "\n",
    "### **Best Use Cases**\n",
    "Fixed chunking is suitable for cases where:\n",
    "- **Homogeneous content**: The content is consistent, and boundaries are not as important.\n",
    "- **Performance**: You need uniform-sized chunks for predictable processing or optimization of large-scale systems.\n",
    "- **Simplified text processing**: When chunk boundaries do not need to match natural semantic structures like paragraphs or sentences.\n",
    "\n",
    "Examples include:\n",
    "- **General document indexing**: When large datasets are involved, and uniform chunk sizes optimize retrieval.\n",
    "- **Text summarization**: Fixed chunking is helpful when generating summaries from uniformly sized data pieces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b18e06c-98ef-41bc-99fd-d5e6c8977e4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accountNumber': '677276078734',\n",
       " 'regionName': 'us-east-1',\n",
       " 'collectionArn': 'arn:aws:aoss:us-east-1:677276078734:collection/h4x23xd1thd0kpl13b67',\n",
       " 'collectionId': 'h4x23xd1thd0kpl13b67',\n",
       " 'vectorIndexName': 'ws-index-fixed',\n",
       " 'bedrockExecutionRoleArn': 'arn:aws:iam::677276078734:role/advanced-rag-workshop-bedrock_execution_role-us-east-1',\n",
       " 's3Bucket': '677276078734-us-east-1-advanced-rag-workshop',\n",
       " 's3_ground_truth_path': 's3://677276078734-us-east-1-advanced-rag-workshop/data/ground_truth_data/kbqa_questions_answers.json'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open(\"./results/variables.json\", \"r\") as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbaee1f0-65f2-4f23-80f4-767dcf78a459",
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_base_name = \"advanced-rag-kbs_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc663d8e-4c14-46a9-ba91-309cf17947bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49ceaa5e-2ddd-4b97-853f-527c92326ebd",
   "metadata": {},
   "source": [
    "### 1. Create a Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9933328d-76dc-44bb-a985-591a61c47fca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from retrying import retry\n",
    "import boto3\n",
    "\n",
    "# Initialize the Bedrock Agent client using the provided AWS region\n",
    "bedrock_agent = boto3.client(\"bedrock-agent\", region_name=variables[\"regionName\"])\n",
    "\n",
    "# Retry decorator: If the function fails, it will retry up to 3 times with a random wait time between 1-2 seconds\n",
    "@retry(wait_random_min=1000, wait_random_max=2000, stop_max_attempt_number=3)\n",
    "def create_knowledge_base_func(name, description, chunking_type):\n",
    "    \"\"\"\n",
    "    Creates a knowledge base in Amazon Bedrock with OpenSearch Serverless as the vector store.\n",
    "    \n",
    "    Parameters:\n",
    "        name (str): The name of the knowledge base.\n",
    "        description (str): A description of the knowledge base.\n",
    "        chunking_type (str): The type of chunking strategy applied to vector indexing.\n",
    "\n",
    "    Returns:\n",
    "        dict: The response containing details of the created knowledge base.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the ARN of the embedding model used for vectorization\n",
    "    embedding_model_arn = f\"arn:aws:bedrock:{variables['regionName']}::foundation-model/amazon.titan-embed-text-v2:0\"\n",
    "\n",
    "    # Configure OpenSearch Serverless for vector storage\n",
    "    opensearch_serverless_configuration = {\n",
    "        \"collectionArn\": variables[\"collectionArn\"],  # ARN of the OpenSearch collection\n",
    "        # \"vectorIndexName\": variables[\"vectorIndexName\"] + chunking_type,  # Index name based on chunking strategy\n",
    "        \"vectorIndexName\": variables[\"vectorIndexName\"],  # Index name based on chunking strategy\n",
    "        \"fieldMapping\": {  # Define field mappings for vectors, text, and metadata\n",
    "            \"vectorField\": \"vector\",\n",
    "            \"textField\": \"text\",\n",
    "            \"metadataField\": \"text-metadata\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    print(opensearch_serverless_configuration)  # Print configuration for debugging\n",
    "\n",
    "    # Create the knowledge base in Amazon Bedrock\n",
    "    create_kb_response = bedrock_agent.create_knowledge_base(\n",
    "        name=name,\n",
    "        description=description,\n",
    "        roleArn=variables[\"bedrockExecutionRoleArn\"],  # IAM Role ARN for Bedrock execution\n",
    "        knowledgeBaseConfiguration={\n",
    "            \"type\": \"VECTOR\",\n",
    "            \"vectorKnowledgeBaseConfiguration\": {\n",
    "                \"embeddingModelArn\": embedding_model_arn  # Reference to the embedding model\n",
    "            }\n",
    "        },\n",
    "        storageConfiguration={\n",
    "            \"type\": \"OPENSEARCH_SERVERLESS\",\n",
    "            \"opensearchServerlessConfiguration\": opensearch_serverless_configuration\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return create_kb_response[\"knowledgeBase\"]  # Return the created knowledge base details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb862cb-81b5-48e6-88d3-8d1cce9c6f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68035c04-af12-4ae4-a48d-2e6391bb662f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'collectionArn': 'arn:aws:aoss:us-east-1:677276078734:collection/h4x23xd1thd0kpl13b67', 'vectorIndexName': 'ws-index-fixed', 'fieldMapping': {'vectorField': 'vector', 'textField': 'text', 'metadataField': 'text-metadata'}}\n",
      "OpenSearch Knowledge Response: {\n",
      "    \"ResponseMetadata\": {\n",
      "        \"RequestId\": \"5e1f30b8-815b-412f-9511-741735d73404\",\n",
      "        \"HTTPStatusCode\": 200,\n",
      "        \"HTTPHeaders\": {\n",
      "            \"date\": \"Tue, 15 Apr 2025 05:50:15 GMT\",\n",
      "            \"content-type\": \"application/json\",\n",
      "            \"content-length\": \"940\",\n",
      "            \"connection\": \"keep-alive\",\n",
      "            \"x-amzn-requestid\": \"5e1f30b8-815b-412f-9511-741735d73404\",\n",
      "            \"x-amz-apigw-id\": \"JDL_yFbrIAMEGbw=\",\n",
      "            \"x-amzn-trace-id\": \"Root=1-67fdf397-457d16927a023d24440d44c3\"\n",
      "        },\n",
      "        \"RetryAttempts\": 0\n",
      "    },\n",
      "    \"knowledgeBase\": {\n",
      "        \"createdAt\": \"2025-04-15 05:50:15.208159+00:00\",\n",
      "        \"description\": \"Knowledge base using Amazon OpenSearch Service as a vector store\",\n",
      "        \"knowledgeBaseArn\": \"arn:aws:bedrock:us-east-1:677276078734:knowledge-base/TJSZIWHAIM\",\n",
      "        \"knowledgeBaseConfiguration\": {\n",
      "            \"type\": \"VECTOR\",\n",
      "            \"vectorKnowledgeBaseConfiguration\": {\n",
      "                \"embeddingModelArn\": \"arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-text-v2:0\"\n",
      "            }\n",
      "        },\n",
      "        \"knowledgeBaseId\": \"TJSZIWHAIM\",\n",
      "        \"name\": \"advanced-rag-kbs_1_2\",\n",
      "        \"roleArn\": \"arn:aws:iam::677276078734:role/advanced-rag-workshop-bedrock_execution_role-us-east-1\",\n",
      "        \"status\": \"CREATING\",\n",
      "        \"storageConfiguration\": {\n",
      "            \"opensearchServerlessConfiguration\": {\n",
      "                \"collectionArn\": \"arn:aws:aoss:us-east-1:677276078734:collection/h4x23xd1thd0kpl13b67\",\n",
      "                \"fieldMapping\": {\n",
      "                    \"metadataField\": \"text-metadata\",\n",
      "                    \"textField\": \"text\",\n",
      "                    \"vectorField\": \"vector\"\n",
      "                },\n",
      "                \"vectorIndexName\": \"ws-index-fixed\"\n",
      "            },\n",
      "            \"type\": \"OPENSEARCH_SERVERLESS\"\n",
      "        },\n",
      "        \"updatedAt\": \"2025-04-15 05:50:15.208159+00:00\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "try:\n",
    "    # Create a knowledge base using the predefined function\n",
    "    kb = create_knowledge_base_func(\n",
    "        name=knowledge_base_name+\"_2\",\n",
    "        description=\"Knowledge base using Amazon OpenSearch Service as a vector store\",\n",
    "        chunking_type=\"fixed\"\n",
    "    )\n",
    "\n",
    "    # Retrieve details of the newly created knowledge base\n",
    "    get_kb_response = bedrock_agent.get_knowledge_base(knowledgeBaseId=kb['knowledgeBaseId'])\n",
    "\n",
    "    # Update the variables dictionary with the new knowledge base ID\n",
    "    variables[\"kbFixedChunk\"] = kb['knowledgeBaseId']\n",
    "\n",
    "    # Save updated variables to a JSON file, handling datetime serialization\n",
    "    with open(\"variables.json\", \"w\") as f:\n",
    "        json.dump(variables, f, indent=4, default=str)  # Convert datetime to string\n",
    "\n",
    "    # Print the retrieved knowledge base response in a readable format\n",
    "    print(f'OpenSearch Knowledge Response: {json.dumps(get_kb_response, indent=4, default=str)}')\n",
    "    \n",
    "# except Exception as e:\n",
    "#     # Check if error message indicates the knowledge base already exists\n",
    "#     error_message = str(e).lower()\n",
    "#     if any(phrase in error_message for phrase in [\"already exist\", \"duplicate\", \"already been created\"]):\n",
    "#         print(\"Knowledge Base already exist\")\n",
    "#     else:\n",
    "#         # If it's a different error, re-raise it\n",
    "#         raise e\n",
    "\n",
    "except Exception as e:\n",
    "    # Check if error message indicates the knowledge base already exists\n",
    "    error_message = str(e).lower()\n",
    "    if any(phrase in error_message for phrase in [\"already exist\", \"duplicate\", \"already been created\"]):\n",
    "        print(\"Knowledge Base already exists. Retrieving its ID...\")\n",
    "        \n",
    "        # List all knowledge bases to find the one that already exists\n",
    "        list_kb_response = bedrock_agent.list_knowledge_bases()\n",
    "        \n",
    "        # Look for a knowledge base with the desired name\n",
    "        for kb in list_kb_response.get('knowledgeBaseSummaries', []):\n",
    "            if kb['name'] == knowledge_base_name:\n",
    "                kb_id = kb['knowledgeBaseId']\n",
    "                print(f\"Found existing knowledge base with ID: {kb_id}\")\n",
    "                \n",
    "                # Get the details of the existing knowledge base\n",
    "                get_kb_response = bedrock_agent.get_knowledge_base(knowledgeBaseId=kb_id)\n",
    "                \n",
    "                # With this code that reads existing values first:\n",
    "                try:\n",
    "                    # Read existing variables\n",
    "                    with open(\"variables.json\", \"r\") as f:\n",
    "                        existing_variables = json.load(f)\n",
    "                except (FileNotFoundError, json.JSONDecodeError):\n",
    "                    # If file doesn't exist or is invalid JSON\n",
    "                    existing_variables = {}\n",
    "                \n",
    "                # Update only the semantic chunking value\n",
    "                existing_variables[\"kbFixedChunk\"] = kb_id\n",
    "                                \n",
    "                # Write back all variables\n",
    "                with open(\"variables.json\", \"w\") as f:\n",
    "                    json.dump(existing_variables, f, indent=4, default=str)\n",
    "                \n",
    "                # Print the retrieved knowledge base response\n",
    "                print(f'OpenSearch Knowledge Response: {json.dumps(get_kb_response, indent=4, default=str)}')\n",
    "                break\n",
    "        else:\n",
    "            print(\"Could not find a knowledge base with the specified name.\")\n",
    "    else:\n",
    "        # If it's a different error, re-raise it\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d6a826-f3c8-40b5-aaa2-0c2a9662d5dc",
   "metadata": {},
   "source": [
    "### 2. Create Datasources for Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb183c15-662d-498e-9df2-6814560f471a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing data source 'advanced-rag-example'. Deleting it...\n",
      "Waiting for data source deletion to complete...\n",
      "Data source deleted successfully.\n",
      "Creating new data source 'advanced-rag-example'...\n",
      "Data source created successfully.\n",
      "IX3MATTUHN\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Define the chunking strategy for data ingestion\n",
    "chunking_strategy_configuration = {\n",
    "    \"chunkingStrategy\": \"FIXED_SIZE\",\n",
    "    \"fixedSizeChunkingConfiguration\": {\n",
    "        \"maxTokens\": 1024,\n",
    "        \"overlapPercentage\": 20\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define the S3 bucket configuration for the data source\n",
    "s3_configuration = {\n",
    "    \"bucketArn\": f\"arn:aws:s3:::{variables['s3Bucket']}\",\n",
    "    \"inclusionPrefixes\": [\"data\"]  # Only include objects with the \"data\" prefix\n",
    "}\n",
    "\n",
    "data_source_name = \"advanced-rag-example\"\n",
    "\n",
    "# First, check if a data source with this name already exists in Bedrock (not just locally)\n",
    "try:\n",
    "    # List all data sources for the knowledge base\n",
    "    list_ds_response = bedrock_agent.list_data_sources(\n",
    "        knowledgeBaseId=kb['knowledgeBaseId']\n",
    "    )\n",
    "    \n",
    "    # Check if our named data source exists\n",
    "    existing_ds = None\n",
    "    for ds in list_ds_response.get('dataSourceSummaries', []):\n",
    "        if ds['name'] == data_source_name:\n",
    "            existing_ds = ds\n",
    "            break\n",
    "    \n",
    "    # If it exists, delete it\n",
    "    if existing_ds:\n",
    "        print(f\"Found existing data source '{data_source_name}'. Deleting it...\")\n",
    "        bedrock_agent.delete_data_source(\n",
    "            knowledgeBaseId=kb['knowledgeBaseId'],\n",
    "            dataSourceId=existing_ds[\"dataSourceId\"]\n",
    "        )\n",
    "        print(\"Waiting for data source deletion to complete...\")\n",
    "        time.sleep(10)\n",
    "        print(\"Data source deleted successfully.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error while checking or deleting data source: {e}\")\n",
    "\n",
    "# Now create a new data source\n",
    "try:\n",
    "    print(f\"Creating new data source '{data_source_name}'...\")\n",
    "    create_ds_response = bedrock_agent.create_data_source(\n",
    "        name=data_source_name,\n",
    "        description=\"A data source for Advanced RAG workshop\",\n",
    "        knowledgeBaseId=kb['knowledgeBaseId'],\n",
    "        dataSourceConfiguration={\n",
    "            \"type\": \"S3\",\n",
    "            \"s3Configuration\": s3_configuration\n",
    "        },\n",
    "        vectorIngestionConfiguration={\n",
    "            \"chunkingConfiguration\": chunking_strategy_configuration\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Store the created data source object\n",
    "    ds_fixed_chunk = create_ds_response[\"dataSource\"]\n",
    "    print(f\"Data source created successfully.\")\n",
    "    \n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'ConflictException':\n",
    "        print(f\"Data source '{data_source_name}' still exists. Retrieving it...\")\n",
    "        # Get the existing data source\n",
    "        list_ds_response = bedrock_agent.list_data_sources(\n",
    "            knowledgeBaseId=kb['knowledgeBaseId']\n",
    "        )\n",
    "        for ds in list_ds_response.get('dataSourceSummaries', []):\n",
    "            if ds['name'] == data_source_name:\n",
    "                ds_fixed_chunk = ds\n",
    "                print(f\"Retrieved existing data source: {ds['dataSourceId']}\")\n",
    "                break\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "# Print the data source information\n",
    "print(ds_fixed_chunk[\"dataSourceId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "571b0992-ff5b-47b9-a4d8-8b4dbf825036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'41TAX6HEX9'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb['knowledgeBaseId']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322c1ddb-cfdc-4c54-a688-033e6967073b",
   "metadata": {},
   "source": [
    "### 3. Start Ingestion Job for Amazon Bedrock Knowledge base pointing to Amazon OpenSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b97c8d9",
   "metadata": {},
   "source": [
    "> **Note**: The ingestion process will take approximately 2-3 minutes to complete. During this time, the system is processing your documents by:\n",
    "> 1. Extracting text from the source files\n",
    "> 2. Chunking the content according to the defined strategy (Fixed / Semantic / Hierachical / Custom)\n",
    "> 3. Generating embeddings for each chunk\n",
    "> 4. Storing the embeddings and associated metadata in the OpenSearch vector database\n",
    ">\n",
    "> You'll see status updates as the process progresses. Please wait for the \"Ingestion job completed successfully\" message before proceeding to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49134993-3ce8-4da2-a5a5-9917c974c743",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion job started successfully.\n",
      "running...\n",
      "Job status: IN_PROGRESS\n",
      "running...\n",
      "Job status: IN_PROGRESS\n",
      "running...\n",
      "Job status: COMPLETE\n",
      "Ingestion job completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# List to keep track of all ingestion jobs\n",
    "ingest_jobs = []\n",
    "\n",
    "# Start an ingestion job for the data source\n",
    "try:\n",
    "    start_job_response = bedrock_agent.start_ingestion_job(\n",
    "        knowledgeBaseId=kb['knowledgeBaseId'],  # ID of the Knowledge Base\n",
    "        dataSourceId=ds_fixed_chunk[\"dataSourceId\"]  # ID of the associated data source\n",
    "    )\n",
    "    \n",
    "    # Extract job details\n",
    "    job = start_job_response[\"ingestionJob\"]\n",
    "    print(\"Ingestion job started successfully.\")\n",
    "\n",
    "    # Polling mechanism to check job status until it is complete\n",
    "    while job['status'] != 'COMPLETE':\n",
    "        # Sleep for a brief period to ensure the job is fully completed\n",
    "        print(\"running...\")\n",
    "        time.sleep(10)\n",
    "        get_job_response = bedrock_agent.get_ingestion_job(\n",
    "            knowledgeBaseId=kb['knowledgeBaseId'],  # ID of the Knowledge Base\n",
    "            dataSourceId=ds_fixed_chunk[\"dataSourceId\"],  # ID of the data source\n",
    "            ingestionJobId=job[\"ingestionJobId\"]  # ID of the running ingestion job\n",
    "        )\n",
    "        \n",
    "        # Update job status\n",
    "        job = get_job_response[\"ingestionJob\"]\n",
    "        print(f\"Job status: {job['status']}\")  # Log the current job status\n",
    "\n",
    "    print(\"Ingestion job completed successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error: Couldn't start ingestion job.\")\n",
    "    print(e)  # Print the exact error message for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5e6de5d-4274-4dbf-8a65-aeef5063229f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataSourceId': 'IX3MATTUHN',\n",
       " 'ingestionJobId': 'YCFSSURZQP',\n",
       " 'knowledgeBaseId': 'TJSZIWHAIM',\n",
       " 'startedAt': datetime.datetime(2025, 4, 15, 5, 55, 34, 375056, tzinfo=tzutc()),\n",
       " 'statistics': {'numberOfDocumentsDeleted': 0,\n",
       "  'numberOfDocumentsFailed': 0,\n",
       "  'numberOfDocumentsScanned': 8,\n",
       "  'numberOfMetadataDocumentsModified': 0,\n",
       "  'numberOfMetadataDocumentsScanned': 7,\n",
       "  'numberOfModifiedDocumentsIndexed': 0,\n",
       "  'numberOfNewDocumentsIndexed': 8},\n",
       " 'status': 'COMPLETE',\n",
       " 'updatedAt': datetime.datetime(2025, 4, 15, 5, 55, 56, 385587, tzinfo=tzutc())}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec8b006-b8af-46d9-b869-77fb08a00ee0",
   "metadata": {},
   "source": [
    "### 4. Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4256881d-7737-4d21-a007-7959b22e8fc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully retrieved relevant documents.\n",
      "[{'content': {'text': 'Retrieval, Re-ranking and Multi-task Learning for Knowledge-Base Question Answering     Zhiguo Wang, Patrick Ng, Ramesh Nallapati, Bing Xiang AWS AI Labs     {zhiguow, patricng, rnallapa, bxiang}@amazon.com     Abstract     Question answering over knowledge bases (KBQA) usually involves three sub-tasks, namely topic entity detection, entity linking and relation detection. Due to the large num- ber of entities and relations inside knowledge bases (KB), previous work usually utilized so- phisticated rules to narrow down the search space and managed only a subset of KBs in memory. In this work, we leverage a retrieve- and-rerank framework to access KBs via tradi- tional information retrieval (IR) method, and re-rank retrieved candidates with more pow- erful neural networks such as the pre-trained BERT model. Considering the fact that di- rectly assigning a different BERT model for each sub-task may incur prohibitive costs, we propose to share a BERT encoder across all three sub-tasks and define task-specific layers on top of the shared layer. The unified model is then trained under a multi-task learning frame- work. Experiments show that: (1) Our IR- based retrieval method is able to collect high- quality candidates efficiently, thus enables our method adapt to large-scale KBs easily; (2) the BERT model improves the accuracy across all three sub-tasks; and (3) benefiting from multi- task learning, the unified model obtains fur- ther improvements with only 1/3 of the origi- nal parameters. Our final model achieves com- petitive results on the SimpleQuestions dataset and superior performance on the FreebaseQA dataset.     1 Introduction     Answering natural language questions by search- ing over large-scale knowledge bases (KBQA) is highly demanded by real-life applications, such as Google Assistant, Siri, and Alexa. Owing to the availability of large-scale KBs, significant ad- vancements have been made over the years. One main research direction views KBQA as a seman- tic matching task (Bordes et al., 2014; Dong et al.,     2015; Dai et al., 2016; Hao et al., 2017; Mohammed et al., 2018; Yu et al., 2018; Wu et al., 2019; Chen et al., 2019a; Petrochuk and Zettlemoyer, 2018), and finds a relation-chain within KBs that is most similar to the question in a common semantic space, where the relation-chain can be 1-hop, 2-hop or multi-hop (Chen et al., 2019b). Another research direction formulates KBQA as a semantic parsing task (Berant et al., 2013; Bao et al., 2016; Luo et al., 2018), and tackles questions that involve complex reasoning, such as ordinal (e.g. What is the sec- ond largest fulfillment center of Amazon?), and aggregation (e.g. How many fulfillment centers does Amazon have?). Most recently, some stud- ies proposed to derive answers from both KBs and free-text corpus to deal with the low-coverage issue of KBs (Xu et al., 2016; Sun et al., 2018; Xiong et al., 2019; Sun et al., 2019). In this paper, we follow the first research direction since the relation- chain type of questions counts the vast majority of real-life questions (Berant et al., 2013; Bordes et al., 2015; Jiang et al., 2019).     Previous semantic matching methods for KBQA usually decompose the task into sequential sub- tasks consisting of topic entity detection, entity linking, and relation detection. For example in Fig- ure 1, given the question “Who wrote the book Beau Geste?”, a KBQA system first identifies the topic entity “Beau Geste” from the question, then the topic entity is linked to an entity node (m.04wxy8) from a list of candidate nodes, and finally the relation book.written work.author is se- lected as the relation-chain leading to the final an- swer. Previous methods usually worked on a subset of KB in order to fit KB into memory. For entity linking, some sophisticated heuristics were com- monly used to collect entity candidates. For rela- tion detection, previous work usually enumerated all possible 1-hop and 2-hop relation-chains (start- ing from linked entity nodes) as candidates. AllWho wrote the book Beau Geste ?', 'type': 'TEXT'}, 'location': {'s3Location': {'uri': 's3://677276078734-us-east-1-advanced-rag-workshop/data/pdf_documents/retrieval-reranking-and-multi-task-learning-for-knowledge-base-question-answering.pdf'}, 'type': 'S3'}, 'metadata': {'x-amz-bedrock-kb-source-uri': 's3://677276078734-us-east-1-advanced-rag-workshop/data/pdf_documents/retrieval-reranking-and-multi-task-learning-for-knowledge-base-question-answering.pdf', 'x-amz-bedrock-kb-document-page-number': 1.0, 'year': 2021.0, 'docType': 'science', 'x-amz-bedrock-kb-data-source-id': 'IX3MATTUHN', 'company': 'Amazon', 'x-amz-bedrock-kb-chunk-id': '1%3A0%3A-FkEOJYBUFU5pE7mrhOT', 'authors': ['Zhiguo Wang', 'Patrick Ng', 'Ramesh Nallapati', 'Bing Xiang']}, 'score': 0.5401888}, {'content': {'text': '[   {     \"question\": \"What are the three main sub-tasks in Knowledge Base Question Answering (KBQA) as identified in the paper?\",     \"answer\": \"The three main sub-tasks in KBQA are topic entity detection, entity linking, and relation detection.\"   },   {     \"question\": \"How does the proposed method handle large-scale knowledge bases efficiently?\",     \"answer\": \"The method uses an IR-based retrieval approach to collect high-quality candidates efficiently, enabling adaptation to large-scale KBs.\"   },   {     \"question\": \"What is the role of BERT in the proposed KBQA framework?\",     \"answer\": \"BERT is used to improve accuracy across all three sub-tasks by serving as a shared encoder in the multi-task learning framework.\"   },   {     \"question\": \"How does multi-task learning benefit the proposed KBQA model?\",     \"answer\": \"Multi-task learning allows the unified model to achieve further improvements with only one-third of the original parameters.\"   },   {     \"question\": \"On which datasets did the proposed model achieve competitive or superior performance?\",     \"answer\": \"The model achieved competitive results on the SimpleQuestions dataset and superior performance on the FreebaseQA dataset.\"   },   {     \"question\": \"What is the primary motivation behind creating the PRACTIQ dataset?\",     \"answer\": \"The PRACTIQ dataset was created to address the limitations of existing text-to-SQL datasets, which primarily focus on clear, answerable user queries. Real-world user questions are often ambiguous or unanswerable due to various factors, and PRACTIQ aims to provide a practical dataset that includes such complexities to better train and evaluate conversational text-to-SQL systems.\"   },   {     \"question\": \"How are ambiguous and unanswerable questions defined in the PRACTIQ dataset?\",     \"answer\": \"In the PRACTIQ dataset, a question is considered ambiguous if it has multiple valid interpretations given the database schema. A question is deemed unanswerable if the corresponding database does not contain the data that the question is asking for.\"   },   {     \"question\": \"What methodology was used to construct conversations in the PRACTIQ dataset?\",     \"answer\": \"Conversations in the PRACTIQ dataset are constructed with four turns: the initial user question, an assistant response seeking clarification, the user\\'s clarification, and the assistant\\'s clarified SQL response along with a natural language explanation of the execution results.\"   },   {     \"question\": \"What are the four categories of ambiguous questions identified in the PRACTIQ dataset?\",     \"answer\": \"The four categories of ambiguous questions identified in the PRACTIQ dataset are: (1) Ambiguity due to multiple columns with similar names, (2) Ambiguity due to multiple tables containing similar information, (3) Ambiguity arising from unspecified aggregation operations, and (4) Ambiguity caused by vague temporal references.\"   },   {     \"question\": \"What are the four categories of unanswerable questions identified in the PRACTIQ dataset?\",     \"answer\": \"The four categories of unanswerable questions identified in the PRACTIQ dataset are: (1) Questions about non-existent entities, (2) Questions requiring external knowledge not present in the database, (3) Questions involving data that is missing or incomplete, and (4) Questions that are logically inconsistent or contradictory.\"   },   {     \"question\": \"How does the PRACTIQ dataset handle ambiguous queries without seeking user clarification?\",     \"answer\": \"For some ambiguous queries, the PRACTIQ dataset includes helpful SQL responses that consider multiple aspects of ambiguity, providing direct answers without requesting user clarification.\"   },   {     \"question\": \"What approach was used to benchmark performance on the PRACTIQ dataset?\",     \"answer\": \"To benchmark performance on the PRACTIQ dataset, the authors implemented large language model (LLM)-based baselines using various LLMs. Their approach involves two steps: question category classification and clarification SQL prediction.\"   },   {     \"question\": \"What were the findings regarding state-of-the-art systems\\' performance on ambiguous and unanswerable questions?\",     \"answer\": \"The experiments revealed that state-of-the-art systems struggle to handle ambiguous and unanswerable questions effectively, highlighting the need for datasets like PRACTIQ to improve system robustness.\"   },   {     \"question\": \"Is the PRACTIQ dataset publicly available for research purposes?\",     \"answer\": \"Yes, the authors have indicated that they will release the code for data generation and experiments on GitHub to facilitate further research in this area.\"   },   {     \"question\": \"What is the significance of the PRACTIQ dataset in the context of real-world applications?\"', 'type': 'TEXT'}, 'location': {'s3Location': {'uri': 's3://677276078734-us-east-1-advanced-rag-workshop/data/ground_truth_data/kbqa_questions_answers.json'}, 'type': 'S3'}, 'metadata': {'x-amz-bedrock-kb-source-uri': 's3://677276078734-us-east-1-advanced-rag-workshop/data/ground_truth_data/kbqa_questions_answers.json', 'x-amz-bedrock-kb-chunk-id': '1%3A0%3AB1kEOJYBUFU5pE7msRQ-', 'x-amz-bedrock-kb-data-source-id': 'IX3MATTUHN'}, 'score': 0.5273623}, {'content': {'text': 'Previous methods usually worked on a subset of KB in order to fit KB into memory. For entity linking, some sophisticated heuristics were com- monly used to collect entity candidates. For rela- tion detection, previous work usually enumerated all possible 1-hop and 2-hop relation-chains (start- ing from linked entity nodes) as candidates. AllWho wrote the book Beau Geste ?     MID name type     m.04wxy8 Beau Geste book     m.0dl_h4 Beau Geste film     m.051vvdc Beau Geste music     … … …     (2) Entity linking     predicate object MID     book.written_work.author m.05f834     book.written_work.subjects m.0322m     book.book.genre m.05hgj     … …     (3) Relation detection     (1) Topic entity detection     Node candidates for entity linking. Relation-chain candidates for relation detection.     Figure 1: A typical workflow for KBQA. Given a question “Who wrote the book Beau Geste?”, the topic entity detection model first identifies a topic entity “Beau Geste” from the question. Then, the entity linking model links the topic entity into an entity node (m.04wxy8) in the KB. Finally, the relation book.written work.author is selected as the relation-chain leading to the final answer node (m.05f834).     these workarounds may prevent their methods from generalizing well to other datasets, and scaling up to bigger KBs.     To tackle these issues, we leverage a retrieve- and-rerank strategy to access KBs. In the retrieval step, we ingest KBs into two inverted indices: one that stores all entity nodes for entity linking, and the other one that stores all subject-predicate-object triples for relation detection. Then, we use TF-IDF algorithm to retrieve candidates for both entity link- ing and relation detection sub-tasks. This method naturally overcomes the memory overhead when dealing with large-scale KBs, therefore makes our method easily scale up to large-scale tasks. In the re-ranking step, we leverage the advanced BERT model to re-rank all candidates by fine-grained se- mantic matching. For the topic entity detection sub-task, we utilize another BERT model to predict the start and end positions of a topic entity within a question. Since assigning a different BERT model for each sub-task may incur prohibitive costs, we therefore propose to share a BERT encoder across sub-tasks and define task-specific layers for each individual sub-task on top of the shared layer. This unified BERT model is then trained under the multi- task learning framework. Experiments on two stan- dard benchmarks show that: (1) Our IR-based re- trieval method is able to collect high-quality candi- dates efficiently; (2) the BERT model improves the accuracy across all three sub-tasks; and (3) bene- fiting from multi-task learning, the unified model obtains further improvements with only 1/3 of the original parameters. Our final model achieves com- petitive results on the SimpleQuestions dataset and superior performance on the FreebaseQA dataset.     2 Task Definition     Knowledge-base question answering (KBQA) aims to find answers for natural language questions from structural knowledge bases (KB). We assume a KB K is a collection of subject-predicate-object triples 〈e1, p, e2〉, where e1, e2 ∈ E are entities, and p ∈ P is a relation type between two entities, E is the set of all entities, and P is the set of all relation types. Given a question Q, the goal of KBQA is to find an entity node a ∈ E from the KB as the final answer, thus can be formulated as     â = argmax a∈E     Pr(a|Q,K) (1)     where Pr(a|Q,K) is the probability of a to be the answer for Q. A general purpose KB usually contains millions of entities in E and billions of relations in K (Bollacker et al., 2008), therefore directly modeling Pr(a|Q,K) is challenging. Pre- vious studies usually factorize this model in dif- ferent ways. One line of research forms KBQA as a semantic parsing task Pr(q|Q,K) to parse a question Q directly into a logical form query q, and execute the query q over KB to derive the final answer. Another line of studies views KBQA as a semantic matching task, and finds a relation-chain within KB that is similar to the question in a com- mon semantic space. Then the trailing entity of the relation-chain is taken as the final answer.', 'type': 'TEXT'}, 'location': {'s3Location': {'uri': 's3://677276078734-us-east-1-advanced-rag-workshop/data/pdf_documents/retrieval-reranking-and-multi-task-learning-for-knowledge-base-question-answering.pdf'}, 'type': 'S3'}, 'metadata': {'x-amz-bedrock-kb-source-uri': 's3://677276078734-us-east-1-advanced-rag-workshop/data/pdf_documents/retrieval-reranking-and-multi-task-learning-for-knowledge-base-question-answering.pdf', 'x-amz-bedrock-kb-document-page-number': 1.0, 'year': 2021.0, 'docType': 'science', 'x-amz-bedrock-kb-data-source-id': 'IX3MATTUHN', 'company': 'Amazon', 'x-amz-bedrock-kb-chunk-id': '1%3A0%3A-VkEOJYBUFU5pE7mrhOT', 'authors': ['Zhiguo Wang', 'Patrick Ng', 'Ramesh Nallapati', 'Bing Xiang']}, 'score': 0.5141089}]\n",
      "\"Retrieval, Re-ranking and Multi-task Learning for Knowledge-Base Question Answering     Zhiguo Wang, Patrick Ng, Ramesh Nallapati, Bing Xiang AWS AI Labs     {zhiguow, patricng, rnallapa, bxiang}@amazon.com     Abstract     Question answering over knowledge bases (KBQA) usually involves three sub-tasks, namely topic entity detection, entity linking and relation detection. Due to the large num- ber of entities and relations inside knowledge bases (KB), previous work usually utilized so- phisticated rules to narrow down the search space and managed only a subset of KBs in memory. In this work, we leverage a retrieve- and-rerank framework to access KBs via tradi- tional information retrieval (IR) method, and re-rank retrieved candidates with more pow- erful neural networks such as the pre-trained BERT model. Considering the fact that di- rectly assigning a different BERT model for each sub-task may incur prohibitive costs, we propose to share a BERT encoder across all three sub-tasks and define task-specific layers on top of the shared layer. The unified model is then trained under a multi-task learning frame- work. Experiments show that: (1) Our IR- based retrieval method is able to collect high- quality candidates efficiently, thus enables our method adapt to large-scale KBs easily; (2) the BERT model improves the accuracy across all three sub-tasks; and (3) benefiting from multi- task learning, the unified model obtains fur- ther improvements with only 1/3 of the origi- nal parameters. Our final model achieves com- petitive results on the SimpleQuestions dataset and superior performance on the FreebaseQA dataset.     1 Introduction     Answering natural language questions by search- ing over large-scale knowledge bases (KBQA) is highly demanded by real-life applications, such as Google Assistant, Siri, and Alexa. Owing to the availability of large-scale KBs, significant ad- vancements have been made over the years. One main research direction views KBQA as a seman- tic matching task (Bordes et al., 2014; Dong et al.,     2015; Dai et al., 2016; Hao et al., 2017; Mohammed et al., 2018; Yu et al., 2018; Wu et al., 2019; Chen et al., 2019a; Petrochuk and Zettlemoyer, 2018), and finds a relation-chain within KBs that is most similar to the question in a common semantic space, where the relation-chain can be 1-hop, 2-hop or multi-hop (Chen et al., 2019b). Another research direction formulates KBQA as a semantic parsing task (Berant et al., 2013; Bao et al., 2016; Luo et al., 2018), and tackles questions that involve complex reasoning, such as ordinal (e.g. What is the sec- ond largest fulfillment center of Amazon?), and aggregation (e.g. How many fulfillment centers does Amazon have?). Most recently, some stud- ies proposed to derive answers from both KBs and free-text corpus to deal with the low-coverage issue of KBs (Xu et al., 2016; Sun et al., 2018; Xiong et al., 2019; Sun et al., 2019). In this paper, we follow the first research direction since the relation- chain type of questions counts the vast majority of real-life questions (Berant et al., 2013; Bordes et al., 2015; Jiang et al., 2019).     Previous semantic matching methods for KBQA usually decompose the task into sequential sub- tasks consisting of topic entity detection, entity linking, and relation detection. For example in Fig- ure 1, given the question \\u201cWho wrote the book Beau Geste?\\u201d, a KBQA system first identifies the topic entity \\u201cBeau Geste\\u201d from the question, then the topic entity is linked to an entity node (m.04wxy8) from a list of candidate nodes, and finally the relation book.written work.author is se- lected as the relation-chain leading to the final an- swer. Previous methods usually worked on a subset of KB in order to fit KB into memory. For entity linking, some sophisticated heuristics were com- monly used to collect entity candidates. For rela- tion detection, previous work usually enumerated all possible 1-hop and 2-hop relation-chains (start- ing from linked entity nodes) as candidates. AllWho wrote the book Beau Geste ?\"\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize the Bedrock Agent Runtime client\n",
    "bedrock_agent_runtime = boto3.client(\"bedrock-agent-runtime\", region_name=variables[\"regionName\"])\n",
    "\n",
    "# Define the query for retrieving relevant documents\n",
    "query = 'What are three sub-tasks in question answering over knowledge bases?'\n",
    "\n",
    "try:\n",
    "    # Retrieve the top 3 most relevant documents from the knowledge base\n",
    "    relevant_documents_os = bedrock_agent_runtime.retrieve(\n",
    "        retrievalQuery={\n",
    "            'text': query  # Query text for document retrieval\n",
    "        },\n",
    "        knowledgeBaseId=kb['knowledgeBaseId'],  # ID of the Knowledge Base to search in\n",
    "        retrievalConfiguration={\n",
    "            'vectorSearchConfiguration': {\n",
    "                'numberOfResults': 3  # Fetch the top 3 most relevant documents\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Print the retrieved documents for debugging\n",
    "    print(\"Successfully retrieved relevant documents.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error: Unable to retrieve relevant documents.\")\n",
    "    print(e)  # Print the error details for debugging\n",
    "\n",
    "# Output the retrieved documents\n",
    "print(relevant_documents_os[\"retrievalResults\"])\n",
    "print(json.dumps([i[\"content\"][\"text\"] for i in relevant_documents_os[\"retrievalResults\"]][0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2641ae4b",
   "metadata": {},
   "source": [
    "> **Note**: After creating the knowledge base, you can explore its details and settings in the Amazon Bedrock console. This gives you a more visual interface to understand how the knowledge base is structured.\n",
    "> \n",
    "> **[➡️ View your Knowledge Bases in the AWS Console](https://us-west-2.console.aws.amazon.com/bedrock/home?region=us-west-2#/knowledge-bases)**\n",
    ">\n",
    "> In the console, you can:\n",
    "> - See all your knowledge bases in one place\n",
    "> - View ingestion status and statistics\n",
    "> - Test queries through the built-in chat interface\n",
    "> - Modify settings and configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c8194cc9-a4f0-4560-8dd7-59f6af40ea0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IMXM4XCO1G'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb['knowledgeBaseId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5bc1cc-040b-462e-b27f-76657e891276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2440bd1-03ff-4ea9-b63b-efc2280e659b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a908760e-7cd2-42fe-82a1-6df7bb127912",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
