{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a6fdaec-a86e-47f7-9ed9-3d0242e5a9b3",
   "metadata": {},
   "source": [
    "# üìä Evaluation in Flotorch\n",
    "\n",
    "[Flotorch](https://www.flotorch.ai/) provides a comprehensive evaluation framework for Retrieval-Augmented Generation (RAG) systems. It helps assess and compare Large Language Models (LLMs) based on relevance, quality, cost, and performance to support enterprise-grade deployments.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Key Evaluation Features\n",
    "\n",
    "- **Automated LLM Evaluation**  \n",
    "  Flotorch automates evaluation across:\n",
    "  - Relevance\n",
    "  - Fluency\n",
    "  - Robustness\n",
    "  - Cost\n",
    "  - Execution Speed\n",
    "\n",
    "- **Performance Metrics**  \n",
    "  It generates quantitative scores for evaluating how well a model performs across different criteria.\n",
    "\n",
    "- **Cost and Time Insights**  \n",
    "  Offers pricing and latency breakdowns for different LLM setups, enabling cost-effective choices.\n",
    "\n",
    "- **Data-Driven Decision-Making**  \n",
    "  Helps teams align LLM usage with specific application goals, budget, and performance needs.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Evaluation Workflow\n",
    "\n",
    "1. **Experiment Configuration**  \n",
    "   Define models, parameters, and goals for evaluation.\n",
    "\n",
    "2. **Automated Execution**  \n",
    "   Run evaluation pipelines to generate performance data.\n",
    "\n",
    "3. **Results Analysis**  \n",
    "   View dashboards or reports that summarize evaluation results.\n",
    "\n",
    "4. **Expert Evaluation (Optional)**  \n",
    "   Combine automatic evaluation with human review for more nuanced feedback.\n",
    "\n",
    "---\n",
    "\n",
    "This evaluation framework enables continuous monitoring, benchmarking, and optimization of RAG systems using LLMs, helping organizations deploy more reliable and efficient AI solutions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1470f69-1c2d-437b-b6a2-77de15fd11ef",
   "metadata": {},
   "source": [
    "## Load inference varibales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e362a6f-a4fe-4929-ab95-ce596b86a199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accountNumber': '677276078734',\n",
       " 'regionName': 'us-east-1',\n",
       " 'collectionArn': 'arn:aws:aoss:us-east-1:677276078734:collection/h4x23xd1thd0kpl13b67',\n",
       " 'collectionId': 'h4x23xd1thd0kpl13b67',\n",
       " 'vectorIndexName': 'ws-index-fixed',\n",
       " 'bedrockExecutionRoleArn': 'arn:aws:iam::677276078734:role/advanced-rag-workshop-bedrock_execution_role-us-east-1',\n",
       " 's3Bucket': '677276078734-us-east-1-advanced-rag-workshop',\n",
       " 's3_ground_truth_path': 's3://677276078734-us-east-1-advanced-rag-workshop/ground_truth_data/kbqa_questions_answers.json',\n",
       " 'kbFixedChunk': 'TJSZIWHAIM'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open(\"./inference/variables.json\", \"r\") as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce74afb-8660-4576-b446-2b73f4b38191",
   "metadata": {},
   "source": [
    "## Load basepaths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54042c51-bdab-4524-922c-d65651d09004",
   "metadata": {},
   "source": [
    "## Load inference metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "982fb77b-f992-4cf9-ba5f-6e8e63941a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"./inference/inference_metrics.json\", \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3f7e0c2-87b7-43ed-8c1c-5803fb6abd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_file_path = './data/eval_prompt.json'\n",
    "with open(prompt_file_path, 'r') as f:\n",
    "    prompt = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59153913-5f0a-4a5f-8fd6-a7af3f5ae19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_config_data = {\n",
    "            \"temp_retrieval_llm\": \"0.1\",\n",
    "            \"eval_retrieval_model\": \"bedrock/cohere.command-r-v1:0\",\n",
    "            \"eval_prompt\": prompt\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0c847c-dec5-4bb0-a29f-1f74f2ee5d91",
   "metadata": {},
   "source": [
    "## Load Evaluator Class\n",
    "\n",
    "### üß† Evaluation with `CustomEvaluator`\n",
    "\n",
    "```python\n",
    "processor = CustomEvaluator(evaluator_llm=exp_config_data['eval_retrieval_model'])\n",
    "results = processor.evaluate(data)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üîπ Step-by-Step Breakdown\n",
    "\n",
    "| Line | Description |\n",
    "|------|-------------|\n",
    "| `processor = CustomEvaluator(...)` | Instantiates a `CustomEvaluator` using a language model specified in the config (`exp_config_data['eval_retrieval_model']`). |\n",
    "| `results = processor.evaluate(data)` | Runs the evaluation on the `data` using the evaluator, returning performance metrics or scoring output. |\n",
    "\n",
    "---\n",
    "\n",
    "#### üß© Key Components\n",
    "\n",
    "- **`CustomEvaluator`**: A custom class designed to handle evaluation logic, potentially wrapping RAGAS or similar frameworks.\n",
    "- **`evaluator_llm`**: The evaluation language model (e.g. GPT, Claude, etc.) used for scoring responses.\n",
    "- **`data`**: A list of evaluation items (e.g. questions, answers, reference contexts).\n",
    "- **`results`**: The output from the evaluation ‚Äî typically a dictionary or structured result with metric scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "913229fb-86b2-4da2-9321-a461618a5fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluator import CustomEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d81ee62-cff0-4581-9274-ba047fe199b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = CustomEvaluator(evaluator_llm_info = exp_config_data)\n",
    "results = processor.evaluate(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15768f7b-e1bb-43f4-a4e4-a736f074e372",
   "metadata": {},
   "source": [
    "## Save results to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97573f33-7047-420b-adf3-1c7e52af13fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "csv_file = './inference/evaluation_output.csv'\n",
    "with open(csv_file, mode='w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=['question', 'answer', 'ground answer','message','score'])\n",
    "    writer.writeheader()\n",
    "    for _id, item in enumerate(results):\n",
    "        # print(_id)\n",
    "        writer.writerow({\n",
    "            'question': item.get('question', ''),\n",
    "            'answer': item.get('answer', ''),\n",
    "            'ground answer': item.get('gt_answer', ''),\n",
    "            'message': item.get('response', '').get('message',''),\n",
    "            'score': item.get('response', '').get('score','')\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb01294-86c6-4f9d-9744-57c3ab68dd73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18b928b-c5b3-41b5-a1ad-f6ce6f0a2dea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542e2d4b-6610-4937-9efc-e024e45022d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db72d1b-3c27-4dd3-91d1-42a33de705b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
