{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a6fdaec-a86e-47f7-9ed9-3d0242e5a9b3",
   "metadata": {},
   "source": [
    "# üìä Evaluation in Flotorch\n",
    "\n",
    "[Flotorch](https://www.flotorch.ai/) provides a comprehensive evaluation framework for Retrieval-Augmented Generation (RAG) systems. It helps assess and compare Large Language Models (LLMs) based on relevance, quality, cost, and performance to support enterprise-grade deployments.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Key Evaluation Features\n",
    "\n",
    "- **Automated LLM Evaluation**  \n",
    "  Flotorch automates evaluation across:\n",
    "  - Relevance\n",
    "  - Fluency\n",
    "  - Robustness\n",
    "  - Cost\n",
    "  - Execution Speed\n",
    "\n",
    "- **Performance Metrics**  \n",
    "  It generates quantitative scores for evaluating how well a model performs across different criteria.\n",
    "\n",
    "- **Cost and Time Insights**  \n",
    "  Offers pricing and latency breakdowns for different LLM setups, enabling cost-effective choices.\n",
    "\n",
    "- **Data-Driven Decision-Making**  \n",
    "  Helps teams align LLM usage with specific application goals, budget, and performance needs.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Evaluation Workflow\n",
    "\n",
    "1. **Experiment Configuration**  \n",
    "   Define models, parameters, and goals for evaluation.\n",
    "\n",
    "2. **Automated Execution**  \n",
    "   Run evaluation pipelines to generate performance data.\n",
    "\n",
    "3. **Results Analysis**  \n",
    "   View dashboards or reports that summarize evaluation results.\n",
    "\n",
    "4. **Expert Evaluation (Optional)**  \n",
    "   Combine automatic evaluation with human review for more nuanced feedback.\n",
    "\n",
    "---\n",
    "\n",
    "This evaluation framework enables continuous monitoring, benchmarking, and optimization of RAG systems using LLMs, helping organizations deploy more reliable and efficient AI solutions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8190ce8b-26dc-45e9-bb0c-adff7aaf6487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Loading eval prompt file\n",
    "prompt_file_path = './dataset/eval_prompt.json'\n",
    "with open(prompt_file_path, 'r') as f:\n",
    "    prompt = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce74afb-8660-4576-b446-2b73f4b38191",
   "metadata": {},
   "source": [
    "## Load experiment config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7bb7517-ea81-426b-8d2f-2e246970b996",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_config_data = {\n",
    "            \"temp_retrieval_llm\": \"0.1\",\n",
    "            \"retrieval_service\": \"bedrock\",\n",
    "            \"retrieval_model\": \"us.amazon.nova-pro-v1:0\",\n",
    "            \"eval_retrieval_model\": \"bedrock/cohere.command-r-v1:0\",\n",
    "            \"eval_prompt\": prompt,\n",
    "            \"aws_region\": \"us-west-2\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54042c51-bdab-4524-922c-d65651d09004",
   "metadata": {},
   "source": [
    "## Load inference metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "982fb77b-f992-4cf9-ba5f-6e8e63941a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(f\"./results/{exp_config_data['retrieval_service']}_inference_metrics.json\", \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0c847c-dec5-4bb0-a29f-1f74f2ee5d91",
   "metadata": {},
   "source": [
    "## Load Evaluator Class\n",
    "\n",
    "### üß† Evaluation with `CustomEvaluator`\n",
    "\n",
    "```python\n",
    "processor = CustomEvaluator(evaluator_llm=exp_config_data['eval_retrieval_model'])\n",
    "results = processor.evaluate(data)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üîπ Step-by-Step Breakdown\n",
    "\n",
    "| Line | Description |\n",
    "|------|-------------|\n",
    "| `processor = CustomEvaluator(...)` | Instantiates a `CustomEvaluator` using a language model specified in the config (`exp_config_data['eval_retrieval_model']`). |\n",
    "| `results = processor.evaluate(data)` | Runs the evaluation on the `data` using the evaluator, returning performance metrics or scoring output. |\n",
    "\n",
    "---\n",
    "\n",
    "#### üß© Key Components\n",
    "\n",
    "- **`CustomEvaluator`**: A custom class designed to handle evaluation logic, potentially wrapping RAGAS or similar frameworks.\n",
    "- **`evaluator_llm`**: The evaluation language model (e.g. GPT, Claude, etc.) used for scoring responses.\n",
    "- **`data`**: A list of evaluation items (e.g. questions, answers, reference contexts).\n",
    "- **`results`**: The output from the evaluation ‚Äî typically a dictionary or structured result with metric scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "913229fb-86b2-4da2-9321-a461618a5fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluator import CustomEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d81ee62-cff0-4581-9274-ba047fe199b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:12<00:00,  1.54it/s]\n"
     ]
    }
   ],
   "source": [
    "processor = CustomEvaluator(evaluator_llm_info = exp_config_data)\n",
    "results = processor.evaluate(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a128b02a-8a76-4368-ae4f-3614b0eff7f7",
   "metadata": {},
   "source": [
    "### üì¶ Calculate Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e83c53f-dfea-45c4-a706-760094554bf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.cost_calculation import calculate_total_cost\n",
    "total_cost, results = calculate_total_cost(exp_config_data, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15768f7b-e1bb-43f4-a4e4-a736f074e372",
   "metadata": {},
   "source": [
    "## Save results to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "feb01294-86c6-4f9d-9744-57c3ab68dd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "csv_file = './results/evaluation_output.csv'\n",
    "\n",
    "# Check if 'sagemaker_cost' exists in any item\n",
    "include_sagemaker_cost = any('sagemaker_cost' in item for item in results)\n",
    "include_inference_cost = any('inference_cost' in item for item in results)\n",
    "\n",
    "fieldnames=['question', 'answer', 'inputTokens', 'outputTokens', 'totalTokens', 'latencyMs', 'ground answer','message','score']\n",
    "\n",
    "if include_sagemaker_cost:\n",
    "    fieldnames.insert(fieldnames.index('message'), 'sagemaker_cost')  # Insert before 'ground answer'\n",
    "\n",
    "if include_inference_cost:\n",
    "    fieldnames.insert(fieldnames.index('message'), 'bedrock_input_cost')  # Insert before 'ground answer'\n",
    "    fieldnames.insert(fieldnames.index('message'), 'bedrock_output_cost')  # Insert before 'ground answer'\n",
    "    \n",
    "\n",
    "with open(csv_file, mode='w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for _id, item in enumerate(results):\n",
    "        answer_metadata = item.get('answer_metadata', {})\n",
    "        response = item.get('response', {})\n",
    "\n",
    "        row = {\n",
    "            'question': item.get('question', ''),\n",
    "            'answer': item.get('answer', ''),\n",
    "            'inputTokens': answer_metadata.get('inputTokens', ''),\n",
    "            'outputTokens': answer_metadata.get('outputTokens', ''),\n",
    "            'totalTokens': answer_metadata.get('totalTokens', ''),\n",
    "            'latencyMs': answer_metadata.get('latencyMs', ''),\n",
    "            'ground answer': item.get('gt_answer', ''),\n",
    "            'message': response.get('message', ''),\n",
    "            'score': response.get('score', ''),\n",
    "        }\n",
    "\n",
    "        if include_sagemaker_cost:\n",
    "            sagemaker_cost = item.get('sagemaker_cost', {})\n",
    "            row['sagemaker_cost'] = sagemaker_cost.get('sagemaker_inference_cost', '')\n",
    "        if include_inference_cost:\n",
    "            inference_cost = item.get('inference_cost', {})\n",
    "            row['bedrock_input_cost'] = inference_cost.get('inference_input_cost', '')\n",
    "            row['bedrock_output_cost'] = inference_cost.get('inference_output_cost', '')\n",
    "\n",
    "        writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18b928b-c5b3-41b5-a1ad-f6ce6f0a2dea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52afb743-9f12-4b1a-87ad-521b064f43ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8498afa3-f570-4428-b3e1-15efb96bbb20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
