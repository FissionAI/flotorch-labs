{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3afd887e-afe6-4dc8-8f45-328c1c0a86ef",
   "metadata": {},
   "source": [
    "# Retrieval and Generation with Bedrock Foundational Models\n",
    "\n",
    "### Overview  \n",
    "This notebook demonstrates how to perform retrieval-augmented generation (RAG) using Amazon Bedrock's foundational models. It covers retrieving relevant documents from a knowledge base and generating responses based on the retrieved context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b971fe80-0562-4628-9c1a-e3dd32da163e",
   "metadata": {},
   "source": [
    "# üîç Retrieval in Flotorch\n",
    "\n",
    "[Flotorch](https://www.flotorch.ai/) is a real-time Retrieval-Augmented Generation (RAG) orchestration engine designed to streamline operational complexity and enhance observability in deploying AI workflows.\n",
    "\n",
    "In Flotorch, **retrieval** refers to the process of fetching relevant information from external knowledge bases to augment the responses generated by language models. This ensures that the AI system provides accurate, timely, and context-aware answers by combining its pre-trained knowledge with up-to-date external data.\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Key Components of Retrieval in Flotorch\n",
    "\n",
    "1. **Retriever**  \n",
    "   Searches external databases or knowledge sources to find relevant information based on the user's query.\n",
    "\n",
    "2. **Augmentation**  \n",
    "   Incorporates the retrieved data into the model's input to enhance the quality and relevance of the generated response.\n",
    "\n",
    "3. **Generator**  \n",
    "   Synthesizes a response by integrating the retrieved information with the model's existing knowledge.\n",
    "\n",
    "---\n",
    "\n",
    "This retrieval mechanism is integral to Flotorch's ability to deliver precise and context-aware AI solutions across various industries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054c315e-e054-4f58-ac90-7e5f7e8ec0a0",
   "metadata": {},
   "source": [
    "### Build your own Retrieval Augmented Generation (RAG) system\n",
    "When constructing your own retrieval augmented generation (RAG) system, you can leverage a retriever system and a generator system. The retriever can be an embedding model that identifies the relevant chunks from the vector database based on similarity scores. The generator can be a Large Language Model (LLM) that utilizes the model's capability to answer questions based on the retrieved results (also known as chunks). In the following sections, we will provide additional tips on how to optimize the prompts for your RAG system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c114e690-2fef-4dc8-8a37-f909e512dc56",
   "metadata": {},
   "source": [
    "## üîß Step 1: load aws variables created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5381fe1f-8477-421a-83ea-a498f1780662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"./results/variables.json\", \"r\") as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fb3777-e1ba-4686-9a76-7faed4fd9f8b",
   "metadata": {},
   "source": [
    "## Load Prompt json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5093f2d-ff0b-44d1-8dee-5415e039a49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_file_path = './dataset/prompt.json'\n",
    "with open(prompt_file_path, 'r') as f:\n",
    "    prompt = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a199fd02-48ac-42d4-ad88-96682f3e3e01",
   "metadata": {},
   "source": [
    "## Sample experiment JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19762bd0-5991-4b6a-8f95-2017e8b786f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_config_data = {\n",
    "            \"temp_retrieval_llm\": \"0.1\",\n",
    "            \"gt_data\": variables[\"s3_ground_truth_path\"],\n",
    "            \"rerank_model_id\": \"none\",\n",
    "            \"embedding_model\": \"amazon.titan-embed-text-v2:0\",\n",
    "            \"bedrock_knowledge_base\": True,\n",
    "            \"kb_data\": variables.get('kbFixedChunk', 'TJSZIWHAIM'),\n",
    "            \"retrieval_service\": \"sagemaker\",\n",
    "            \"knn_num\": \"3\",\n",
    "            \"knowledge_base\": True,\n",
    "            \"retrieval_model\": \"meta-textgeneration-llama-3-1-8b-instruct\",\n",
    "            \"index_id\": variables['vectorIndexName'],\n",
    "            \"gateway_api_key\": \"\",\n",
    "            \"vector_dimension\": \"1024\",\n",
    "            \"gateway_enabled\": False,\n",
    "            \"gateway_url\": \"\",\n",
    "            \"chunking_strategy\": \"Fixed\",\n",
    "            \"aws_region\": \"us-east-1\",\n",
    "            \"n_shot_prompt_guide_obj\": prompt,\n",
    "            \"n_shot_prompts\": 1\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f53614-8184-4a99-b5c0-a1dcbce28428",
   "metadata": {},
   "source": [
    "## üîç Load env config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12371288-c445-42e2-baee-8ac5ee3863aa",
   "metadata": {},
   "source": [
    "### Load Retriver function and other dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd348a6b-7fd2-4b11-ba90-f2a1a4a73cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flotorch_core.storage.storage_provider_factory import StorageProviderFactory\n",
    "from flotorch_core.reader.json_reader import JSONReader\n",
    "from flotorch_core.storage.db.vector.vector_storage_factory import VectorStorageFactory\n",
    "from flotorch_core.inferencer.inferencer_provider_factory import InferencerProviderFactory\n",
    "from flotorch_core.embedding.embedding_registry import embedding_registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19275dfc",
   "metadata": {},
   "source": [
    "### Initialize storage provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d922ecf0-a40f-4c19-8785-cea9e01670e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_data = exp_config_data['gt_data']\n",
    "storage = StorageProviderFactory.create_storage_provider(gt_data)\n",
    "gt_data_path = storage.get_path(gt_data)\n",
    "json_reader = JSONReader(storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f36e755",
   "metadata": {},
   "source": [
    "### Setting embedding to None if bedrock KB is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bb60cb-63d0-4213-b997-77a1f7441566",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd97d675-b399-4761-b7a2-b0c197241c39",
   "metadata": {},
   "source": [
    "## üóÉÔ∏è Vector Storage Initialization\n",
    "\n",
    "This section initializes the `VectorStorage` component using a factory method that dynamically selects the appropriate vector storage backend (e.g., OpenSearch, Bedrock Knowledge Base) based on the experimental configuration.\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è `VectorStorageFactory.create_vector_storage(...)`\n",
    "\n",
    "Creates an instance of vector storage using configuration flags and credentials.\n",
    "\n",
    "- **Parameters:**\n",
    "  - `knowledge_base`: *(bool)* ‚Äì Whether a knowledge base is used as a backend.\n",
    "  - `use_bedrock_kb`: *(bool)* ‚Äì If set, uses AWS Bedrock Knowledge Base.\n",
    "  - `embedding`: *(BaseEmbedding)* ‚Äì Embedding generator to use for vector creation.\n",
    "  - `knowledge_base_id`: *(str | None)* ‚Äì ID of the Bedrock knowledge base.\n",
    "  - `aws_region`: *(str | None)* ‚Äì AWS region for Bedrock and related services.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Dynamic Backend Selection\n",
    "\n",
    "The factory method chooses the backend as follows:\n",
    "\n",
    "- If `bedrock_knowledge_base` is enabled ‚Üí connects to **Bedrock KB**.\n",
    "- Else if `knowledge_base` is enabled ‚Üí connects to **custom knowledge base**.\n",
    "\n",
    "---\n",
    "\n",
    "### üìù Result\n",
    "\n",
    "Returns a configured `VectorStorage` instance ready for:\n",
    "- KNN-based vector search\n",
    "- Bedrock KB search\n",
    "- Integration into QA or retrieval pipelines\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6ad13e",
   "metadata": {},
   "source": [
    "### Initialize vector storage with configuration for embedding and optional OpenSearch/Bedrock KB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79351c77-52b5-4238-8b7c-a33dae9f880d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_storage = VectorStorageFactory.create_vector_storage(\n",
    "                knowledge_base=exp_config_data.get(\"knowledge_base\", False),\n",
    "                use_bedrock_kb=exp_config_data.get(\"bedrock_knowledge_base\", False),\n",
    "                embedding=embedding,\n",
    "                knowledge_base_id=exp_config_data.get(\"kb_data\"),\n",
    "                aws_region=exp_config_data.get(\"aws_region\")\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e2d0ee-4675-4afc-8218-1c2790bf480f",
   "metadata": {},
   "source": [
    "## ü§ñ Inferencer Initialization\n",
    "\n",
    "This block initializes the **Inferencer** using a factory method that configures the inference engine for text generation or question answering based on the experimental setup.\n",
    "\n",
    "---\n",
    "\n",
    "### üèóÔ∏è `InferencerProviderFactory.create_inferencer_provider(...)`\n",
    "\n",
    "Creates and returns an appropriate `Inferencer` instance depending on configuration such as API gateway usage, model settings, region, and credentials.\n",
    "\n",
    "---\n",
    "\n",
    "### üîß Parameters\n",
    "\n",
    "- `gateway_enabled`: *(bool)* ‚Äì Enables API gateway-based invocation if set to `True`.\n",
    "- `base_url`: *(str)* ‚Äì URL endpoint for the API Gateway (e.g., `/api/openai/v1`).\n",
    "- `api_key`: *(str)* ‚Äì API key for authenticating requests to the gateway.\n",
    "- `service`: *(str)* ‚Äì Name of the retrieval service (e.g., Bedrock, sagemaker).\n",
    "- `model_id`: *(str)* ‚Äì The model to use for inference (e.g., `anthropic.claude-v2`).\n",
    "- `region`: *(str)* ‚Äì AWS region for service provisioning (e.g., `us-east-1`).\n",
    "- `arn_role`: *(str)* ‚Äì IAM role ARN for Bedrock invocation permissions.\n",
    "- `n_shot_prompts`: *(int)* ‚Äì Number of few-shot examples to include in prompt.\n",
    "- `temp_retrieval_llm`: *(float)* ‚Äì Temperature setting for the language model.\n",
    "- `n_shot_prompt_guide_obj`: *(Any)* ‚Äì Few-shot guide object for prompt engineering.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Behavior\n",
    "\n",
    "- If `gateway_enabled` is `True`, connects to the specified API Gateway using credentials.\n",
    "- If disabled, falls back to direct model invocation through supported services like AWS Bedrock.\n",
    "- Supports dynamic few-shot prompting and custom temperature configuration.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Outcome\n",
    "\n",
    "Returns a fully configured `Inferencer` object capable of generating answers or completions for queries using the selected language model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe940888",
   "metadata": {},
   "source": [
    "### Initialize inferencer provider with configuration for gateway, retrieval service, and AWS integration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddfc720-b994-41e2-8491-35881547f325",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferencer = InferencerProviderFactory.create_inferencer_provider(\n",
    "                gateway_enabled = False,\n",
    "                base_url = \"\",\n",
    "                api_key = \"\",\n",
    "                service = exp_config_data.get(\"retrieval_service\"),\n",
    "                model_id = exp_config_data.get(\"retrieval_model\"), \n",
    "                region = exp_config_data.get(\"aws_region\"), \n",
    "                arn_role = variables.get('bedrockExecutionRoleArn', 'arn:aws:iam::677276078734:role/flotorch-bedrock-role-qamain'),\n",
    "                n_shot_prompts = int(exp_config_data.get(\"n_shot_prompts\", 0)), \n",
    "                temperature = float(exp_config_data.get(\"temp_retrieval_llm\", 0)), \n",
    "                n_shot_prompt_guide_obj = exp_config_data.get(\"n_shot_prompt_guide_obj\")\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e74bb4-5639-4216-a1d1-2b2f0d830e3b",
   "metadata": {},
   "source": [
    "## üîÅ Reranker Initialization\n",
    "\n",
    "This code conditionally initializes the **`BedrockReranker`**, which reorders retrieved documents based on relevance using a reranking model.\n",
    "\n",
    "---\n",
    "\n",
    "### üèóÔ∏è `BedrockReranker(...)` Initialization\n",
    "\n",
    "The reranker is only instantiated if a valid rerank model ID is provided in the experiment configuration.\n",
    "\n",
    "---\n",
    "\n",
    "### üîß Parameters\n",
    "\n",
    "- `aws_region`: *(str)* ‚Äì AWS region where the Bedrock reranking model is hosted.\n",
    "- `rerank_model_id`: *(str)* ‚Äì ID of the Bedrock reranking model to be used.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Behavior\n",
    "\n",
    "- If `rerank_model_id` is **not** `\"none\"` (case-insensitive), a `BedrockReranker` is created.\n",
    "- If the value is `\"none\"`, no reranker is used and the value is set to `None`.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Outcome\n",
    "\n",
    "- A `BedrockReranker` object if reranking is enabled.\n",
    "- Otherwise, `reranker = None`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a20966",
   "metadata": {},
   "source": [
    "### Initialize reranker if a valid rerank model ID is provided in the configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38800c24-aa54-4d67-afcb-0fd5c41c4bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker = BedrockReranker(exp_config_data.get(\"aws_region\"), exp_config_data.get(\"rerank_model_id\")) \\\n",
    "                if exp_config_data.get(\"rerank_model_id\").lower() != \"none\" \\\n",
    "                else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c335b9f7",
   "metadata": {},
   "source": [
    "### Load ground truth data in JSON reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7e2cd3-41d7-42a6-a588-5f7d06a5fcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read ground truth json\n",
    "from pydantic import BaseModel\n",
    "from flotorch_core.chunking.chunking import Chunk\n",
    "class Question(BaseModel):\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "    def get_chunk(self) -> Chunk:\n",
    "        return Chunk(data=self.question)\n",
    "\n",
    "questions_list = json_reader.read_as_model(gt_data_path, Question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0992f6f5",
   "metadata": {},
   "source": [
    "### ü§ñ Perform vector search for each question chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8c5c53-9308-4878-8cfc-378e718418c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hierarchical = exp_config_data.get(\"chunking_strategy\") == 'hierarchical'\n",
    "\n",
    "responses_list = []\n",
    "for question in questions_list:\n",
    "    question_chunk = question.get_chunk()\n",
    "    vector_response = vector_storage.search(question_chunk, int(exp_config_data.get(\"knn_num\")), hierarchical)\n",
    "    vector_response_result = vector_response.to_json()['result']\n",
    "    responses_list.append({'question':question, 'question_chunk':question_chunk, 'vector_response':vector_response, 'vector_response_result':vector_response_result, 'response_status':vector_response.status})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aba61c",
   "metadata": {},
   "source": [
    "### üîÅ Rerank vector responses using the reranker if enabled and response is valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a955930e-6e00-44e4-919d-6d6bb0953b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for each_response in responses_list:\n",
    "    response_status = each_response['response_status']\n",
    "    vector_response_result = each_response['vector_response_result']\n",
    "    if reranker and response_status:\n",
    "        vector_response = reranker.rerank_documents(each_response['question_chunk'].data, vector_response_result)\n",
    "        each_response['vector_response'] = vector_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38cd325",
   "metadata": {},
   "source": [
    "### üß† Generate answers and extract metadata for each response, applying guardrail checks if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c739274-0df4-4708-b5e4-400a088cb928",
   "metadata": {},
   "outputs": [],
   "source": [
    "for each_response in responses_list:\n",
    "    response_status = each_response['response_status']\n",
    "    if response_status:\n",
    "        question = each_response['question']\n",
    "        vector_response = each_response['vector_response']\n",
    "        vector_response_result = each_response['vector_response_result']\n",
    "        metadata, answer = inferencer.generate_text(question.question, vector_response_result)\n",
    "        guardrail_blocked = metadata['guardrail_blocked'] if 'guardrail_blocked' in metadata else False\n",
    "        if guardrail_blocked:\n",
    "            answer_metadata = {}\n",
    "        else:\n",
    "            answer_metadata = metadata\n",
    "    else:\n",
    "        answer = metadata['guardrail_output']\n",
    "        metadata = {}\n",
    "        answer_metadata = {}\n",
    "        guardrail_blocked = vector_response.metadata['guardrail_blocked'] if 'guardrail_blocked' in vector_response.metadata else False\n",
    "    each_response['metadata'] = metadata\n",
    "    each_response['answer'] = answer\n",
    "    each_response['answer_metadata'] = answer_metadata\n",
    "    each_response['guardrail_blocked'] = guardrail_blocked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932195d0",
   "metadata": {},
   "source": [
    "### üì¶ Aggregate final results with question, answer, guardrail assessments, and reference context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eeaedf-be15-4f47-9ac1-af1023ea8d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for each_response in responses_list:\n",
    "    metadata = each_response['metadata']\n",
    "    vector_response = each_response['vector_response']\n",
    "    vector_response_result = each_response['vector_response_result']\n",
    "    result.append(\n",
    "                {'question':each_response['question'].question,\n",
    "                'answer':each_response['answer'],\n",
    "                'guardrails_output_assessment':metadata['guardrail_output_assessment'] if 'guardrail_output_assessment' in metadata else None,\n",
    "                'guardrails_context_assessment':vector_response.metadata['guardrail_context_assessment'] if 'guardrail_context_assessment' in vector_response.metadata else None,\n",
    "                'guardrails_input_assessment':vector_response.metadata['guardrail_input_assessment'] if 'guardrail_input_assessment' in vector_response.metadata else None,\n",
    "                'guardrails_blocked':each_response['guardrail_blocked'],\n",
    "                'guardrails_block_level':vector_response.metadata['block_level'] if 'block_level' in vector_response.metadata else \"\",\n",
    "                'answer_metadata':each_response['answer_metadata'],\n",
    "                'reference_contexts':[res['text'] for res in vector_response_result] if vector_response_result else [],\n",
    "                'gt_answer':each_response['question'].answer,\n",
    "                'query_metadata':vector_response.metadata['embedding_metadata'].to_json() if 'embedding_metadata' in vector_response.metadata else None\n",
    "                })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84674286",
   "metadata": {},
   "source": [
    "### üíæ Save the aggregated results to a JSON file for inference metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc91fa0-5952-4d83-8372-ed46d47dd91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./results/{exp_config_data['retrieval_service']}_inference_metrics.json\", \"w\") as json_file:\n",
    "    json.dump(results, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a903173-61d3-4bca-b423-cf92d3c0e5b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2fa652-dd59-4f5e-9b73-2af0b79020d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
