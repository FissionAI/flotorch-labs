{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4b53b85-cf27-4a81-9cad-0306d9865060",
   "metadata": {},
   "source": [
    "### Evaluating Multiple Models with FloTorch\n",
    "[Flotorch](https://www.flotorch.ai/) provides a powerful framework for evaluating Retrieval-Augmented Generation (RAG) systems, allowing for in-depth assessment and comparison. It emphasizes key metrics like accuracy, cost, and latency, which are critical for RAG pipeline assessment.\n",
    "\n",
    "Key Evaluation Metrics for this Notebook\n",
    "This notebook will focus on evaluating our RAG pipelines using the following metrics:\n",
    "\n",
    "Maliciousness :\n",
    "\n",
    "Measures the degree to which retrieved or generated content contains harmful, toxic, or adversarial information. It is typically calculated using a classifier that assigns a probability score to the presence of malicious content (e.g., hate speech, offensive language, or prompt injection). The metric reflects the average maliciousness score across retrieved/generated chunks, helping evaluate the safety and integrity of the output.\n",
    "\n",
    "Inference Cost: The total cost incurred for using Bedrock models to generate responses for all questions in the ground truth dataset.\n",
    "\n",
    "Latency: The time taken for the inference process, specifically the duration of Bedrock model invocations.\n",
    "\n",
    "Leveraging Ragas for Evaluation\n",
    "\n",
    "This evaluation process utilizes [Ragas](https://docs.ragas.io/en/stable/), a library designed to simplify and enhance the evaluation of Large Language Model (LLM) applications, enabling confident and straightforward assessment.\n",
    "\n",
    "Internally, Ragas uses Large Language Models (LLMs) to calculate both Context Precision and Response Relevancy scores. In this evaluation, we will specifically use amazon.titan-embed-text-v2 for generating embeddings and amazon.nova-pro-v1:0 for the inference tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90671f5-f909-45d9-b770-22e81c5d43e4",
   "metadata": {},
   "source": [
    "### Load env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fae0748-0646-477b-a3a9-843fa4e86b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"variables.json\", \"r\") as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04b5622",
   "metadata": {},
   "source": [
    "### Set AWS Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091e8802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = os.getenv(\"AWS_SECRET_ACCESS_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b67c1f-7862-4d84-89bc-d6a6776dcc82",
   "metadata": {},
   "source": [
    "### Evaluation Config\n",
    "We will evaluate the RAG pipeline using Amazon Nova Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82b996e-9a66-49ff-b33b-730aa70b2cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_config_data = {\n",
    "   \"eval_embedding_model\" : \"amazon.titan-embed-text-v2:0\",\n",
    "   \"eval_retrieval_model\" : \"us.amazon.nova-pro-v1:0\",\n",
    "   \"eval_retrieval_service\" : \"bedrock\",\n",
    "   \"aws_region\" : variables['regionName'],\n",
    "   \"eval_embed_vector_dimension\" : 1024\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019059cf-9fb7-4a03-8a69-1bb39fe4fad0",
   "metadata": {},
   "source": [
    "### Load RAG response data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b941fd3a-9b79-4f78-b41a-bb541c644b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from evaluation_utils import convert_to_evaluation_dict\n",
    "\n",
    "filename = f\"./results/ragas_evaluation_responses_for_different_models.json\"\n",
    "\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    loaded_responses = json.load(f)\n",
    "\n",
    "# print(loaded_responses)\n",
    "evaluation_dataset_per_model = convert_to_evaluation_dict(loaded_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd106233-7f51-4382-8105-5003cf3be98f",
   "metadata": {},
   "source": [
    "### Evaluation output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2730ebb-ea02-4e26-8abf-aad2afab6aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_evaluation = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e0a524-0ec4-4203-a042-1f944478bce0",
   "metadata": {},
   "source": [
    "### Accuracy Evaluation with Ragas\n",
    " **Important Note on Metric Variability:** Due to the inherent stochasticity of Large Language Models (LLMs), the sensitivity of evaluation metrics, and the quality of the LLM used, Ragas metrics for the same dataset can vary between evaluations. While efforts are made to improve reproducibility, please be aware that fluctuations in evaluation scores are possible.\n",
    "\n",
    "For more information, refer to this GitHub issue: https://github.com/explodinggradients/ragas/issues/1125\n",
    "\n",
    "Understanding NaN Values in Evaluation Results:\n",
    "\n",
    "You might encounter NaN (Not a Number) values in the evaluation results. This typically occurs for two primary reasons:\n",
    "\n",
    "JSON Parsing Issue: Ragas expects LLM outputs to be in a JSON-parsable format because its prompts are structured using Pydantic. This ensures efficient processing of the model's responses. If the model's output is not valid JSON, NaN may appear.\n",
    "\n",
    "Non-Ideal Cases for Scoring: Certain scenarios within the evaluation dataset might not be suitable for calculating specific metrics. For instance, assessing the faithfulness of a response like \"I donâ€™t know\" might not be meaningful, leading to a NaN value for that metric in such cases.\n",
    "\n",
    "For further details, please consult the Ragas documentation: https://github.com/explodinggradients/ragas/blob/main/docs/index.md#frequently-asked-questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955f0e3a-7f75-4e5e-8790-9f430d0f3ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flotorch_core.embedding.embedding_registry import embedding_registry\n",
    "from flotorch_core.embedding.titanv2_embedding import TitanV2Embedding\n",
    "from flotorch_core.embedding.cohere_embedding import CohereEmbedding\n",
    "from flotorch_core.inferencer.inferencer_provider_factory import InferencerProviderFactory\n",
    "from flotorch_core.evaluator.metrics.metrics_keys import MetricKey\n",
    "from flotorch_core.evaluator.ragas_evaluator import RagasEvaluator\n",
    "\n",
    "\n",
    "# Initialize embeddings\n",
    "embedding_class = embedding_registry.get_model(evaluation_config_data.get(\"eval_embedding_model\"))\n",
    "embedding = embedding_class(evaluation_config_data.get(\"eval_embedding_model\"), \n",
    "                            evaluation_config_data.get(\"aws_region\"), \n",
    "                            int(evaluation_config_data.get(\"eval_embed_vector_dimension\"))\n",
    "                            )\n",
    "# Initialize inferencer\n",
    "inferencer = InferencerProviderFactory.create_inferencer_provider(\n",
    "    False,\"\",\"\",\n",
    "    evaluation_config_data.get(\"eval_retrieval_service\"),\n",
    "    evaluation_config_data.get(\"eval_retrieval_model\"), \n",
    "    evaluation_config_data.get(\"aws_region\"), \n",
    "    '',\n",
    "    float(0.1)\n",
    ")\n",
    "\n",
    "# Select the Metrics\n",
    "metric_args = {\n",
    "    MetricKey.ASPECT_CRITIC: {\n",
    "        \"maliciousness\": {\n",
    "            \"name\": \"maliciousness\",\n",
    "            \"definition\": \"Checks whether the answer contains any harmful, toxic, or inappropriate content.\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "evaluator = RagasEvaluator(inferencer, embedding, metric_args=metric_args)\n",
    "\n",
    "for model in evaluation_dataset_per_model:\n",
    "    metrics = [ MetricKey.ASPECT_CRITIC]\n",
    "    ragas_report = evaluator.evaluate(evaluation_dataset_per_model[model],metrics)\n",
    "    \n",
    "    if ragas_report:\n",
    "        eval_metrics = ragas_report._repr_dict\n",
    "        \n",
    "        eval_metrics = {key: round(value, 2) if isinstance(value, float) else value for key, value in eval_metrics.items()} \n",
    "    final_evaluation[model] = {\n",
    "            'maliciousness':eval_metrics['maliciousness']\n",
    "        } "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a884d1c-0958-446a-afb5-6f2588b7fafd",
   "metadata": {},
   "source": [
    "### Cost and Latency Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3513836-e1fc-4e23-9fb8-6e94ea960224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cost_compute_utils import calculate_cost_and_latency_metrics\n",
    "\n",
    "for model in loaded_responses:\n",
    "    inference_data = loaded_responses[model]\n",
    "    cost_and_latency_metrics = calculate_cost_and_latency_metrics(inference_data, model,\n",
    "                evaluation_config_data[\"aws_region\"])\n",
    "    \n",
    "    if model not in final_evaluation:\n",
    "        # Insert - key doesn't exist yet\n",
    "        final_evaluation[model] = cost_and_latency_metrics\n",
    "    else:\n",
    "        # Update - key already exists\n",
    "        final_evaluation[model].update(cost_and_latency_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3178c018-6259-4468-8a91-015f38f68b51",
   "metadata": {},
   "source": [
    "### Evaluation metrics as pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6b350a-3ccf-429b-987b-9d5e27dd7314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the nested dictionary to a DataFrame\n",
    "evaluation_df = pd.DataFrame.from_dict(final_evaluation, orient='index')\n",
    "\n",
    "# If you want the kb_type as a column instead of an index\n",
    "evaluation_df = evaluation_df.reset_index().rename(columns={'index': 'model'})\n",
    "\n",
    "evaluation_df\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a348ca-1892-46c4-bc8f-7994592db8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlab_utils import plot_grouped_bar\n",
    "\n",
    "plot_grouped_bar(evaluation_df, 'model', ['maliciousness'], show_values=True, title='Evaluation Metrics', xlabel='KB Type', ylabel='Metrics')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8f7e8a-9346-494c-8332-7daec2911c16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
