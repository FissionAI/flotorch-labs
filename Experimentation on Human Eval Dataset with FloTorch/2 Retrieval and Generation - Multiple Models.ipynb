{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Semantic Chunking KB with multiple models for RAG\n",
    "\n",
    "### Overview\n",
    "\n",
    "This notebook demonstrates Retrieval-Augmented Generation (RAG) using a predefined ground truth to evaluate the effectiveness of multiple models. We utilize Amazon Nova lite, Nova micro, Claude Haiku and Claude Sonnet for generating responses and FloTorch for evaluating these responses against the ground truth after retrieving information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"variables.json\", \"r\") as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set AWS Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load prompt.json\n",
    "\n",
    "prompt.json file includes the following:\n",
    "\n",
    "* system_prompt\n",
    "* examples for n shot learning\n",
    "* user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_file_path = './data/prompt.json'\n",
    "with open(prompt_file_path, 'r') as f:\n",
    "    prompt = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the evaluation against multiple models\n",
    "\n",
    "\n",
    "\n",
    "Inference Models considered - Amazon Nova Lite, Amazon Nova Pro, Claude Haiku 3.5, Claude Sonnet 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_models = ['flotorch/nova-pro', 'flotorch/nova-micro',\n",
    "                'flotorch/us-anthropic-claude-3-5-haiku','flotorch/anthropic-claude-3-5-sonnet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment Config\n",
    "\n",
    "\n",
    "* **Rerank Model:** Amazon Rerank\n",
    "* **N-Shot Prompt:** 1\n",
    "* **Temperature:** 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_config_data = {\n",
    "    \"temp_retrieval_llm\": \"0.1\",\n",
    "    \"gt_data\": variables[\"s3_ground_truth_path\"],\n",
    "    \"rerank_model_id\": \"amazon.rerank-v1:0\",\n",
    "    \"retrieval_service\": \"bedrock\",\n",
    "    \"retrieval_model\": \"us.amazon.nova-lite-v1:0\",\n",
    "    \"aws_region\": variables['regionName'],\n",
    "    \"n_shot_prompt_guide_obj\": prompt,\n",
    "    \"n_shot_prompts\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load ground truth data\n",
    "\n",
    "We utilize FloTorch core's S3StorageProvider and JSONReader to load ground truth data for evaluating the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flotorch_core.storage.storage_provider_factory import StorageProviderFactory\n",
    "from flotorch_core.reader.json_reader import JSONReader\n",
    "from flotorch_rag_utils import Question\n",
    "gt_data = exp_config_data['gt_data']\n",
    "storage = StorageProviderFactory.create_storage_provider(gt_data)\n",
    "gt_data_path = storage.get_path(gt_data)\n",
    "json_reader = JSONReader(storage)\n",
    "questions = json_reader.read_as_model(gt_data_path, Question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Inferencer\n",
    "\n",
    "Creates and returns an appropriate `Inferencer` instance depending on service and the model \n",
    "\n",
    "#### Parameters\n",
    "\n",
    "- `gateway_enabled`: *(bool)* – Enables FloTorch LLM gateway-based invocation if set to `True`.\n",
    "- `gateway_url`: *(str)* – URL endpoint for the FloTorch LLM Gateway.\n",
    "- `gateway_api_key`: *(str)* – API key for authenticating requests to the FloTorch LLM gateway.\n",
    "- `retrieval_service`: *(str)* – Name of the retrieval service (e.g., bedrock, sagemaker).\n",
    "- `retrieval_model`: *(str)* – The model to use for inference (e.g., `anthropic.claude-v2`).\n",
    "- `aws_region`: *(str)* – AWS region for service provisioning (e.g., `us-east-1`).\n",
    "- `iam_role`: *(str)* – IAM role ARN for SageMaker invocation permissions.\n",
    "- `n_shot_prompts`: *(int)* – Number of few-shot examples to include in prompt.\n",
    "- `temp_retrieval_llm`: *(float)* – Temperature setting for the language model.\n",
    "- `n_shot_prompt_guide_obj`: *(Any)* – Few-shot guide object for prompt engineering.\n",
    "\n",
    "---\n",
    "\n",
    "#### Behavior\n",
    "\n",
    "- If `gateway_enabled` is `True`, connects to the FloTorch LLM Gateway using credentials.\n",
    "- If disabled, falls back to direct model invocation through supported services like AWS Bedrock or AWS SageMaker.\n",
    "- Supports dynamic few-shot prompting and custom temperature configuration.\n",
    "\n",
    "---\n",
    "\n",
    "#### Outcome\n",
    "\n",
    "Returns a fully configured `Inferencer` object capable of generating answers or completions for queries using the selected language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flotorch_core.inferencer.inferencer_provider_factory import InferencerProviderFactory\n",
    "\n",
    "def initialize_inferencer(inference_model: str, exp_config_data: dict):\n",
    "    inferencer = InferencerProviderFactory.create_inferencer_provider(\n",
    "        True,\n",
    "        \"https://qa-gateway.flotorch.cloud/api/openai/v1/\",\n",
    "        \"sk_YjhlZjc4ZDYtYjQ5Mi00OGE2LWFiZmItMGFjZTE2NTQwMjBi_YsZXC94WX3rvwNkO2B4lJJy04EDb6C7VhNSASTKyO4Y=\",\n",
    "        exp_config_data.get(\"retrieval_service\"),\n",
    "        inference_model, \n",
    "        exp_config_data.get(\"aws_region\"), \n",
    "        \" \",\n",
    "        int(exp_config_data.get(\"n_shot_prompts\", 0)), \n",
    "        float(exp_config_data.get(\"temp_retrieval_llm\", 0)), \n",
    "        exp_config_data.get(\"n_shot_prompt_guide_obj\")\n",
    "    )\n",
    "    return inferencer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute RAG against all the inference models\n",
    "\n",
    "Perform the retrieval, reranking, and inference steps using the `flotorch-core` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flotorch_rag_utils import rag_with_flotorch\n",
    "\n",
    "rag_response_dict = {}\n",
    "# The evaluation process duration is dependent on the volume of questions and the number of models bases being evaluated. \n",
    "# Larger evaluations require more time, generally around 5-6 minutes.\n",
    "for inference_model in inference_models:\n",
    "    print(f\"{inference_model} inferece started\")\n",
    "    inferencer = initialize_inferencer(inference_model, exp_config_data)\n",
    "    responses = rag_with_flotorch(exp_config_data, None, None, inferencer, questions)\n",
    "    rag_response_dict[inference_model] = responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_response_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write the results to a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "filename = f\"./results/ragas_evaluation_responses_for_different_models.json\"\n",
    "\n",
    "# Save to JSON with proper formatting\n",
    "with open(filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(rag_response_dict, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
