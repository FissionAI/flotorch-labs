{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Multiple Models\n",
    "\n",
    "[FloTorch](https://www.flotorch.ai/) offers a robust evaluation framework for Retrieval-Augmented Generation (RAG) systems, enabling comprehensive assessment and comparison of Large Language Models (LLMs). It focuses on key metrics such as accuracy, cost, and latency, crucial for enterprise-level deployments.\n",
    "\n",
    "## Key Evaluation Metrics for this Notebook\n",
    "\n",
    "In this notebook, we will focus on evaluating our RAG pipelines using the following metrics:\n",
    "\n",
    "* **Correctness:** This refers to the total number of samples that semantically both generated and expected are mateched\n",
    "\n",
    "* **Inference Cost:** This refers to the total cost incurred for invoking Bedrock models to generate responses for all entries in the ground truth dataset.\n",
    "\n",
    "* **Latency:** This measures the time taken for the inference process, specifically the duration of the Bedrock model invocations.\n",
    "\n",
    "\n",
    "RAG systems are evaluated using a scoring method that measures response quality to questions in the evaluation set. Responses are rated as correct or incorrect:\n",
    "\n",
    "Correct: The predicted code passes all the test cases. It correctly answers the user question and contains no hallucinated or irrelevant content.\n",
    "\n",
    "Incorrect: The predicted code fails to pass one or more test cases, or provides wrong or irrelevant information to answer the user question.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"variables.json\", \"r\") as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set AWS Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = os.getenv(\"AWS_SECRET_ACCESS_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_config_data = {\n",
    "   \"eval_embedding_model\" : \"amazon.titan-embed-text-v2:0\",\n",
    "   \"eval_retrieval_model\" : \"us.amazon.nova-pro-v1:0\",\n",
    "   \"eval_retrieval_service\" : \"bedrock\",\n",
    "   \"aws_region\" : variables['regionName'],\n",
    "   \"eval_embed_vector_dimension\" : 1024\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load RAG response data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "filename = f\"./results/ragas_evaluation_responses_for_different_models.json\"\n",
    "\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    loaded_responses = json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Evaluation with Custom Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_evaluation import CustomEvaluator\n",
    "\n",
    "evaluator = CustomEvaluator(evaluator_llm_info = evaluation_config_data)\n",
    "evaluation_metrics = {}\n",
    "for model_id, inference_data in loaded_responses.items():\n",
    "    results = evaluator.evaluate(inference_data)\n",
    "    evaluation_metrics[model_id] = results\n",
    "    print(f\"Evaluation completed for {model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_evaluation = evaluator.evaluate_results_dict(evaluation_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost and Latency Evaluation (In Progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_responses['flotorch/anthropic-claude-3-5-sonnet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cost_compute_utils import calculate_cost_and_latency_metrics\n",
    "\n",
    "# for model in loaded_responses:\n",
    "#     inference_data = loaded_responses[model]\n",
    "#     cost_and_latency_metrics = calculate_cost_and_latency_metrics(inference_data, model,\n",
    "#                 evaluation_config_data[\"aws_region\"])\n",
    "#     print(cost_and_latency_metrics)\n",
    "#     if model not in final_evaluation:\n",
    "#         # Insert - key doesn't exist yet\n",
    "#         final_evaluation[model] = cost_and_latency_metrics\n",
    "#     else:\n",
    "#         # Update - key already exists\n",
    "#         final_evaluation[model].update(cost_and_latency_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metrics as pandas df (In Progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Convert the nested dictionary to a DataFrame\n",
    "# evaluation_df = pd.DataFrame.from_dict(final_evaluation, orient='index')\n",
    "\n",
    "# # If you want the kb_type as a column instead of an index\n",
    "# evaluation_df = evaluation_df.reset_index().rename(columns={'index': 'model'})\n",
    "\n",
    "# evaluation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
